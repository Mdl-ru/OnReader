<html>
<head>
   <link rel="icon" href="/i/MdlLogo.gif" type="image/gif">
   <title>Глава 6. Выбор соответствующих аппаратных средств. Реализация облачного хранилища с OpenStack Swift.</title>
   <meta name="Keywords" content="OpenStack, Cloud computing">
   <meta name="Description" content="Глава 6. Выбор соответствующих аппаратных средств. Реализация облачного хранилища с OpenStack Swift.">
   <meta name="Robots" content="INDEX, FOLLOW">
   <meta name="Author" content="Module-Projects,Ltd">
   <meta name="Copyright" content="Copyright 1998..2014 Module-Projects,Ltd">
   <meta http-equiv="Pragma" content="no-cache">
<script language="javascript" src="/css/v.0/mdlcss.js"></script>
<style type="text/css" media="screen, print">@import url("i/global-20140610.css");</style>
<script language="javascript" src="http://www.mdl.ru/usd.js"></script>
	<script language="javascript" src="http://www.mdl.ru/js/common.js"></script>
	<script language="javascript" src="http://www.mdl.ru/Solutions/ABC.js"></script>
</head>
<body>
<table class="bg_White" width="1024" align="center" valign="top" border="0" cellpadding="0" cellspacing="0"><tbody>
<tr>
<td>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td width="150" valign="top" align="center"><img src="http://www.mdl.ru/RMC9.jpg" border=0 /></td>
<td width="724" valign="bottom" align="center">
<a class="item-t" href="http://www.mdl.ru"><img src="http://www.mdl.ru/i/MdlBigLogo.gif" border="0"></a><br/>
<a class="item-t" href="http://www.mdl.ru">С 1991 года на компьютерном рынке России</a>
</td>
<td align="center" valign="bottom">
<a class="item-t" href="javascript:tocall()" onmouseover="this.href=mail"><img src="http://www.mdl.ru/i/9563499.gif" border="0" alt="e-mail" /><br/><br/>т.: 676 0965, 676 0396<br/>Москва, Сосинская ул. 43, <br/>м. Волгоградский проспект</a>
</td>
</tr>
<tr>
<td class="big_16y" colspan="3" align="center"><a href="ToC.htm">Реализация облачного хранилища с OpenStack Swift.</a></td>
</tr>
<tr><td colspan="2">


<h2 align="right">ГЛАВА 6</h2>
<hr />
<h1 id="Chapter_06" align="right">Выбор соответствующих аппаратных средств.</h1>
<p>
Пользователи, использующие OpenStack Swift в качестве частного облака будут сталкиваться с задачей выбора аппаратного обеспечения.
Данная глава проведет вас по всему подлежащему вашему выбору аппаратному оборудованию, используемым критериям и, наконец, стратегии выбора поставщика.
Если вы используете общедоступное облако, единственная аппаратура, которую вы можете выбрать это шлюз облака, следовательно вы можете пропустить эту главу.
</p>

<h2 id="Ch0601">Список аппаратных средств</h2>
<p>
Список минимальной аппаратуры, требующейся для реализации Swift включает в себя:
<table style="border: none; align=center;" align="center" cellspacing="0" cellpadding="0"><tbody>
<tr>
 <td width="20%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Элемент</strong>
 <td width="80%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Описание</strong>
</tr><tr>
 <td><strong>Серверы хранения</strong>
 <td>Это реальные сервера, на которых исполняется программное обеспечение серверов объектов, а также обычно и программное обеспечение серверов учетных записей и контейнеров.
Сервера хранения требуют диски для хранения объектов.
</tr><tr>
 <td><strong>Сервер(ы) прокси</strong>
 <td>Это реальные сервера, на которых исполняется программное обеспечение серверов прокси.
Требуется по крайней мере один.
</tr><tr>
 <td style="border-bottom: thin solid;"><strong>Сетевой(ые) коммутатор(ы)</strong>
 <td style="border-bottom: thin solid;"><a href="Ch03ru.htm"><em>Глава 3.Установка OpenStack Swift</em></a> описывает различные необходимые сети.
Требуется, по крайней мере, один коммутатор.
</tr></tbody></table>
</p><p>
Ниже приводится список опциональных аппаратных средств, которые вы можете приобрести:
<table style="border: none; align=center;" align="center" cellspacing="0" cellpadding="0"><tbody>
<tr>
 <td width="20%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Элемент</strong>
 <td width="80%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Описание</strong>
</tr><tr>
 <td><strong>Серверы учетных записей</strong>
 <td>Для больших установок, в которых списки контейнеров и обновления подавляют сервера хранения, могут потребоваться отдельные сервера учетных записей.
</tr><tr>
 <td><strong>Серверы контейнеров</strong>
 <td>Для больших установок, в которых списки объектов и обновления подавляют сервера хранения, могут потребоваться отдельные сервера контейнеров.
</tr><tr>
 <td><strong>Серверы авторизации</strong>
 <td>Для больших установок, в которых авторизация подавляет прокси сервера, могут потребоваться отдельные сервера учетных записей.
</tr><tr>
 <td><strong>JBOD</strong>
 <td>Для установок, в которых важна большая плотность дисков, сервера хранения могут быть соединены с JBOD(just a bunch of disks, массивом независимых дисков) с использованием соединения SAS для увеличения плотности дисков.
</tr><tr>
 <td><strong>Балансировщик нагрузки /ускорители SSL</strong>
 <td>Полезно предоставлять один IP адрес всему кластеру (существуют программные средства, реализующие такой механизм, однако его описание выходит за рамки данной книги. <em>Прим. пер.: см., например, <a href="http://nginx.org/ru/">http://nginx.org/ru/</a></em>)<br />
Функциональность SSL в балансировщике нагрузки разгружает программное обеспечение SSL прокси- сервера.
</tr><tr>
 <td><strong>Оборудование межсетевого экрана и безопасности</strong>
 <td>Для общедоступных сетей, сетей сообществ и некоторых частных сетей, в зависимости от политики безопасности вашей компании, могут потребоваться межсетевые экраны и аппаратура обеспечения безопасности, например такая, как средства обнаружения/ предотвращения вторжений.
</tr><tr>
 <td style="border-bottom: thin solid;"><strong>Локальный шлюз облака</strong>
 <td style="border-bottom: thin solid;">Для адаптации приложений, пока не перенесенных пока на REST HTTP API, вам потребуется устройство преобразования протокола, которое преобразует обычные протоколы файлового и блокового обмена в REST API.
Такие устройства называются шлюзами облака и являются только частью оборудования, которое может понадобиться даже для общедоступного облака.
</tr></tbody></table>
</p><p>
Чтобы еще более усложнить задачу, каждый сервер имеет следующие конструктивные элементы, подлежащие настройке:<ul>
 <li><strong>Производительность ЦПУ</strong>: 
Производительность ЦПУ описывается в терминах числа процессоров и числа ядер в процессоре.
Она имеет наиболее прямое влияние на производительность сервера.
 <li><strong>Оперативная память</strong>: 
Следующим важнейшим элементом рассмотрения является количество DRAM памяти (динамической памяти), которая описывается в ГигаБайтах.
 <li><strong>Флэш- память</strong>: 
Флэш- память является другим элементом, критичным для производительности и обычно находится в диапазоне ТераБайтов.
 <li><strong>Диски /JBOD</strong>: 
Для серверов хранения вы должны описать число дисков и тих типы (интерфейсы, скорости, рейтинги и т.л.).
Эти диски могут находится в серверах, подключаться через JBOD-ы, или в их комбинации. 
 <li><strong>Устройства сетевого ввода/вывода</strong>: 
Серверы требуют подключений сетевого ввода/вывода через <strong>LAN-on moterboard (LOM)</strong> (сетевой порт на материнской плате) или устройство расширения <strong>network interface card (NIC)</strong> (сетевую плату).
Это обычно 1Gbps (Гигабит в секунду) или 10Gbps в терминах скорости передачи данных.
 <li><strong>Управление аппаратурой</strong>: 
Серверы различаются по функциям управления аппаратными средствами: начиная с  исключительно рудиментарного мониторинга через средства операционной системы, независимых от операционных систем IPMI, вплоть до сложных дистанционных KVM и удаленных систем хранения.
</ul>
</p>


<h2 id="Ch0602">Критерии выбора аппаратных средств</h2>
<p>
Очевидно, что мир аппаратных средств, представляющий их выбор и компоненты внутри каждого сервера ошеломляют воображение.
Кроме того, соотношения между прокси серверами с одной стороны и серверами учетных записей, контейнеров и хранения с другой стороны являются дополнительным усложнением.
Прешде чем мы пройдем сквозь систематический выбор согласно критериев, мы должны определить следующие характеристики в вашем окружении:<ul>
 <li><strong>Место оптимизации в вашей среде</strong>:
 Вам необходимо будет выбрать о чем вы будете больше заботиться: о производительности или стоимости.
 <li><strong>Масштаб</strong>: 
Масштаб также имеет огромное влияние на выбор аппаратных средств.
Для простоты будем говорить о <em>маленьком</em> в диапазоне сотен ТераБайт, <em>средний</em> находится в пределах ПетаБайт и <em>большой</em> начинается с десятков ПетаБайт и выше.
Вы должны определиться: в каком диапазоне вы находитесь.
</ul> 
</p><p>
Процесс выбора аппаратных средств следующий.
</p>

<h3 id="Ch060201">Шаг 1 - выбор конфигурации сервера хранения</h3>
<p>
Для малых и средних реализаций сервер хранения может содержать в себе программное обеспечение серверов объектов, учетных записей и контейнеров.
Для больших систем мы бы рекомендовали выделить сервера учетных записей и/или контейнеров.
Для оптимизируемых под производительность кластеров совокупная производительность дисковых систем должна соответствовать общей производительности других компонентов серверов (ЦПУ, оперативной памяти, флеш- памяти и систем ввода/ вывода).
Для оптимизирующих стоимость кластеров, производительность дисковых систем должна должна превосходить производительность других компонентов (другими словами, экономим деньги, зажимая производительность).
На самом деле, рассмотрите возможность подключения JBOD для получения на практике высокой плотности дисков.
</p><p>
Высокая плотность дисков также приводит к небольшой деградации надежности, поскольку отказ узла требует большего времени самовосстановления, следовательно, два дополнительных отказа (если у вас есть три копии) имеют слегка более высокую вероятность возникновения в силу более длительного промежутка времени.
Конечно, вероятность двух отказов, происходящих в окне одного самовосстановления очень низкая в обоих случаях.
Следующий рисунок обозначет сервер хранения с дисками (естественно, опциональный JBOD может быть также подключен к нему):
	<div style="width: 240px; margin: 1.75em auto;">
	<p style="text-align: center;">
		<img src="i/Pic06-01.jpg"  alt="Сервер хранения" title="Сервер хранения" width="240" height="74" style="width: 240px" />
		<span class="image-caption">Сервер хранения</span>
	</p></div>
</p><p>
Руководство по настройке OpenStack (<a href="http://docs.openstack.org/havana/install-guide/install/apt/content/object-storage-system-requirements.html"><code>http://docs.openstack.org/havana/install-guide/install/apt/content/object-storage-system-requirements.html</code></a>) рекомендует следующую спецификацию сервера:<ul>
 <li><strong>Процессор</strong>: Два четырехядерных.
 <li><strong>Оперативная память</strong>: от 8 до 12ГБайт.
 <li><strong>Сетевой ввод/вывод</strong>: 1 x 1Гбит/сек (Gbps) NIC.
Ограничивается доступным бюджетом, наша рекомендация будет выходить за рамки официальной рекомендации и состоит в использовании 10Gbps.
</ul>
</p><p>
В связи с деградацией производительности не следует включать RAID (существует исключение: если вы хотите быть уверенными в согласованности даже в случае полной потери электропитания, возможно, вам придется рассмотреть возможность организации RAID).
</p><p>
Наконец, ключевым фактором является тип диска: уровня предприятия или рабочего места.
В рамках дисков уровня предприятия (enterprise) существуют диски с 15K, 10K или 7.2K оборотами в минуту  (<strong>RPM, rotations per minute</strong>) и различными конфигурациями емкости.
Для малых и средних реализаций вы можете захотеть выбрать диски уровня предприятия, поскольку они более надежны, чем диски для рабочих мест.
Малые и средние системы, как правило, не настраиваются на обработку большой интенсивности отказов дисков уровня рабочих мест.
Производительность и емкость, которые вы выбираете для дисков уровня предприятия, очевидно, зависят от ваших конкретных требований.
</p><p>
Для больших систем, которые <em>также</em> являются очень чувствительными к стоимости, вы можете захотеть рассмотреть возможность использования дисков уровня рабочих мест.
Высокая емкость дисков уровня рабочих мест (до 6ТБ на момент написания) также вносит вклад в выгоду для больших реализаций.
В дополнение к вопросам надежности, диски уровня рабочих мест не предназначены для работы в режиме 24 x 7. 
Это означает, что ИТ- персонал должен иметь достаточную квалификацию, чтобы справляться с большим числом отказов и/или остановов дисков для соответствия спецификации.
</p>

<h3 id="Ch060202">Шаг 2 - определение настроек регионов и зон</h3>
<p>
Далее мы должны определиться с регионами и зонами.
Чило регионов произрастает из желаниязащитить данные от катастрофы или приблизить данные к источникам, потребляющим их.
Как только мы определились с числом регионов, выберите число зон для каждого региона.
По крайней мере, вы должны иметь такое количество зон, сколько есть реплик.
Мы рекомендуем иметь не менее трех зон, а Rackspace рекомендует пять (<a href="http://docs.openstack.org/havana/install-guide/install/apt/content/example-object-storageinstallation-architecture.html"><code>http://docs.openstack.org/havana/install-guide/install/apt/content/example-object-storageinstallation-architecture.html</code></a>).
Кластеры малых размеров могут удовлетвориться четырьмя.
Ознакомьтесь, пожалуйста, с <a href="Ch02ru.htm"><em>Главой 2, Архитектура OpenStack Swift</em></a> для уточнения определений регионов и зон.
</p>

<h3 id="Ch060203">Шаг 3 - выбор конфигурации сервера учетных записей и контейнеров</h3>
<p>
Как уже упоминалось ранее, если вы не устанавливаете большую конфигурацию, вам не следует беспокоиться об отдельных серверах учетных записей и контейнеров.
Путем подбора правильных объемов оперативной памяти и флеш- памяти для отдельных серверов учетных записей и/или контейнеров вы должны убедиться, что производительность SQLite достаточна для соответствия требованиям ваших спсиков баз данных и обновлений.
Руководство по настройке OpenStack рекомендует следующие спецификации (вы можете уменьшить требования на основе размера вашего кластера и требований производительности):
	<div style="width: 221px; margin: 1.75em auto;">
	<p style="text-align: center;">
		<img src="i/Pic06-02.jpg"  alt="Опциональный сервер учетных записей и контейнеров" title="Опциональный сервер учетных записей и контейнеров" width="221" height="71" style="width: 221px" />
		<span class="image-caption">Опциональный сервер учетных записей и контейнеров</span>
	</p></div>
<ul>
 <li><strong>Процессор</strong>: 
Два четырехядерных.
 <li><strong>Оперативная память</strong>: 
От 8 до 12ГБайт.
 <li><strong>Сетевой ввод/вывод</strong>: 
1 x 1Гбит/сек (Gbps) NIC.
Ограничивается доступным бюджетом, наша рекомендация будет выходить за рамки официальной рекомендации и состоит в использовании 10Gbps.
 <li><strong>Флеш- память</strong>:
Не специфицирована.
Зависит от требований производительности пользователя.
</ul>
</p>

<h3 id="Ch060204">Шаг 4 - выбор конфигурации прокси- сервера</h3>
<p>
В общем случае прокси- сервер должен соответсвовать количеству запросов API.
Как обсуждалось в <a href="Ch02ru.htm"><em>Главе 2, Архитектура OpenStack Swift</em></a>, дополнительные модули программного обеспечения промежуточного уровня могут также работать на прокси сервере.
Следовательно, прокси- сервер должен удовлетворять уровню производительности, соотвествующему такой нагрузке.
Использование нескольких можных прокси- серверов в противоположность большому числу &quot;хилых&quot; серверов более эффективно со стоимостной точки зрения, как было доказано Zmanda (<a href="http://www.zmanda.com/blogs/?p=774"><code>http://www.zmanda.com/blogs/?p=774</code></a>).
Мы готовы согласиться с руководством по настройке OpenStack, которое рекомендует следующие спецификации:<ul>
 <li><strong>Процессор</strong>: 
Два четырехядерных.
 <li><strong>Сетевой ввод/вывод</strong>: 
1 x 1Гбит/сек (Gbps) NIC.
Нашей рекомендацией будет наличие по крайней мере двух NIC, одна для внутреннего кластера хранения и одна для трафика развернутого в сторону клиента (API). 
1Гбит/сек - требование, вытекающее из ограничения бюджета, наша рекомендация будет выходить за рамки официальной рекомендации и состоит в использовании 10Gbps.
Также ознакомьтесь со связанным с этим обсуждением SSL в разделе <a href="#Ch060207"><em>Шаг 7 - выбор дополнительного сетевого оборудования</em></a>, которое влияет на сетевой ввод/вывод.
</ul>
</p><p>
Если прокси- сервер выполняет большой набор программного обеспечения среднего уровня, рассмотрите перемещение некоторого объема из этого набора на выделенный сервер.
Наиболее часто выносимым на отдельный сервер программным обеспечением среднего уровня является программное обеспечение авторизации. 
</p>

<h3 id="Ch060205">Шаг 5 - Выбор сетевых аппаратных средств</h3>
<p>
Существует три упоминавшиеся ранее сети - развернутая в сторону клиента (API), внутренняя сеть кластера хранения и сеть репликаций.
Знакомьтесь с <a href="Ch03ru.htm"><em>Главой 3, Установка OpenStack Swift</em></a> для понимания ахитектуры этих трех сетей.
Они могут быть комбинацией 1Gbps, 10Gbps или гибридными коммутаторами ethernet 1/10Gbps.
Ниже приведены некоторые методы калибровки, связанные с производительностью. <ul>
 <li><strong>Развернутая в сторону клиента сеть</strong>: 
Сетевой ввод вывод этой сети определяется требованиями пропускной способности всего кластера.
Наприме, если ваш кластер имеет 10 прокси- серверов и имеет размеры, достаточные для обеспечения 10000 запросов ввода/вывода в секунду размером 1МБайт каждый, то очевидно, что каждый прокси- сервер должен иметь 10Gbps сетевые возможности ввода/вывода.
 <li><strong>Внутренняя сеть кластера хранения</strong>: 
Требования к этой сети зависят от общей пропускной способности кластера и его размера.
Размер кластера имеет значение, так как кластер генерирует большой объем обмена компонентами завершающего программного обеспечения (см. <a href="Ch02ru.htm"><em>Главу 2, Архитектура OpenStack Swift</em></a>).
Как уже упоминалось в стоимостных ограничениях, мы рекомендуем использовать 10Gbps сети.
 <li><strong>Сеть репликаций</strong>: 
Данная сеть зависит от общей пропускной способности записи и размера кластера.
Например, вы ожидаете 1000 запросов на запись в секунду, причем каждый имеет размер 1КБайт, с этой работой справится сеть 10Mbps.
</ul> 
</p><p>
Кроме этого следует рассмотреть модель живучести.
Поскольку коммутаторы в случае выхода из строя могут привести к отключению целых зон или даже регионов, вы должны рассмотреть конфигурации с двойным резервированием.
Следующий рисунок обозначает сетевой коммутатор:
	<div style="width: 223px; margin: 1.75em auto;">
	<p style="text-align: center;">
		<img src="i/Pic06-03.jpg"  alt="Сетевой коммутатор" title="Сетевой коммутатор" width="223" height="72" style="width: 223px" />
		<span class="image-caption">Сетевой коммутатор</span>
	</p></div>
</p>

<h3 id="Ch060206">Шаг 6 - выбор соотношений между серверами различного типа</h3>
<p>
После того, как вы определились с конфигурацией отдельных серверов, необходимо выбрать соотношения между серверами различных типов.
Поскольку большинство конфигураций будут иметь только два вида среверов, а именно: сревера прокси и сервера хранения, мы обсудим соотношения только между этими двумя типами.
Согласно работе, выполненой Zmanda, сервер прокси должен быть хорошо сбалансирован (не иметь недозагруженности и не быть перегруженным).
Если пропускная способность одного сервера хранения составляет 1Gbps, а пропускная способность прокси сервера, например, 10Gbps, то соотношение будет равно 10 (такой простой расчет применим при доминировании больших объектов в общей пропускной способности. Для объектов меньшего размера вычисления должны быть сосредоточены на количестве зпросов).
</p><p>
Вместо покупки оборудования по частям, данный пример соотношения позволяет пользователю определить &quot;блок&quot; закупки.
Блоком может быть целая стойка оборудования, несколько стоек или несколько элементов, монтируемых в стойку.
Блок оборудования ортогонален зонам Swift и обычно вы предпочитаете добавлять по одному блоку в каждую зону симметричным образом.
Каждый блок имеет набор прокси серверов, серверов хранения и сетевых коммутаторов и тому подобные детали компонентов.
Масштабирование кластера Swift по мере роста данных становится с использованием такой техники закупок намного проще.
Как уже упоминалось ранее, вы должны начинать с конфигурации, имеющей по крайней мере два сервера прокси для обеспечения адекватной живучести. 
</p><p>
Например, предположим, что вы хотите увеличивать кластер примерно с шагом в 1ПетаБайт сырых данных с плотной конфигурацией.
Вы можете рассмотреть блок аппаратуры с одним сервером прокси, 2 x 10Gbps коммутаторами, одним управляющим коммутатором и пятью серверами хранения с 60 дисками по 4ТБ каждый (т.е. 240 x 5 = 1.2ПБ).
Принимая во внимание предыдущее замечание о необходимости по крайней мере двух серверов прокси, начальная установка должна иметь 2.4ПБ.
При трех репликах 1.2ПБ сырых данных переводится в 400ТБ используемых данных.
Данный пример не является совершенным, поскольку он может не быть кратным обему стойки, но мы приводим его с точки зрения иллюстрации  
</p>

<h3 id="Ch060207">Шаг 7 - Выбор дополнительного сетевого оборудования</h3>
<p>
Окончательный шаг заключается в выборе балансировщика нагрузки, аппаратуры ускорения SSL и оборудования обеспечения безопасности.
Балансировщик нагрузки требуется в том случае, если у вса есть более одного узла прокси.
Более того, вы должны быть уверены, что балансировщик нагрузки не является бутылочным горлышком производительности.
Аппаратура ускорения SSL требуется в случае, если большая часть сетевого обмена идет в безопасном режиме HTTP (HTTPS) и операции программного обеспечения SSL перегружают серверы прокси.
Наконец, если облако находится в общедоступной сети, требуется оборудование обеспечения безопасности, подобное IPS и IDS.
Аналогично балансировщику нагрузки эти дополнительные элементы оборудования должны иметь достаточную производительность, чтобы соответствовать требованиям общей производительности всех серверов прокси.
Следующий рисунок обозначает дополнительное сетевое оборудование, необходимое вашему кластеру Swift:
	<div style="width: 215px; margin: 1.75em auto;">
	<p style="text-align: center;">
		<img src="i/Pic06-04.jpg"  alt="Дополнительное сетевое оборудование" title="Дополнительное сетевое оборудование" width="215" height="70" style="width: 215px" />
		<span class="image-caption">Дополнительное сетевое оборудование</span>
	</p></div>
</p>

<h3 id="Ch060208">Шаг 8 - выбор шлюза облака</h3>
<p>
Эта часть оборудования является третьм лишним.
Она не требуется для построения кластера OpenStack.
Вместо этого оно необходимо на входе (в случае общедоступного облака) или около приложения (в случае частного облака) в случае, если ваше приложение пока не портировано под REST HTTP API.
В этом случае приложение ожидает традиционную блочную или файловую систему хранения, которая преобразуется такими шлюзами облака.
Шлюз осуществляет трансляцию протокола и интерфейсы с облаком на другой стороне.
Помимо трансляции протокола, шлюзы облака часто добавляют ряд других особенностей, таких как оптимизация WAN, сжатие, исключение дублирования и кодирование.
</p><p>
В то время как большая часть данного раздела имеет отношение к производительности, существуют также другие вопросы для обсуждения и они рассматриваются в следующем разделе.
</p>


<h2 id="Ch0603">Дополнительные критерии выбора</h2>
<p>
В дополнение к предыдущим критериям, следующие элементы должы быть рассмотрены перед окончанием выбора аппаратных средств:<ul>
 <li><strong>Надежность</strong>:
Надежность (долговечность) измеряет достоверность и определяется как 100 процентов минус вероятность потери объекта размером в 1КБайт за один год.
Следовательно, 99.999999999 процентная надежность (проще говоря, как 11 х 9 в данном случае) будет означать, что каждый год вы, согласно статистике, вы теряете один объект если у вас есть 100 миллиардов объектов с размером 1КБайт, или, при наличии 10000 объектов, ожидаете потерб одного объекта каждые 10`000`000 лет.
Вычисление надежности кластера выходит за рамки данной книги, однако выбираемое оборудование должно отвечать вашим требованиям надежности.
Для пользователей, которым требуется высокая надежность предметом рассотрения являются диски корпоративного уровня с низкой плотностью, серверы с дублированной системой охлаждения и электропитаня и т.п.
 <li><strong>Доступность</strong>:
Доступность определяется как процент времени, в течение которого запросы успешно выполняются в кластере.
Доступность в основном влияет на архитектуру сети пользовательского интерфейса с точки зрения наличия единой сети (т.е. одной точки отказа) в противоположность сети с избыточным дублированием.
Как уже упоминалось ранее, сети в данной зоне могут быть сетями с одной точкой отказа, если только ваш ИТ- персонал способен быстро устранять возникающие проблемы. 
 <li><strong>Наработка на отказ</strong>: (Эксплутационная надежность, работоспособность).
Исправность различных компонентов оборудования сильно зависит от вашей стратегии.
Если вы выбираете обслуживание на месте (типичное для больших установок), эксплутационная надежность не является большой проблемой.
Если вы выбираете стратегию ремонт/ обслуживание (что типично для малых и средних реализаций), эксплутационная надежность должна рассматриваться.
Каждое устройство должно подлежать восстановлению или обслуживанию.
Меньший размер системы может также побудить выбор более дорогостоящего оборудования при подборе вентиляторов с резервированием, блоков питания и тому подобного.
Причина заключается в том, что в случае возникновения отказа просто может не оказаться запасных устройств доступных для выбора из кольца Swift. 
 <li><strong>Управляемость</strong>:
Как уже обсуждалось ранее, серверы поступают с самыми разнообразными особенностями когда они подключаются к системам аппаратного управления и связанного с ними программного обеспечения.
Вам следует выбирать сервера с опциями управления, соответствующими вашей ИТ-стратегии.
</ul>
</p>


<h2 id="Ch0604">Стратегия выбора производителя</h2>
<p>
Если вы действительно хотите стать веб- гигантом, вы должны покупать аппаратные средства у оригинальных производителей оборудования (ODM) и прочих производителей стандартного оборудования (либо непосредственно, или через <a href="http://www.mdl.ru"><em>системных интеграторов</em></a>).
Вопросы, на которые вы должны ответить сами себе следующие:
<table style="border: none; align=center;" align="center" cellspacing="0" cellpadding="0"><tbody>
<tr>
 <td width="60%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Вопрос</strong>
 <td width="20%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Да для всех вопросов</strong>
 <td width="20%" style="border-top: thin solid; border-bottom: thin solid; align=center;"><strong>Нет хотя бы для одного вопроса</strong>
</tr><tr>
 <td style="border-bottom: thin solid;">Можете ли вы определить конфигурацию каждого сервера с учетом производительности, надежности, доступности, наработки на отказ и управляемости (в противоположность необходимости помощи инженеров по продажам производителя)?
<br /><br />
  Можете ли вы самостоятельно (т.е., если вы получаете вызов в 2 часа ночи, готовы ли вы определить основные причины происшествия, или вам необходима помощь производителя)?
<br /><br />
  Готовы ли вы принять менее искушенные условия обслуживания, периоды освоения новой продукции, политики прекращения выпуска и прочие темины?
<br /><br />
  Достаточны ли вам минимальные возможности, предоставляемые производителем на оборудование и программное обеспечение?
 <td style="border-bottom: thin solid;">Вы готовы к использованию стандартного оборудования!
 <td style="border-bottom: thin solid;">Вы должны использовать фирменные аппаратные средства.
</tr></tbody></table>
</p>

<h3 id="Ch060401">Фирменное оборудование</h3>
<p>
Если вы выбрали подход оснащения фирменным оборудованием, процесс достаточно прост и включает выдачу запроса на установление цены (RFQ, requets for quotation) вашему любимому производителю серверов, такому как HP, Dell, IBM и Fujitsu или производителю сетевого оборудования такому как Cisco, Juniper и Arista с последующим выбором того, что вам понравилось.
</p>

<h3 id="Ch060402">Стандартное оборудование</h3>
<p>
Если вы идете по этому пути, то существует множество производителей, изикоторых вы можете выбирать - от тайваньских ODM и прочих специалистов по аппаратуре хранения, таких как Xyratex, Sanmina, Mdl.
Возможно, наиболее интересный вариант, это поиск в движении с аппаратурой с открытым исходным кодом под названием <strong>Open Compute Platform</strong> (<strong>OCP</strong>)
</p><p>
Согласно их вебсайту, <a hre="http://www.opencompute.org"><code>http://www.opencompute.org</code></a>, миссия OCP состоит в разработке и предоставлении самых эффективных серверов, систем хранения и оборудования центров данных, разрабатываемых для масштабируемого компьютинга.
Все участники OCP работают с open source.
Множество производителей продают OCP-совместимые аппаратные средства, и подобная совместимость делает несколько более простым для пользователя выбор совместимого оборудования у большого количества произодителей.
</p><p>
Например, OCP Intel Motherboard Hardware v2.0 поддерживает два ЦПУ, четыре канала оперативной памяти DDR3 на ЦПУ, мини- SAS порт для потенциального расширения JBOD, 1Gbps сетевой ввод/вывод и ряд свойств управления аппаратными средствами.
Он также может допускать NIC мезанинную плату PCIe для 10Gbps сетевого ввода/вывода. 
Такой сервер может подойти и для сервера прокси и для сервера хранения (с различными кстановленными компонентами).
<br />
 (<em>Прим. пер.: Также см., например, 
<a href="http://www.mdl.ru/Solutions/MdlCloud/68nodesReferences20140822.xls">Примеры реализации облака на аппаратуре различных вендоров c рекомендованными к продаже ценами производителей</a>.
Другой подход: 
<a href="http://www.mdl.ru/Solutions/Put.htm?Nme=OpenPOWER">OpenPOWER</a></em>).
</p><p>
 Другой пример, OCP OpenVault JBOD является 2U шасси, которое может содержать в себе до 30 дисков.
Это может сделать его удобным спутником для серверов хранения с большой плотностью.
 (<em>Прим. пер.: Также см., например, <a href="http://www.mdl.ru/Solutions/Put.htm?Nme=MdlStorage">СХД на 50 дисков с JBOD до 60 дисков</a></em>).
</p>


<h2 id="Ch0605">Заключение</h2>
<p>
В этой главе мы рассмотрели сложный процесс выбора аппаратных средств для реализации OpenStack Swift и различные компромиссы, которые при этом могут быть сделаны.
В следующей главе мы рассомтрим как проводить тестирование и тонкую настройку кластера Swift.
</p>



<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody><tr>
 <td align="left"><a href="Ch05ru.htm">Глава 5</a></td>
 <td align="center"><a href="ToC.htm">Оглавление</a></td>
 <td align="right"><a href="Ch07ru.htm">Глава 7</a></td>
</tr><tr><td colspan="3" style="border-bottom: thin solid;">&nbsp;</tr>
<tr><td colspan="2" valign="top">Перевод: Copyright ©&nbsp;2014 &nbsp;<img src="/i/mdl-reg.jpg" widht="35" height="12" style="border-style: none;">.<br>
All rights reserved.<br />
Ссылки обязательны (Refs and links obligatory).</td>
<td valign="top" align="right"><em><a href="http://www.mdl.ru">http://www.mdl.ru</a></em></td></tr>
</tbody></table>


<td align="right" valign="top">
<script language="javascript">
WriteABC('GPFS');
//--></script>
</tr>
</tbody></table>
</body>
</html>
