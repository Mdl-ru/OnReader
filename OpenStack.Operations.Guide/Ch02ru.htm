<html>
<head>
   <link rel="icon" href="/i/MdlLogo.gif" type="image/gif">
   <title>Глава 2. Схема контроллера облака. Руководство по эксплуатации OpenStack</title>
   <meta name="Keywords" content="OpenStack, Cloud computing, Swift, RESTful, Object Storage, Ceph, CORS, CNAME lookup, Domain remap, Health check, Rate limiting, Bulk delete, Container qoutas, account qoutas, TempURL, Static Web, Form post, recon, Swift origin server, Bulk archive auto-extraction">
   <meta name="Description" content="Глава 2. Схема контроллера облака. Руководство по эксплуатации OpenStack">
   <meta name="Robots" content="INDEX, FOLLOW">
   <meta name="Author" content="Module-Projects,Ltd">
   <meta name="Copyright" content="Copyright 1998..2014 Module-Projects,Ltd">
   <meta http-equiv="Pragma" content="no-cache">
<script language="javascript" src="/css/v.0/mdlcss.js"></script>
<style type="text/css" media="screen, print">@import url("i/global-20140610.css");</style>
<script language="javascript" src="http://www.mdl.ru/usd.js"></script>
	<script language="javascript" src="http://www.mdl.ru/js/common.js"></script>
	<script language="javascript" src="http://www.mdl.ru/Solutions/ABC.js"></script>
</head>
<body>

<table class="bg_White" width="1024" align="center" valign="top" border="0" cellpadding="0" cellspacing="0"><tbody>
<tr>
<td>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td width="150" valign="top" align="center"><img src="http://www.mdl.ru/RMC9.jpg" border=0 /></td>
<td width="724" valign="bottom" align="center">
<a class="item-t" href="http://www.mdl.ru"><img src="http://www.mdl.ru/i/MdlBigLogo.gif" border="0"></a><br/>
<a class="item-t" href="http://www.mdl.ru">С 1991 года на компьютерном рынке России</a>
</td>
<td align="center" valign="bottom">
<a class="item-t" href="javascript:tocall()" onmouseover="this.href=mail"><img src="http://www.mdl.ru/i/9563499.gif" border="0" alt="e-mail" /><br/><br/>т.: 676 0965, 676 0396<br/>Москва, Сосинская ул. 43, <br/>м. Волгоградский проспект</a>
</td>
</tr>
<tr>
<td class="big_16y" colspan="3" align="center"><a href="index.htm">Руководство по эксплуатации OpenStack</a></td>
</tr>
<tr><td colspan="2">


<h2 align="right">ГЛАВА 2</h2>
<hr />
<h1 id="Chapter_02" align="right">Схема контроллера облака</h1>

<em>Пользуйтесь переводом <strong>существенно переработанной и дополенной <a href="http://onreader.mdl.ru/cloud_controller_design.html">2й редакции</a></strong> (12-дек-2014),<br />
находящейся теперь и в режиме <a href="http://docs.openstack.org/openstack-ops/content/cloud_controller_design.html">постоянно обновляемой документации</a> <br />
(последняя пока доступна только на англ.яз.).</em><br />
<p></p>

<p>OpenStack разработан для массивной горизонтальной масштабируемости, которая делает возможной широкое 
распределение всех служб.
Тем не менее, для упрощения данного руководства мы решили обсудить службы более центральной природы с 
использованием концепции единого <em>контроллера облака</em>.</p>

<p>Для получения более подробной информации об общей архитектуре отсылаем вас к 
<a class="red-heading" href="Ch07ru.htm">Главе 7</a>.</p>

<p>Как описывается в данном руководстве, контроллер облака является отдельным узлом, который поддерживает 
базы данных, службу очереди сообщений, службу аутентификации и авторизации, службу управления образами, 
инструментальную панель пользователей, а также API конечных точек 
(<span class="red-heading">API endpoint</span>). </p>

<p>Контроллер облака обеспечивает централизованную систему управления для развертывания OpenStack 
с многими узлами.
Обычно контроллер облака управляет аутентификацией и отсылкой сообщений всем системам с использованием  
очереди сообщений.</p>

<p>В нашем примере контроллер облака имеет коллекцию компонентов nova-*, которая представляет глобальное 
состояние облака, общается со службами, такими как аутентификация, обработка информации об облаке в 
базе данных, с помощью очередей устанавливает связь со всеми вычислительными узлами и работающими 
устройствами (<span class="red-heading">worker</span>) систем хранения, а также обеспечивает API.
 Каждая работающая на выделенном контроллере облака служба может быть разбита на отдельные узлы для 
 масштабируемости или доступности.</p>

<h2 id="Ch0201">Анализ аппаратных средств</h2>

<p>Аппаратные средства контроллера облака могут быть теми же, что и у вычислительных узлов, 
  хотя, возможно, в дальнейшем вы пожелаете определять спецификацию на основе размера и типа облака, 
  с которым вы работаете.</p>

<p>Кроме того, можно использовать виртуальные машины для всех или некоторых из услуг, которыми управляет 
контроллер облака, например очередями сообщений.
 В этом руководстве мы предполагаем, что все службы работают непосредственно в контроллере облака.</p>

<ul>Чтобы установить правильный размер сервера и определить, можно ли выполнить виртуализацию любой 
его части, вы должны оценить:<li>
Число предполагаемых к запуску экземпляров<li>
Число имеющихся у вас вычислительных узлов<li>
Число пользователей, которые будут иметь доступ к службам вычислений или хранения<li>
Будут ли пользователи взаимодействовать с вашим облаком через REST API или через инструментальную панель<li>
Будут ли пользователи выполнять аутентификацию во внешней системе (такой, как LDAP или 
<span  class="red-heading">Active Directory</span>)<li>
насколько продолжительной будет работа отдельных экземпляров</ul>

<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr><td class="big_16h">Анализ</td>
	<td class="big_16h">Результат</td></tr>
<tr><td>Как много экземпляров будет работать одновременно?</td>
	<td>Соответствующий размер вашего сервера базы данных,
	а также, если отчет сообщит о необходимости одновременного наличия многих экземпляров, масштабирование 
	за пределы контроллера облака и планирование вычислительной мощности для запуска нового экземпляра.</td></tr>

<tr><td>Сколько вычислительных узлов работает одновременно?</td>
	<td>Убедитесь, что ваши очереди сообщений успешно обрабатывают запросы и имеют соответствующий размер.</td></tr>

<tr><td>Как много пользователей будут иметь API доступ?</td>
	<td>Если много пользователей будет делать большое количество запросов, убедитесь, что нагружаемые 
	ЦПУ контроллера облака смогут их обработать.</td></tr>

<tr><td>Как много пользователей будет иметь доступ к <span  class="red-heading">инструментальной панели</span>?</td>
	<td>Инструментальная панель выполняет много запросов, даже больше, чем API доступ, 
	следовательно, добавьте еще больше ЦПУ, если ваша инструментальная панель выступает в роли 
	главного инструмента для ваших пользователей.</td></tr>

<tr><td>Сколько nova-api служб вы предполагаете использовать в своем облаке?</td>
	<td>Вы должны подогнать контроллер по размеру ядер под службы.</td></tr>

<tr><td>Как продолжительно работает одна служба?</td>
	<td>Запуск и останов служб запрашивается на вычислительном узле, однако, поскольку существуют 
	потребности очередей API и планировщиков, нагрузка также меняется на узле контроллера.</td></tr>

<tr><td>Выполняется ли вовне верификация вашей системы аутентификации?</td>
	<td>Убедитесь в достаточности коммуникаций между контроллером облака и внешней системы  аутентификации 
	и мощности ЦПУ контроллера облака для обработки запросов.</td></tr>
</tbody></table>


<h2 id="Ch0202">Разделение служб</h2>

<p>Хотя наш пример содержит все центральные службы в одном месте, возможно, хорошей идеей будет 
разнесение служб на различные физические сервера.
 Ниже мы приводим список наблюдавшихся нами сценариев и их обоснования.</p>

<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr><td>Запуск серверов glance-* на сервере swift-proxy</td>
	<td>Подобная реализация получает достаточную дополнительную поддержку ввода/вывода прокси- сервера 
	хранилища объектов (Object Storage proxy), к тому же часть Glance, отвечающая за доставку образов 
	получает преимущества от физического размещения на аппаратных средствах хранилища объектов 
	(Object Storage) и, следовательно, наличия хороших коммуникаций с ним.</td></tr>

<tr><td>Запуск центрального выделенного сервера базы данных</td>
	<td>Такая реализация имеет центральный выделенный сервер, обеспечивающий базами данных все службы. 
	Это упрощает операции по изоляции обновлений сервера баз данных, а также позволяет упростить 
	создание вторичных серверов баз данных для отказоустойчивости.</td></tr>

<tr><td>Запуск одной VM на службу</td>
	<td>Подобное развертывание выполняет центральные службы на наборе серверов с запущенным KVM. 
	Выделенная VM создавалась для каждой службы (планировщик nova, rabbitmq, база данных и т.д.). 
	Это помогает развертыванию при масштабировании, поскольку делает возможной настройку ресурсов 
	для каждой виртуальной машины на основе полученной ими нагрузки (нечто не очень понятное в 
	процессе установки).</td></tr>

<tr><td>Использование внешнего балансировщика нагрузки</td>
	<td>Подобная реализация использует дорогостоящее оборудование для балансировщика нагрузки в 
	своей организации.
	В ней работает много серверов <code>nova-api</code> и <code>swift-proxy</code> на различных 
	физических серверах и используется балансировщик нагрузки для переключения между ними.</td></tr>
</tbody></table>

<p>Один из выборов который всегда возникает, использовать виртуализацию или нет. 
Некоторые службы, такие как вычислители nova, а также серверы прокси и объектов swift не подлежат 
виртуализации.
Однако, серверы управления часто могут быть успешно виртуализованы -падение производительности часто 
может быть компенсировано простым запуском большего числа служб.</p>

<h2 id="Ch0203">Базы данных</h2>

<p>Большинство центральных служб OpenStack Compute, а в нстоящее время и узлы вычислений nova 
используют базу данных для информации о текущих соединениях.
 Потеря этой возможности ведет к ошибкам.
 Как результат, мы рекомендуем вам использовать кластер баз данных в некоем виде для придания 
 отказоустойчивости.</p>

<h2 id="Ch0204">Очередь сообщений</h2>

<p>Большинство служб OpenStack Compute общаются друг с другом с использованием очередей сообщений. 
Обычно при отказе очереди сообщений или возникновении ее недоступности кластер медленно деградирует и 
полностью останавливается в состоянии &quot;доступен только для чтения&quot; с зависшей информацией в 
точке, в которой было отправлено последнее сообщение. 
Соответственно, мы рекомендуем, чтобы ваша очередь сообщений кластера - и RabbitMQ имели встроенный 
механизм решать это.</p>

<h2 id="Ch0205">Интерфейс прикладного программирования (API)</h2>

<p>Весь открытый доступ, будь то прямой доступ, доступ с использованием командной строки или через 
инструментальную панель на основе веб- интерфейса использует службу API.
 Справочные материалы по API вы найдете по адресу 
 <span class="red-heading">http://api.openstack.org/</span>.</p>

<p>Вы можете выбрать между Amazon EC2 совместимым API или только OpenStack API.
 Одна из проблем, которая может возникнуть при работе сразу с двумя API, заключается в несовместимости 
 при обращении к образам и экземплярам.
 Например, EC2 API обращается к экземпляру с использованием шестнадцатиричного ID, в то время как 
 OpenStack API использует имена и числа.
 Аналогично, EC2 API, как правило, полагается на псевдонимы DNS для соединения с виртуальными машинами, 
 в отличие от OpenStack, который обычно перечисляет IP адреса.
</p>

<p>Если OpenStack не установлен надлежащим образом, элементарно получить ситуацию, при которой 
пользователи не смогут связаться со своими экземплярами просто из-за неправильных псевдонимов DNS.
 Несмотря на это, EC2- совместимость может помочь пользователям мигрировать на ваше облако.
 Как и в случае с базой данных, и с очередями сообщений, наличие более одного 
 <span class="red-heading">API сервера</span> может оказаться полезным.
 Для достижения высокой доступности служб <code>Nova-api</code> могут быть использованы 
 традиционные методы балансировки нагрузки HTTP.</p>

<h2 id="Ch0206">Расширения</h2>

<p><span class="red-heading">Спецификация API</span>  (http://docs.openstack.org/api/api-specs.html) 
определяет основные операции, совместимости и типы носителей OpenStack API. 
Клиент всегда может зависеть от доступности этого ядра API и разработчиков реализации всегда требуется 
поддерживать его в полном объеме.
Требование строгого соответствия ядра API дает возможность клиентам полагаться на минимальный уровень 
функциональности при взаимодействии с различными реализациями одного и того же API.</p>

<p>OpenStack Compute API является расширяемым.
Расширение добавляет возможности к API поверх уже определенных в ядре.
Внедрение новых функций, типов MIME, действий, состояний, заголовков, параметров и ресурсов 
могут быть выполнены с помощью расширений в ядре API.
Это позволяет введение новых функций в API, не требуя изменения версии и позволяет введения
 ниши функциональности конкретного производителя.</p>

<h2 id="Ch0207">Планировщик</h2>

<p>Подгонка различных по размеру виртуальных машин (на разный вкус) к физическим nova- вычислительным 
узлам различных размеров является слжной задачей - обычно изучаемая в теории вычислительной техники как 
проблема упаковки.</p>

<p>Вы можете использовать различные методы для решения этой проблемы, один из которых заключается в 
наличии линейно масштабируемых предпочтительных размеров, который будет пропорционален долям физической 
производительности узла, хотя решение данной проблемы выходит за пределы данной книги.  
Для поддержки вашего выбора планирования OpenStack Compute предоставляет несколько драйверов планировщиков, 
полное обсуждение которых можно найти в <span class="red-heading">справочном руководстве</span> 
(http://docs.openstack.org/folsom/openstack-compute/admin/content/ch_scheduling.html).</p>

<p>Для целей обеспечения высокой доступности, а также для инсталляций очень больших размеров или 
установок с часто изменяемыми расписаниями вы должны рассмотреть возможность запуска нескольких служб 
планирования nova. 
Никаких специальных средств балансировки нагрузки для планировщиков nova, поскольку их взаимодействие 
полностью основано на очередях сообщений.</p>

<h2 id="Ch0208">Образы</h2>

<p>Служба доставки и каталога образов OpenStack (OpenStack Image Catalog and Delivery) состоит из двух 
частей -  <code>glance-api</code> и <code>glance-registry</code>. 
Первая отвечает за доставку образов и их использование вычислительными узлами для загрузки образов с 
серверов баз данных. 
Последняя поддерживает метаданные, связанные с образами виртуальных машин и необходимыми серверами 
хранения.</p>

<ul>Часть <code>glance-api</code> является уровнем абстракции, допускающим выбор сервера образов (back-end). 
В настоящее время, он поддерживает:

<li>Хранилище объектов OpenStack. Позволяет вам хранить образы в качестве объектов.
<li>Файловая система. Использует любые существующие файловые системы для хранения образов в виде файлов.
<li>S3. Позволяет вам осуществлять выборку образов из Amazon S3.
<li>HTTP. Позволяет вам осуществлять выборку образов с веб- сервера.
  При использовании данного режима вы не можете записывать образы.
</ul>

<p>Если у вас есть служба хранения объектов OpenStack (OpenStack Object Storage), мы рекомендуем 
использовать ее как масштабируемое пространство для хранения ваших образов. 
Вы также можете использовать файловую систему с достаточной производительностью или Amazon S3 - если 
вы не нуждаетесь в возможности загрузки новых образов с помощью OpenStack.</p>

<h2 id="Ch0209">Инструментальная доска</h2>

<p>Инструментальная доска OpenStack (OpenStack Dashboard) реализуется в виде веб приложения Python 
запущенного на <span class="red-heading">Apache</span> <code>httpd</code>. 
Таким образом, вы можете рассматривать его аналогичным любому другому веб- приложению, при условии, 
что ему доступны API серверы (включая его административные конечные приложения) с использованием сети.
</p>

<h2 id="Ch0210">Аутентификация и авторизация</h2>

<p>Понятия, поддерживающие аутентификацию и авторизацию OpenStack происходят из хорошо изученных и 
широко используемых систем аналогичной природы. 
Пользователи имеют полномочия, которые они могут использовать для аутентификации и они могут быть 
членами одной или более групп (называемыми проектами или владельцами, что взаимозаменяемо).</p>

<p>Например, администратор облака мог бы перечислить список всех доступных экземпляров в данном облаке, 
в то время как пользователь может видеть только те, которые входят в его текущую группу. 
Могут использоваться связанные с данным проектом квотируемые ресурсы, такие как число ядер, дисковое 
пространство и так далее. </p>

<p> Служба идентификации OpenStack (OpenStack Identity Service, Keystone) является пунктом, который 
обеспечивает решения об аутентификации и информацию об атрибутах пользователя, которые затем используются 
другими службами OpenStack для выполнения авторизации.
 Политики устанавливаются в файле policy.json. 
 Для информации об их настройке, см. <a class="red-heading" href="Ch09ru.htm">Главу 9</a>.</p>

<ul>Служба идентификации поддерживает различные плагины для серверных решений аутентификации и хранения 
информации.
Они варьируются начиная с простого варианта хранения до внешних систем и в настоящее время включают в себя:

<li>Хранимые в памяти значения ключей
<li>база данных SQL
<li>PAM
<li>LDAP
</ul>

<p>Многие реализации используют базу данных SQL, однако LDAP также является популярным выбором для решений, 
которые должны быть интегрированы с существующей инфраструктурой.</p>

<h2 id="Ch0211">Анализ сети</h2>

<p>Поскольку контроллер облака поддерживает очень много различных служб, он должен быть в состоянии 
обрабатывать бьющий по нему объем трафика. 
Например, если вы решите принимать на контроллере облака службу образов OpenStack (OpenStack Imaging), 
контроллер облака должен обеспечивать передачу этих образов с приемлемой скоростью.</p>

<p>В качестве другого примера, если вы решите использовать сеть с одиночным хостом, в которой контроллер 
облака является сетевым шлюзом для всех экземпляров, контроллер облака должен поддерживать общий объем 
трафика, происходящего между вашим облаком и общедоступным интернетом.</p>

<p>Мы рекомендуем вам использовать быстрые NIC, например, 10 GB.
 Вы также можете выбрать использование двух сетевых карт 10 GB  и объединить их вместе.
 Хотя вы не можете получить всю объединенную скорость 20 GB, различные потоки используют различные 
 сетевые адаптеры. 
 Например, если контроллер облака передает два образа, каждый образ использует отличную сетевую карту и 
 получает полную пропускную способность 10 GB.
</p>

<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody><tr>
 <td align="left"><a href="Ch01ru.htm">Глава 1</a></td>
 <td align="center"><a href="index.htm">Оглавление</a></td>
 <td align="right"><a href="Ch03ru.htm">Глава 3</a></td>
</tr><tr><td colspan="3" style="border-bottom: thin solid;">&nbsp;</tr>
<tr><td colspan="2" valign="top">Перевод: Copyright ©&nbsp;2014 &nbsp;<img src="/i/mdl-reg.jpg" widht="35" height="12" style="border-style: none;">.<br>
All rights reserved.<br />
Ссылки обязательны (Refs and links obligatory).</td>
<td valign="top" align="right"><em><a href="http://www.mdl.ru">http://www.mdl.ru</a></em></td></tr>
</tbody></table>


<td align="right" valign="top">
<script language="javascript">
WriteABC('GPFS');
//--></script>
</tr>
</tbody></table>
</tbody></table>
</body>
</html>
