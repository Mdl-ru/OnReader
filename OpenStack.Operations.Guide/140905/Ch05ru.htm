<html>
<head>
   <link rel="icon" href="/i/MdlLogo.gif" type="image/gif">
   <title>Глава 5. Решения систем хранения. Руководство по эксплуатации OpenStack.</title>
   <meta name="Keywords" content="OpenStack, Cloud computing">
   <meta name="Description" content="Глава 5. Решения систем хранения. Руководство по эксплуатации OpenStack.">
   <meta name="Robots" content="INDEX, FOLLOW">
   <meta name="Author" content="Module-Projects,Ltd">
   <meta name="Copyright" content="Copyright 1998..2014 Module-Projects,Ltd">
   <meta http-equiv="Pragma" content="no-cache">
<script language="javascript" src="/css/v.0/mdlcss.js"></script>
<style type="text/css" media="screen, print">@import url("i/global-20140610.css");</style>
<script language="javascript" src="http://www.mdl.ru/usd.js"></script>
<script language="javascript" src="/js/common.js"></script>
</head>
<body>

<table class="bg_White" width="800" align="center" valign="top" border="0" cellpadding="0" cellspacing="0"><tbody>
<tr>
<td>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td width="150" valign="top" align="center"><img src="http://www.mdl.ru/RMC9.jpg" border=0 /></td>
<td width="500" valign="bottom" align="center">
<a class="item-t" href="http://www.mdl.ru"><img src="http://www.mdl.ru/i/MdlBigLogo.gif" border="0"></a><br/>
<a class="item-t" href="http://www.mdl.ru">С 1991 года на компьютерном рынке России</a>
</td>
<td align="center" valign="bottom">
<a class="item-t" href="javascript:tocall()" onmouseover="this.href=mail"><img src="http://www.mdl.ru/i/9563499.gif" border="0" alt="e-mail" /><br/><br/>т.: 676 0965, 676 0396<br/>Москва, Сосинская ул. 43, <br/>м. Волгоградский проспект</a>
</td>
</tr>
<tr>
<td class="big_16y" colspan="3" align="center"><a href="index.htm">Руководство по эксплуатации OpenStack</a></td>
</tr>
</table>


<h2 align="right">ГЛАВА 5</h2>
<hr />
<h1 id="Chapter_05" align="right">Решения систем хранения</h1>

<p>Хранилища находятся во многих частях стека OpenStack и различные типы могут привести к путанице даже у искушенных иженеров, работающих с облаками.
 В данном разделе рассматриваются параметры устройств постоянного хранения, которые вы можете настраивать в вашем облаке.</p>

<h2 id="Ch0501">Концепции хранилища OpenStack</h2>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td class="big_16h">&nbsp;</td>
<td class="big_16h">Временное хранение</td>
<td class="big_16h">Блочное хранение</td>
<td class="big_16h">Хранение объектов</td>
</tr><tr>
<td>Используется для ...</td>
<td>Работы операционной системы и оперативного пространства</td>
<td>Добавляет дополнительные устройства постоянного хранения виртуальным машинам (ВМ)</td>
<td>Хранит данные, включающие образы ВМ</td>
</tr><tr>
<td>Доступ через ...</td>
<td>Файловую систему</td>
<td><span class="red-heading">Блочные устройства</span>, которые могут разбиваться на разделы, форматироваться и монтироваться (такие как /dev/vdc)</td>
<td>REST API</td>
</tr><tr>
<td>Доступны из ...</td>
<td>В пределах ВМ</td>
<td>В пределах ВМ</td>
<td>Где угодно</td>
</tr><tr>
<td>Управляется ...</td>
<td>OpenStack Compute (Nova)</td>
<td>OpenStack Block Storage (Cinder)</td>
<td>OpenStack Object Storage (Swift)</td>
</tr><tr>
<td>Существует до ...</td>
<td>Останова ВМ</td>
<td>Удаляется пользователем</td>
<td>Удаляется пользователем</td>
</tr><tr>
<td>Размеры устанавливаются ...</td>
<td>Установками администратором настроек размеров, известных как <em>предпочтения</em> (<em>flavor</em>)</td>
<td>Описываются пользователем в начальном запросе</td>
<td>Размером доступного хранилища</td>
</tr><tr>
<td>Пример типичного применения ...</td>
<td>10ГБ первый диск, 30ГБ второй диск</td>
<td>1ТБ диск</td>
<td>10-ки ТБ-ов наборов данных СХД</td>
</tr>
</tbody></table>

<p>Если вы разворачиваете только OpenStack Compute Service (nova), вашим пользователям по умолчанию не будут доступны никакие устройства постоянного хранения.
 Диски связанные с ВМ являются "эфемерными" (ephemeral, временными), что означает (с точки зрения пользователя), что они эффективно исчезают по завершению сеанса.
 Вы должны определить,какой тип устройств постоянного хранения вы хотите предоставить своим пользователям.</p>
<p>На сегодняшний день OpenStack вявном видеподдерживает два вида устройств непрерывного хранения: <em>хранилища объектов</em> (<em>object storage</em>) и <em>блочные хранилища</em> (<em>block storage</em>)</p>

<h3 id="Ch050102">Хранилища объектов</h3>
<p>При помощи хранилища объектов пользователи получают доступ к двоичным объектам через REST API.
 Вы можете быть знакомы с Amazon S3, который является широко известным примером системы хранения объектов.
 Если у ваших пользователей существует необходимость архивирования или управления большими наборами данных, значит вы хотите предоставитьим хранилище объектов.
 Помимо этого, OpenStack может хранитьобразы ваших виртуальных машин (ВМ) внутри системыхранения объектов, как альтернатива хранению образов в файловой системе.</p>

<h3 id="Ch050103">Блочные хранилища</h3>
<p>Блочные хранилища (иногла называемые томами хранения) выставляют для пользователя блочные устройства.
 Пользователи взаимодействуют с блочными хранилищами путем присоединения томов ксвоим работающим экземплярам.</p>
<p>Эти тома являются постоянными: они могут быть отделены от одного экземпляра и повторно подключены к другому, причем данные остаются не поврежденными.
 Блочные хранилища реализуются в OpenStack проектом OpenStack Block Storage (Cinder), который поддерживает множество серверов хранения в форме устройств (drivers).
 Выбранный вами сервер хранения должен поддерживаться устройством блочного хранилища.</p>
<p>Большинство устройств блочного хранилища позволяют экземпляру осуществлять прямой доступ к аппаратным средствам утройства блочной СХД.
 Это позволяет увеличить общее чтение/запись системы ввода/вывода.</p>
<p>В редакции Folsom начала осуществляться поддержка использования файлов в качестве томов.
 Изначально она стартовала как драйвер ссылок для поддержки NFS в Cinder.
 Начиная с редакции Grizzly, поддержка была расширена до полномасштабного драйвера NFS, а также дополнена драйвером GlusterFS.<br />
 <em>Прим. пер.: Havana (17 октября 2013) и Icehouse (17 апреля 2014) существенно расширили список поддерживаемых файловых систем и СХД.
 Например, начиная с Havana осуществляется поддержка <strong><a href="http://www.mdl.ru/Solutions/Put.htm?Nme=GPFS">GPFS NSD</a></strong>.
 Полный действующий список поддерживаемых файловых систем и систем хранения данных доступен по адресу <a href="https://wiki.openstack.org/wiki/CinderSupportMatrix">https://wiki.openstack.org/wiki/CinderSupportMatrix</a></em></p>
<p>Работа этих драйверов слегка отличается от обычных драйверов "блочных" систем хранения.
 В файловых системах NFS и GlusterFS создается некий файл, который затем отображается в виде тома в экземпляре.
 Такое отображение/ трансляция аналогично использованию OpenStack-ом в QEMU виртуальных маших на основе файлов, хранящихся в <code>/var/lib/nova/instances</code></p>

<h3 id="Ch050104">Хранилища файлового уровня</h3>
<p>Применяя хранилще файлового уровня, пользователи получают доступ хранимым данным с помощью файловой системы  операционной системы.
 Большинство пользователей, если они ранее использовали решения сетевого хранения данных, они встречались с такой формой сетевого хранения.
 В мире Unix наиболее распространенной формой является NFS.
 В мире Windows, наиболее распространенная форма называется CIFS (ранее SMB).</p>
<p>Облака OpenStack не предоставляют конечным пользователям хранилища файлового уровня.
 Тем не менее, когда вы проектируете свое облако, важно предусмотреть возможность хранения на файловом уровне для хранения экземпляров в <code>/var/lib/nova/instances</code> при проектировании облака, так как вы должны иметь совместно используемую файловую систему, если вы хотите использовать миграцию в режиме онлайн.</p>

<h2 id="Ch0502">Выбор сервера хранения</h2>
<p>В общем случае, когда вы выбираете <span class="red-heading">сервер хранения</span>, вы должны ответить на следующие вопросы:</p>
<ul>
 <li>Требуются ли моим пользователям блочные хранилища?
 <li>Требуются ли моим пользователям хранилища объектов?
 <li>Требуются ли мне миграция в режиме реального времени?
 <li>Должны ли мои устройства постоянного хранения содержаться в моих вычислительных узлах, или я должен использовать внешнюю СХД?
 <li>На какое количество пластин устройств хранения я могу рассчитывать?
   Даст ли лучший результат ввода/вывода большее количество шпинделей, несмотря на сетевой доступ?
 <li>Какой результат является моей целью в выборе наилучшего соотношения стоимость- производительность? 
 <li>Как я должен управлять эксплуатацией систем хранения?
 <li>Насколько резервируемой и распределенной является система хранения?
  Что произойдет в случае отказа узла хранения?
  В какой степени это может смягчить сценарии угрозы потери моих данных? 
</ul> 
<p>Для развертывания вашей системы хранения с использованием серийных аппаратных средств, вы можете использовать ряд пакетов соткрытым исходным кодом, что демонстрируется в следующей таблице:</p>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td class="big_16h">&nbsp;</td>
<td class="big_16h">Объекты</td>
<td class="big_16h">Блоки</td>
<td class="big_16h">Файловый уровень<sup>*</sup><br/> (поддержка миграции в реальном времени)</td>
</tr><tr>
<td>Swift</td>
<td class="numClmn">V</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">&nbsp;</td>
</tr><tr>
<td>LVM</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">V</td>
<td class="numClmn">&nbsp;</td>
</tr><tr>
<td>Ceph</td>
<td class="numClmn">V</td>
<td class="numClmn">V</td>
<td class="numClmn">экспериментально</td>
</tr><tr>
<td>Gluster</td>
<td class="numClmn">V</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">V</td>
</tr><tr>
<td>NFS</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">V</td>
<td class="numClmn">V</td>
</tr><tr>
<td>ZFS</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">V</td>
<td class="numClmn">&nbsp;</td>
</tr><tr>
<td>Sheepdog</td>
<td class="numClmn">&nbsp;</td>
<td class="numClmn">экспериментально</td>
<td class="numClmn">&nbsp;</td>
</tr><tr>
</tbody></table>

<p><sup>*</sup>Данный список решений хранения совместно используемых данных на файловом уровне с открытым исходным кодом не является исчерпывающим, существуют идругие решения с открытым исходным кодом (MooseFS).
 Возможно, ваша организация уже развернула решение хранения совместно используемых данных файлового уровня, которое можно использовать.</p>
<p>Помимо существующих технологий с открытым исходным кодом, существует ряд фирменных решений, официально поддерживаются OpenStack Block Storage.
 Они предлагаются в следующем списке поставщиков:</p>
<ul>
 <li>IBM (Storwize family/SVC, XIV)
 <li>NetApp
 <li>Nexenta
 <li>SolidFire
</ul>
<p>Вы можете найти матрицу функциональных возможностей, предоставляемых всеми поддерживаемыми драйверами блочных хранилищ, на  <span class="red-heading">OpenStack вики</span> (<a href="https://wiki.openstack.org/wiki/CinderSupportMatrix">https://wiki.openstack.org/wiki/CinderSupportMatrix</a>).</p>
<p>Кроме того, вы должны решить: хотите ли вы поддерживать в своем облаке хранилище объектов.
 Двумя общими случаями использования для поддержки хранилищ объектов в компьютерномоблаке являются:</p>
<ul>
 <li>Чтобы снабдить пользователей механизмом устройств постоянного хранения 
 <li>Для масштабируемых, надежных систем хранения для образов виртуальных машин
</ul>

<h3 id="Ch050202">Технологии серийно выпускаемых серверов хранения</h3>
<p>В данном разделе приводится общий анализ различий между различными технологиями серийных серверов хранения.</p>

<ul>
 <li><strong>OpenStack Object Storage (Swift).</strong> Официальная хранилища объекта OpenStack. 
  Это зрелая технология, которая использовалась в течение ряда лет в практической деятельности Rackspace как технология, лежащая в основе Rackspace Cloud Files. 
  Поскольку технология обладает высокой масштабируемостью, она хорошо подходит для управления петабайтами хранениимых данных.
  Преимуществами хранилищ объектов OpenStack являются лучшая интеграция с OpenStack (интегрируется с OpenStack Identity, работает с интерфейсом инструментальной панели OpenStack), а также лучшая поддержка многократного развертывания центров обработки данных за счет поддержки асинхронной эпизодических непротиворечивых репликаций.
  <br />
  Поэтому, если вы в конечном итоге планируете раccредоточить свой кластер хранения в нескольких центрах обработки данных и если вам нужно унифицировать учетные данные пользователей, как для вычислений, так и для объектов хранения, или, если вы хотите управлять своими храненимые объекты через инструментальную панель OpenStack, вы должны рассмотреть OpenStack Object Storage.
  Более подробную информацию о OpenStack Object Storage можно найти в следующем разделе.

 <li><strong>Ceph.</strong> Масштабируемое решение для хранения, которое реплицирует данные между серийными узлами хранения.
  Ceph первоначально был разработан одним из основателей DreamHost и в настоящее время используется в данном продукте.
  <br />
  Ceph был разработан, чтобы редставить конечному пользователю различные типы интерфейсов хранилищ данных: он поддерживает хранение объектов, блочные хранилища и интерфейсы файловой системы, хотя интерфейс файловой системы пока не считается готовым продуктом.
  Ceph поддерживает тот же API, что и Swift, для хранения объекта, может использоваться в качестве сервера хранения для блочных хранилищ Cinder, а также сервером хранения для Glance образов.
  Ceph поддерживает "Слабую инициализаию" ("Thin Provisioning"), реализованную копированием при записи.
  <br />
  Это может быть полезно при загрузке с тома, поскольку новый том может быть подготовлен очень быстро.
  Ceph также поддерживает аутентификацию на основе замкового камня (keystone) (начиная с версии 0.56), поэтому он может быть бесшовным обменником в реализации OpenStack Swift по умолчанию.
  Преимущество Ceph в том, что он предоставляет администратору более тонкое управление стратегий распределения данных и репликации, позволяет консолидировать ваши хранилище объектов и блочное хранилище, допускает очень быструю инициализацию загрузки с томов при использовании слабой инициализации, и поддерживает интерфейс распределенной файловой системы, хотя этот интерфейс <span class="red-heading">пока не рекомендуется</span> (<em>Прим. пер.: в текущих версиях файловая система Ceph <strong><span class="red-heading">является штатным компонентом</span></strong>, см. например, <a href="http://ceph.com/docs/master/cephfs/">http://ceph.com/docs/master/cephfs/</a>, что явно отменяет данную рекомендацию!</em>) для использования в развертывании производства в рамках проекта Ceph.
  <br />
  Если вы хотите управлять хранилищем объектов и блочным храненилищем в рамках одной системы, или если вы хотите поддержать быструю загрузку-с-тома, вы должны рассмотреть вариант использования Ceph.

 <li><strong>Gluster.</strong> Распределенная, совместно используемая файловая система. 
  По состоянию на момент выхода версии Gluster 3.3, вы можете использовать Gluster для консолидации хранилища объектов и хранилища файлов в единое решение хранения файлов и объектов, которое называется Gluster UFO.
  Gluster UFO использует настраиваемую версию Swift, которая, в свою очередь, использует Gluster в качестве сервера хранения. 
  Основное преимущество использования Gluster UFO над обычным Swift, прояавляется, когда вы также хотите поддерживать распределенную файловую систему, либо для поддерживать миграцию совместно используемого хранилища в реальном времени или предоставить ее в качестве отдельной услуги для ваших конечных пользователей.
  Если вы хотите управлять хранилищами объектов и файлов в пределах одной системы, вы должны рассмотреть использование Gluster UFO.

 <li><strong>LVM.</strong> Cистема управления логическим томами (Logical Volume Manager) на основе Linux, которая обеспечивает уровень абстракции поверх физических дисков, чтобы представлять логические тома операционной системе.
  Сервер LVM (Logical Volume Manager) реализует блочные храненилища как LVM логических разделов.
  На каждом хосте, где размещается блочное храненилще, администратор должен сначала создать группу томов, выделенную для томов блочног храненилища.
  Блоки создаются из логических томов LVM.
  <br/>
  <img src="i/Tip.jpg" alt="Tip"> LVM не обеспечивает никакой репликации.
  Как правило, администраторы настраивают RAID на узлах, использующих LVM как блочное хранилище, для защиты от сбоев отдельных жестких дисков.
  Однако, RAID не защищает от отказа всего хоста.
  <br />
  Драйвер iSCSI Solaris для OpenStack Block Storage реализует блоки как примитивы ZFS.
  ZFS является файловой системой, которая также имеет функциональность диспетчера томов.
  Это отличает ее от системы Linux, в которой существет разделение диспетчера томов (LVM) и файловых систем (таких как, ext3, ext4, XFS, btrfs).
  ZFS имеет ряд преимуществ по сравнению с ext4, в том числе улучшенную проверку целостности данных.

 <li><strong>Sheepdog.</strong> Недавний проект, который призван обеспечить блочные храненилища для экземпляров на основе KVM при поддержке репликации между хостами.
  Мы не рекомендуем Sheepdog для производства облака, поскольку ее авторы в NTT Labs рассматривают Sheepdog в качестве экспериментальной технологии. 

 <li><em>Прим. пер.: <strong><a href="http://www.mdl.ru/Solutions/Put.htm?Nme=GPFS">GPFS</a></strong> - коммерческая файловая система, поддерживаемая OpenStack начиная с редакции Havana (17 октября 2013).
  Данная файловая система предоставляет поддержку всех типов хранилищ при наличии собственных систем обеспечения надежности, отказоустойчивости, масштабируемости хранимых объемов и пропускной способности, при совместимости с большим спектром серийно выпускаемого оборудования различных производителей и гарантированном сопровождении и поддержке производителем самой GPFS (<a href="http://www-03.ibm.com/systems/platformcomputing/products/gpfs/index.html">IBM</a>)
  При наличии требований уровня критических приложений или доступности бюжетных средств, рекомендуем обратить внимание на данный продукт.
  </em>
</ul>

<h2 id="Ch0503">Замечания по OpenStack Object Storage</h3>
<p>OpenStack Object Storage обеспечивает высоко масштабируемую и надежную систему хранения, ослабляя некоторые из ограничений традиционных файловых систем.
 При проектировании и закупке для такого кластера, важно понять некоторые ключевые концепции, связанные с работой системы.
 По существу, этот тип системы хранения построен на идее, что любое оборудование для систем хранения испытывает отказы на любом уровне в некоторый момент.
 Изредка встречаются отказы, которые подставляют подножку другим системам хранения, такие как вопросы замена RAID карты, или целого сервера изящно обрабатываются в OpenStack Object Storage.</p>
<p>Хороший документ, описывающий архитектуру объектного хранилища находится в <span class="red-heading">документации разработчика</span> (<a href="http://docs.openstack.org/developer/swift/overview_architecture.html">http://docs.openstack.org/developer/swift/overview_architecture.html</a>) - для начала ознакомьтесь с ним.
 После того, как вы представили себе архитектуру, вы должны понять, что делает прокси-сервер и как работают зоны.
 Тем не менее, обозначим некоторые существующие важные моменты, которые часто упускаются при первом ознакомлении.</p>
<p>При проектировании кластера, необходимо учитывать долговечность и доступность.
 Поймите, что их основным источником является распределение и расположение ваших данных, а не надежность оборудования.
 Рассмотрим значение по умолчанию количества копий, равное 3.
 Это означает, что, перед тем, как объект будет помечен успешно записанным, существуют. по крайней мере, две копии - в случае отказа при записи на одном сервере, третий экземпляр может существовать или пока еще нет, вто время как начально возвращена операция записи.
 Изменение этого числа увеличивает надежность ваших данных, но уменьшает объем имеющейся у вас памяти.
 Далее обратим внимание на размещение серверов. 
 Рассмотрим их распределение по всей сети и зонам отказа электропитания вашего центра обработки данных.
 Является зона стойкой, сервером или диском?</p>
<p>На первый взгляд сетевые структуры хранилища объектов могут показаться непривычными.
 Рассмотрим эти основные потоки траффика.</p>
<ul>
 <li>Среди <span class="red-heading">объектов</span>, <span class="red-heading">контейнеров</span> и <span class="red-heading">серверов учетных записей</span> 
 <li>Между этими серверами и прокси
 <li>Между прокси и вашими пользователями
</ul>
<p>Хранилище объектов очень 'болтливое' среди всех серверов, хранящих данные - даже небольшой кластер делает трафик, измеряемый мегабайтами в секунду, причем он преимущественно состоит из &quot;У вас есть объект&quot; / &quot;Да у меня есть объект!&quot;.
 Конечно, если ответ на вышеупомянутой вопрос отрицательный или истекает время ожидания, начинается репликация объекта.</p> 
<p>Рассмотрим ситуацию, когда отказывает весь сервер, и 24ТБ данных должно быть передано "немедленно", чтобы остаться в трех экземплярах - это может составить значительную нагрузку для сети.</p>
<p>Другой часто забываемый факт заключается в том, что когда загружаен новый файл, прокси-сервер должен записать столько потоков, сколько существует реплик - порождая кратное умножение сетевого трафика.
 Для кластера c 3-мя репликами, 10Гбит в секунду означает порождение 30Гбит в секунду.
 Объединяя это с предыдущими высокими запросами пропускной способности репликаций порождает в результате рекомендацию необходимости для вашей локальной сети значительно более высокой пропускной способности, того требует сеть общего полбзования.
 Oh и OpenStack Object Storage взаимодействуют внутри с использованием незашифрованного, без аутентификации протокола rsync для получения производительности - вам действительно нужна собственная сеть, чтобы быть приватной.</p>
<p>Оставшимся пунктом в полосе пропускания является часть, обащенная к общественной сети.
 swift- прокси не имеет состояний, а это означает, что вы легко можете добавить еще прокси и использовать методы балансировки нагрузки HTTP, чтобы разделить полосу пропускания и доступность между ними.</p>
<p>Дополнительные прокси означают более высокую пропускную способность, если это позволяет хранилище.</p>


<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody><tr>
 <td align="left"><a href="Ch04ru.htm">Глава 4</a></td>
 <td align="center"><a href="index.htm">Оглавление</a></td>
 <td align="right"><a href="Ch06ru.htm">Глава 6</a></td>
</tr></tbody></table>

</tbody></table>

</body>
</html>
