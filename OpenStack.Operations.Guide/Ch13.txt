Logging and Monitoring

As an OpenStack cloud is composed of so many different services, there are a large number of log files.
 This section aims to assist you in locating and working with them, and other ways to track the status of your deployment.

Where Are the Logs?
On Ubuntu, most services use the convention of writing their log files to subdirectories of the <code>/var/log directory</code>.

Cloud Controller

<table width="50%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody><tr>
 <td width="50%"><strong>Ñëóæáà</strong><br />&nbsp;</td>
 <td width="50%"><strong>Ìåñòîïîëîæåíèå æóðíàëà</strong><br />&nbsp;</td>
</tr><tr><td>
nova-*</td><td>
 /var/log/nova
</tr><tr><td>
glance-*</td><td>
 /var/log/glance
</tr><tr><td>
cinder-*</td><td>
 /var/log/cinder
</tr><tr><td>
keystone</td><td>
 /var/log/keystone
</tr><tr><td>
horizon</td><td>
 /var/log/apache2/
</tr><tr><td>
misc (Swift, dnsmasq)</td><td>
 /var/log/syslog
</tr></tbody></table>

Compute Nodes
<br /><code>
libvirt: /var/log/libvirt/libvirtd.log<br />
</code>

Console (boot up messages) for VM instances: <code>/var/lib/nova/instances/instance-&lt;instance id&gt;/console.log</code>

Block Storage Nodes
<br /><code>
cinder: /var/log/cinder/cinder-volume.log<br />
</code>

How to Read the Logs
OpenStack services use the standard logging levels, at increasing severity: DEBUG,
INFO, AUDIT, WARNING, ERROR, CRITICAL, and TRACE. That is, messages only
appear in the logs if they are more &quot;severe&quot; than the particular log level with DEBUG allowing all log statements through.
 For example, TRACE is logged only if the software has a stack trace, while INFO is logged for every message including those that are only for information.

To disable DEBUG-level logging, edit <code>/etc/nova/nova.conf</code>:
<br /><code>
debug=false<br />
</code>

Keystone is handled a little differently.
 To modify the logging level, edit the <code>/etc/keystone/logging.conf</code> file and look at the <code>logger_root</code> and <code>handler_file</code> sections.

Logging for Horizon is configured in <code>/etc/openstack_dashboard/local_settings.py</code>.
 As Horizon is a Django web application, it follows the <span class="red-heading">Django Logging</span> (<a href="https://docs.djangoproject.com/en/dev/topics/logging/">https://docs.djangoproject.com/en/dev/topics/logging/</a>) framework conventions.

The first step in finding the source of an error is typically to search for a CRITICAL, TRACE, or ERROR message in the log starting at the bottom of the log file.

An example of a CRITICAL log message, with the corresponding TRACE (Python traceback) immediately following:
<br /><code>
2013-02-25 21:05:51 17409 CRITICAL cinder [-] Bad or unexpected response from the storage volume backend API: volume group cinder-volumes doesn't exist<br />
2013-02-25 21:05:51 17409 TRACE cinder Traceback (most recent call last):<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/bin/cinder-volume&quot;, line 48, in &lt;module&gt;<br />
2013-02-25 21:05:51 17409 TRACE cinder service.wait()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/service.py&quot;, line 422, in wait<br />
2013-02-25 21:05:51 17409 TRACE cinder _launcher.wait()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/service.py&quot;, line 127, in wait<br />
2013-02-25 21:05:51 17409 TRACE cinder service.wait()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/eventlet/greenthread.py&quot;, line 166, in wait<br />
2013-02-25 21:05:51 17409 TRACE cinder return self._exit_event.wait()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/eventlet/event.py&quot;, line 116, in wait<br />
2013-02-25 21:05:51 17409 TRACE cinder return hubs.get_hub().switch()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py&quot;, line 177, in switch<br />
2013-02-25 21:05:51 17409 TRACE cinder return self.greenlet.switch()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/eventlet/greenthread.py&quot;, line 192, in main<br />
2013-02-25 21:05:51 17409 TRACE cinder result = function(*args, **kwargs)<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/service.py&quot;, line 88, in run_server<br />
2013-02-25 21:05:51 17409 TRACE cinder server.start()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/service.py&quot;, line 159, in start<br />
2013-02-25 21:05:51 17409 TRACE cinder self.manager.init_host()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/volume/manager.py&quot;, line 95, <br />
 in init_host<br />
2013-02-25 21:05:51 17409 TRACE cinder self.driver.check_for_setup_error()<br />
2013-02-25 21:05:51 17409 TRACE cinder File &quot;/usr/lib/python2.7/dist-packages/cinder/volume/driver.py&quot;, line 116, <br />
 in check_for_setup_error<br />
2013-02-25 21:05:51 17409 TRACE cinder raise exception.VolumeBackendAPIException(data=exception_message)<br />
2013-02-25 21:05:51 17409 TRACE cinder VolumeBackendAPIException: Bad or unexpected response from the storage volume <br />
 backend API: volume group cinder-volumes doesn't exist<br />
2013-02-25 21:05:51 17409 TRACE cinder<br />
</code>

In this example, cinder-volumes failed to start and has provided a stack trace, since its volume back-end has been unable to setup the storage volume - probably because the LVM volume that is expected from the configuration does not exist.

An example error log:
<br /><code>
2013-02-25 20:26:33 6619 ERROR nova.openstack.common.rpc.common [-] AMQP server on localhost:5672 is unreachable:<br />
 [Errno 111] ECONNREFUSED. Trying again in 23 seconds.<br />
</code>

In this error, a nova service has failed to connect to the RabbitMQ server, because it got a connection refused error.

Tracing Instance Requests
When an instance fails to behave properly, you will often have to trace activity associated with that instance across the log files of various <code>nova-*</code> services, and across both the cloud controller and compute nodes.

The typical way is to trace the UUID associated with an instance across the service logs.

Consider the following example:
<br /><code>
ubuntu@initial:~$ nova list<br />
+--------------------------------+--------+--------+--------------------------+<br />
| ID &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Name &nbsp; | Status | Networks &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />
+--------------------------------+--------+--------+--------------------------+<br />
| fafed8-4a46-413b-b113-f1959ffe | cirros | ACTIVE | novanetwork=192.168.100.3|<br />
+--------------------------------------+--------+--------+--------------------+<br />
</code>

Here the ID associated with the instance is <code>faf7ded8-4a46-413b-b113-f19590746ffe</code>.
 If you search for this string on the cloud controller in the <code>/var/log/nova-*.logfiles</code>, it appears in <code>nova-api.log</code>, and <code>nova-scheduler.log</code>.
 If you search for this on the compute nodes in <code>/var/log/nova-*.log</code>, it appears <code>novanetwork.log</code> and <code>nova-compute.log</code>.
 If no ERROR or CRITICAL messages appear, the most recent log entry that reports this may provide a hint about what has gone wrong.

Adding Custom Logging Statements
If there is not enough information in the existing logs, you may need to add your own custom logging statements to the <code>nova-*services</code>.

The source files are located in <code>/usr/lib/python2.7/dist-packages/nova</code>

To add logging statements, the following line should be near the top of the file.
 For most files, these should already be there:
<br /><code>
from nova.openstack.common import log as logging<br />
LOG = logging.getLogger(__name__)<br />
</code>

To add a DEBUG logging statement, you would do:
<br /><code>
LOG.debug(&quot;This is a custom debugging statement&quot;)<br />
</code>

You may notice that all of the existing logging messages are preceded by an underscore and surrounded by parentheses, for example:
<br /><code>
LOG.debug(_(&quot;Logging statement appears here&quot;))<br />
</code>

This is used to support translation of logging messages into different languages using the <span class="red-heading">gettext</span> (<a href="http://docs.python.org/2/library/gettext.html">http://docs.python.org/2/library/gettext.html</a>) internationalization library.
 You don’t need to do this for your own custom log messages.
 However, if you want to contribute the code back to the OpenStack project that includes logging statements, you must surround your log messages with underscore and parentheses.
	
RabbitMQ Web Management Interface or rabbitmqctl
Aside from connection failures, RabbitMQ log files are generally not useful for debugging OpenStack related issues.
 Instead, we recommend you use the RabbitMQ web management interface. Enable it on your cloud controller:
<br /><code>
# /usr/lib/rabbitmq/bin/rabbitmq-plugins enable rabbitmq_management<br />
# service rabbitmq-server restart<br />
</code>

The RabbitMQ web management interface is accessible on your cloud controller at http://localhost:55672.

Ubuntu 12.04 installs RabbitMQ version 2.7.1, which uses port 55672.
 RabbitMQ versions 3.0 and above use port 15672 instead.
 You can check which version of RabbitMQ you have running on your local Ubuntu machine by doing:
<br /><code>
$ dpkg -s rabbitmq-server | grep &quot;Version:&quot;
Version: 2.7.1-0ubuntu4<br />
</code>

An alternative to enabling the RabbitMQ Web Management Interface is to use the <em>rabbitmqctl</em> commands.
 For example, <em>rabbitmqctl list_queues| grep cinder</em> displays any messages left in the queue.
 If there are, it’s a possible sign that cinder services didn’t connect properly to rabbitmq and might have to be restarted.

Items to monitor for RabbitMQ include the number of items in each of the queues and the processing time statistics for the server.

Centrally Managing Logs
Because your cloud is most likely composed of many servers, you must check logs on each of those servers to properly piece an event together.
 A better solution is to send the logs of all servers to a central location so they can all be accessed from the same area.

Ubuntu uses rsyslog as the default logging service. Since it is natively able to send logs to a remote location, you don’t have to install anything extra to enable this feature, just modify the configuration file.
 In doing this, consider running your logging over a management network, or using an encrypted VPN to avoid interception.

rsyslog Client Configuration
To begin, configure all OpenStack components to log to syslog in addition to their standard log file location.
 Also configure each component to log to a different syslog facility.
 This makes it easier to split the logs into individual components on the central server.
<br /><code>
<strong>nova.conf</strong>:<br />
 &nbsp; use_syslog=True<br />
 &nbsp; syslog_log_facility=LOG_LOCAL0<br />
<strong>glance-api.confand glance-registry.conf</strong>:<br />
 &nbsp; use_syslog=True<br />
 &nbsp; syslog_log_facility=LOG_LOCAL1<br />
<strong>cinder.conf</strong>:<br />
 &nbsp; use_syslog=True<br />
 &nbsp; syslog_log_facility=LOG_LOCAL2<br />
<strong>keystone.conf</strong>:<br />
 &nbsp; use_syslog=True<br />
 &nbsp; syslog_log_facility=LOG_LOCAL3<br />
</code>

Swift<br />

By default, Swift logs to syslog.

Next, create <code>/etc/rsyslog.d/client.conf</code> with the following line:
<br /><code>
*.* @192.168.1.10<br />
</code>

This instructs rsyslog to send all logs to the IP listed.
 In this example, the IP points to the Cloud Controller.

rsyslog Server Configuration
Designate a server as the central logging server.
 The best practice is to choose a server that is solely dedicated to this purpose.
 Create a file called <code>/etc/rsyslog.d/server.conf</code> with the following contents:
<br /><code>
# Enable UDP<br /> 
$ModLoad imudp <br />
# Listen on 192.168.1.10 only <br />
$UDPServerAddress 192.168.1.10<br />
# Port 514 <br />
$UDPServerRun 514 <br />
<br />
# Create logging templates for nova <br />
$template NovaFile,&quot;/var/log/rsyslog/%HOSTNAME%/nova.log&quot; <br />
$template NovaAll,&quot;/var/log/rsyslog/nova.log&quot; <br />
# Log everything else to syslog.log <br />
<br />
<br />
$template DynFile,&quot;/var/log/rsyslog/%HOSTNAME%/syslog.log&quot; <br />
*.* ?DynFile <br />
<br />
# Log various openstack components to their own individual file <br />
local0.* ?NovaFile <br />
local0.* ?NovaAll <br />
&amp;~<br />
</code>

The above example configuration handles the nova service only.
 It first configures rsyslog to act as a server that runs on port 514.
 Next, it creates a series of logging templates.
 Logging templates control where received logs are stored.
 Using the example
above, a nova log from c01.example.com goes to the following locations:
• <code>/var/log/rsyslog/c01.example.com/nova.log</code>
• <code>/var/log/rsyslog/nova.log</code>

This is useful as logs from c02.example.com go to:
• <code>/var/log/rsyslog/c02.example.com/nova.log</code>
• <code>/var/log/rsyslog/nova.log</code>
So you have an individual log file for each compute node as well as an aggregated log that contains nova logs from all nodes.

StackTach
StackTach is a tool created by Rackspace to collect and report the notifications sent by nova.
 Notifications are essentially the same as logs, but can be much more detailed.
 A good overview of notifications can be found at <span class="red-heading">System Usage Data</span> (<a href="https://wiki.openstack.org/wiki/SystemUsageData">https://wiki.openstack.org/wiki/SystemUsageData</a>).
To enable nova to send notifications, add the following to <code>nova.conf</code>:
<br /><code>
notification_topics=monitor 
notification_driver=nova.openstack.common.notifier.rabbit_notifier<br />
</code>

Once novais sending notifications, install and configure StackTach.
 Since StackTach is relatively new and constantly changing, installation instructions would quickly become outdated.
 Please refer to the <span class="red-heading">StackTach GitHub repo</span> (<a href="https://github.com/rackerlabs/stacktach">https://github.com/rackerlabs/stacktach</a>) for instructions as well as a demo video.

Monitoring
There are two types of monitoring: watching for problems and watching usage trends.
 The former ensures that all services are up and running, creating a functional cloud.
 The latter involves monitoring resource usage over time in order to make informed decisions about potential bottlenecks and upgrades.

Process Monitoring
A basic type of alert monitoring is to simply check and see if a required process is running.
 For example, ensure that the nova-apiservice is running on the Cloud Controller:
<br /><code>
[ root@cloud ~ ] # ps aux | grep nova-api<br />
nova 12786 0.0 0.0 37952 1312 ? Ss Feb11 0:00 su -s /bin/sh -c exec nova-api --config-file=/etc/nova/nova.conf nova<br />
nova 12787 0.0 0.1 135764 57400 ? S Feb11 0:01 /usr/bin/python /usr/bin/novaapi --config-file=/etc/nova/nova.conf<br />
nova 12792 0.0 0.0 96052 22856 ? S Feb11 0:01 /usr/bin/python /usr/bin/nova-api  --config-file=/etc/nova/nova.conf<br />
nova 12793 0.0 0.3 290688 115516 ? S Feb11 1:23 /usr/bin/python /usr/bin/novaapi --config-file=/etc/nova/nova.conf<br />
nova 12794 0.0 0.2 248636 77068 ? S Feb11 0:04 /usr/bin/python /usr/bin/novaapi --config-file=/etc/nova/nova.conf<br />
root 24121 0.0 0.0 11688 912 pts/5 S+ 13:07 0:00 grep nova-api<br />
</code>

You can create automated alerts for critical processes by using Nagios and NRPE.
 For example, to ensure that the nova-compute  process is running on compute nodes, create an alert on your Nagios server that looks like this:
<br /><code>
define service { <br />
&nbsp; &nbsp; host_name c01.example.com <br />
&nbsp; &nbsp; check_command check_nrpe_1arg!check_nova-compute <br />
&nbsp; &nbsp; use generic-service <br />
&nbsp; &nbsp; notification_period 24x7 <br />
&nbsp; &nbsp; contact_groups sysadmins <br />
&nbsp; &nbsp; service_description nova-compute <br />
}<br />
</code>

Then on the actual compute node, create the following NRPE configuration:
<br /><code>
command[check_nova-compute]=/usr/lib/nagios/plugins/check_procs -c 1: -a novacompute<br />
</code>

Nagios checks that at least one nova-compute service is running at all times.

Resource Alerting
Resource alerting provides notifications when one or more resources are critically low.
 While the monitoring thresholds should be tuned to your specific OpenStack environment, monitoring resource usage is not specific to OpenStack at all – any generic type of alert will work fine.

Some of the resources that you want to monitor include:
• Disk Usage
• Server Load
• Memory Usage
• Network IO
• Available vCPUs

For example, to monitor disk capacity on a compute node with Nagios, add the following to your Nagios configuration:
<br /><code>
define service {<br /> 
&nbsp; &nbsp; host_name c01.example.com<br /> 
&nbsp; &nbsp; check_command check_nrpe!check_all_disks!20% 10% <br />
&nbsp; &nbsp; use generic-service <br />
&nbsp; &nbsp; contact_groups sysadmins <br />
&nbsp; &nbsp; service_description Disk <br />
}<br />
</code>

On the compute node, add the following to your NRPE configuration:
<br /><code>
command[check_all_disks]=/usr/lib/nagios/plugins/check_disk -w $ARG1$ -c $ARG2$  -e<br />
</code>

Nagios alerts you with a WARNING when any disk on the compute node is 80% full and CRITICAL when 90% is full.

OpenStack-specific Resources
Resources such as memory, disk, and CPU are generic resources that all servers (even non-OpenStack servers) have and are important to the overall health of the server.
 When dealing with OpenStack specifically, these resources are important for a second reason: ensuring enough are available in order to launch instances.
 There are a few ways you can see OpenStack resource usage.

The first is through the <code>nova</code>
command:
<br /><code>
# nova usage-list<br />
</code>

This command displays a list of how many instances a tenant has running and some light usage statistics about the combined instances.
 This command is useful for a quick overview of your cloud, but doesn’t really get into a lot of details.

Next, the novadatabase contains three tables that store usage information.

The <code>nova.quotas</code> and <code>nova.quota_usages</code> tables store quota information.
 If a tenant’s quota is different than the default quota settings, their quota is stored in <code>nova.quotas</code> table.
 For example:
<br /><code>
mysql&gt; select project_id, resource, hard_limit from quotas; <br />
+----------------------------------+-----------------------------+------------+<br />
| project_id &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | resource &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | hard_limit |<br />
+----------------------------------+-----------------------------+------------+<br />
| 628df59f091142399e0689a2696f5baa | metadata_items &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; | 128 &nbsp; &nbsp; &nbsp; &nbsp;|<br />
| 628df59f091142399e0689a2696f5baa | injected_file_content_bytes | 10240 &nbsp; &nbsp; &nbsp;|<br />
| 628df59f091142399e0689a2696f5baa | injected_files &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; | 5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|<br />
| 628df59f091142399e0689a2696f5baa | gigabytes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 1000 &nbsp; &nbsp; &nbsp; |<br />
| 628df59f091142399e0689a2696f5baa | ram &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 51200 &nbsp; &nbsp; &nbsp;|<br />
| 628df59f091142399e0689a2696f5baa | floating_ips &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; | 10 &nbsp; &nbsp; &nbsp; &nbsp; |<br />
| 628df59f091142399e0689a2696f5baa | instances &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 10 &nbsp; &nbsp; &nbsp; &nbsp; |<br />
| 628df59f091142399e0689a2696f5baa | volumes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 10 &nbsp; &nbsp; &nbsp; &nbsp; |<br />
| 628df59f091142399e0689a2696f5baa | cores &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 20 &nbsp; &nbsp; &nbsp; &nbsp; |<br />
+----------------------------------+-----------------------------+------------+ <br />
</code>

The nova.quota_usages table keeps track of how many resources the tenant currently has in use:
<br /><code>
mysql&gt; select project_id, resource, in_use from quota_usages where project_id like '628%';<br />
+----------------------------------+--------------+--------+ <br />
| project_id &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | resource &nbsp; &nbsp; | in_use | <br />
+----------------------------------+--------------+--------+ <br />
| 628df59f091142399e0689a2696f5baa | instances &nbsp; &nbsp;| 1 &nbsp; &nbsp; &nbsp;|<br />
| 628df59f091142399e0689a2696f5baa | ram &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; | 512 &nbsp; &nbsp;| <br />
| 628df59f091142399e0689a2696f5baa | cores &nbsp;&nbsp; &nbsp; &nbsp; | 1 &nbsp; &nbsp; &nbsp;| <br />
| 628df59f091142399e0689a2696f5baa | floating_ips | 1 &nbsp; &nbsp; &nbsp;| <br />
| 628df59f091142399e0689a2696f5baa | volumes &nbsp; &nbsp; &nbsp;| 2 &nbsp; &nbsp; &nbsp;| <br />
| 628df59f091142399e0689a2696f5baa | gigabytes &nbsp; &nbsp;| 12 &nbsp; &nbsp; | <br />
| 628df59f091142399e0689a2696f5baa | images &nbsp; &nbsp; &nbsp; | 1 &nbsp; &nbsp; &nbsp;| <br />
+----------------------------------+--------------+--------+<br />
</code>

By combining the resources used with the tenant’s quota, you can figure out a usage percentage.
 For example, if this tenant is using 1 Floating IP out of 10, then they are using 10% of their Floating IP quota.
 You can take this procedure and turn it into a formatted report:
<br /><code>
+-----------------------------------+------------+------------+---------------+<br />
| some_tenant &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />
+-----------------------------------+------------+------------+---------------+<br />
| Resource &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Used &nbsp; &nbsp; &nbsp; | Limit &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br />
+-----------------------------------+------------+------------+---------------+<br />
| cores &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 20 &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5 % |<br />
| floating_ips &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 1 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; | 10 &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10 % |<br />
| gigabytes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 12 &nbsp; &nbsp; &nbsp; &nbsp; | 1000 &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 % |<br />
| images &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 25 % |<br />
| injected_file_content_bytes &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 10240 &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| injected_file_path_bytes &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 255 &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| injected_files &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| instances &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 10 &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 10 % |<br />
| key_pairs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 100 &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| metadata_items &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 128 &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| ram &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 512 &nbsp; &nbsp; &nbsp; &nbsp;| 51200 &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 % |<br />
| reservation_expire &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 86400 &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| security_group_rules &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 20 &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| security_groups &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 10 &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 % |<br />
| volumes &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 10 &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 20 % |<br />
+-----------------------------------+------------+------------+---------------+<br />
</code>

The above was generated using a custom script which can be found on GitHub (<a href="https://github.com/cybera/novac/blob/dev/libexec/novac-quota-report">https://github.com/cybera/novac/blob/dev/libexec/novac-quota-report</a>).

This script is specific to a certain OpenStack installation and must be modified to fit your environment.
 However, the logic should easily be transferable.

Intelligent Alerting
Intelligent alerting can be thought of as a form of continuous integration for operations.
 For example, you can easily check to see if Glance is up and running by ensuring that the <code>glance-api</code> and <code>glance-registry</code> processes are running or by seeing if <code>glace-api</code> is responding on port 9292.

But how can you tell if images are being successfully uploaded to the Image Service?
 Maybe the disk that Image Service is storing the images on is full or the S3 back-end is down.
 You could naturally check this by doing a quick image upload:
<br /><code>
#!/bin/bash 
# 
# assumes that reasonable credentials have been stored at
# /root/auth 
. /root/openrc 
wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img 
glance image-create --name='cirros image' --is-public=true --containerformat=bare --disk-format=qcow2 &lt; cirros-0.3.0-x86_64-disk.img<br />
</code>

By taking this script and rolling it into an alert for your monitoring system (such as Nagios), you now have an automated way of ensuring image uploads to the Image Catalog are working.

You must remove the image after each test. Even better, test whether you can successfully delete an image from the Image Service.

Intelligent alerting takes a considerable more amount of time to plan and implement than the other alerts described in this chapter.
 A good outline to implement intelligent alerting is:
• Review common actions in your cloud
• Create ways to automatically test these actions
• Roll these tests into an alerting system

Some other examples for Intelligent Alerting include:
• Can instances launch and destroyed?
• Can users be created?
• Can objects be stored and deleted?
• Can volumes be created and destroyed?

Trending
Trending can give you great insight into how your cloud is performing day to day.
 For example, if a busy day was simply a rare occurrence or if you should start adding new compute nodes.

Trending takes a slightly different approach than alerting. While alerting is interested in a binary result (whether a check succeeds or fails), trending records the current state of something at a certain point in time.
 Once enough points in time have been recorded, you can see how the value has changed over time.

All of the alert types mentioned earlier can also be used for trend reporting.
 Some other trend examples include:
• The number of instances on each compute node
• The types of flavors in use
• The number of volumes in use
• The number of Object Storage requests each hour
• The number of nova-api requests each hour
• The I/O statistics of your storage services

As an example, recording <code>nova-api</code> usage can allow you to track the need to scale your cloud controller.
 By keeping an eye on <code>nova-api</code> requests, you can determine if you need to spawn more nova-api processes or go as far as introducing an entirely new server to run <code>nova-api</code>.
 To get an approximate count of the requests, look for standard INFO messages in <code>/var/log/nova/nova-api.log</code>:
<br /><code>
# grep INFO /var/log/nova/nova-api.log | wc<br />
</code>

You can obtain further statistics by looking for the number of successful requests:
<br /><code>
# grep &quot; 200 &quot; /var/log/nova/nova-api.log | wc<br />
</code>

By running this command periodically and keeping a record of the result, you can create a trending report over time that shows whether your <code>nova-api</code> usage is increasing, decreasing, or keeping steady.

A tool such as collectd can be used to store this information.
 While collectd is out of the scope of this book, a good starting point would be to use collectd to store the result as a COUNTER data type. More information can be found in collectd’s documentation (<a href="https://collectd.org/wiki/index.php/Data_source">https://collectd.org/wiki/index.php/Data_source</a>)
