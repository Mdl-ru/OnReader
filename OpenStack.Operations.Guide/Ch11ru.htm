<html>
<head>
   <link rel="icon" href="/i/MdlLogo.gif" type="image/gif">
   <title>Глава 11. Обслуживание, сбои и отладка. Руководство по эксплуатации OpenStack.</title>
   <meta name="Keywords" content="OpenStack, Cloud computing, Swift, RESTful, Object Storage, Ceph, CORS, CNAME lookup, Domain remap, Health check, Rate limiting, Bulk delete, Container qoutas, account qoutas, TempURL, Static Web, Form post, recon, Swift origin server, Bulk archive auto-extraction">
   <meta name="Description" content="Глава 11. Обслуживание, сбои и отладка. Руководство по эксплуатации OpenStack.">
   <meta name="Robots" content="INDEX, FOLLOW">
   <meta name="Author" content="Module-Projects,Ltd">
   <meta name="Copyright" content="Copyright 1998..2015 Module-Projects,Ltd">
   <meta http-equiv="Pragma" content="no-cache">
<script language="javascript" src="/css/v.0/mdlcss.js"></script>
<style type="text/css" media="screen, print">@import url("i/global-20140610.css");</style>
<script language="javascript" src="http://www.mdl.ru/usd.js"></script>
	<script language="javascript" src="http://www.mdl.ru/js/common.js"></script>
	<script language="javascript" src="http://www.mdl.ru/Solutions/ABC.js"></script>
</head>
<body>

<table class="bg_White" width="1024" align="center" valign="top" border="0" cellpadding="0" cellspacing="0"><tbody>
<tr>
<td>
<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody>
<tr>
<td width="150" valign="top" align="center"><img src="http://www.mdl.ru/RMC9.jpg" border=0 /></td>
<td width="724" valign="bottom" align="center">
<a class="item-t" href="http://www.mdl.ru"><img src="http://www.mdl.ru/i/MdlBigLogo.gif" border="0"></a><br/>
<a class="item-t" href="http://www.mdl.ru">С 1991 года на компьютерном рынке России</a>
</td>
<td align="center" valign="bottom">
<a class="item-t" href="javascript:tocall()" onmouseover="this.href=mail"><img src="http://www.mdl.ru/i/9563499.gif" border="0" alt="e-mail" /><br/><br/>т.: 676 0965, 676 0396<br/>Москва, Сосинская ул. 43, <br/>м. Волгоградский проспект</a>
</td>
</tr>
<tr>
<td class="big_16y" colspan="3" align="center"><a href="index.htm">Руководство по эксплуатации OpenStack</a></td>
</tr>
<tr><td colspan="2">


<h2 align="right">ГЛАВА 11</h2>
<hr />
<h1 id="Chapter_11" align="right">Обслуживание, сбои и отладка</h1>

<em>Пользуйтесь переводом <strong>существенно переработанной и дополенной <a href="http://onreader.mdl.ru/openstack-ops/content/maintenance.html">2й редакции</a></strong> (12-дек-2014),<br />
находящейся теперь и в режиме <a href="http://docs.openstack.org/openstack-ops/content/maintenance.html">постоянно обновляемой документации</a> <br />
(последняя пока доступна только на англ.яз.).</em><br />
<p></p>

<p>Простои, вне зависимости от того являются ли они внезапными или запланированными, являются данностью при 
работе облака.
   Целью данной главы является предоставление полезной информации для активной или противодействующей обработки 
   таких событий.</p>

<h2 id="Ch1101">Отказы и техническое обслуживание контроллера облака и прокси системы хранения</h2>
<p>Контроллер облака и прокси системы  хранения очень похожи друг на друга, когда возникают ожидаемые и 
незапланированные простои.
  Обычно в облаке работает по одному серверу каждого типа, что делает их очень заметными при их простоях.</p>
<p>Хорошей новостью для контроллера облака является то, что если ваше облако использует сетевой режим 
мультихостовой высокой доступности (multihost HA) FlatDHCP, то существующие экземпляры и тома 
продолжают работать при уходе контроллера облака в офлайн.
  Однако для прокси системы хранения невозможны никакие потоки хранимой информации до тех пор, 
  пока он не вернется к работе.</p>

<h3 id="Ch110101">Запланированное обслуживание</h3>
<p>Один из способов запланировать обслуживание контроллера облака или прокси системы хранения 
заключается в том, чтобы просто выполнить это во внерабочее время, например, в час или два ночи.
 Такая стратегия окажет воздействие на меньшее число пользователей.
 Если ваш контроллер облака или прокси системы хранения слишком важен и не может быть недоступным 
 в любой момент времени, вы должны рассмотреть вариант высокой доступности.</p>

<h3 id="Ch110102">Перезагрузка контроллера облака или прокси системы хранения</h3>
<p>В общем, просто выполните команду &quot;reboot&quot;.
  Операционная система аккуратно закрывает службы и затем автоматически перезагружается.
  Если вы хотите быть очень аккуратным, выполните свои задания резервного копирования непосредственно 
  перед перезагрузкой.</p>

<h3 id="Ch110103">После того, как контроллер облака и прокси системы хранения перезагрузились</h3>
<p>После того, как контроллер облака перезагрузился, проверьте что успешно стартовали все необходимые службы:
<br /><code>
# ps aux | grep nova-# grep AMQP /var/log/nova/nova-*.log<br />
# ps aux | grep glance-# ps aux | grep keystone<br />
# ps aux | grep cinder<br />
</code></p>
<p>Также проверьте, что все службы функционируют:
<br /><code>
# source openrc<br />
# glance index<br />
# nova list<br />
# keystone tenant-list<br />
</code></p>
<p>Для прокси системы хранения убедитесь, что возобновилась служба системы хранения объектов (Object Storage):
<br /><code>
# ps aux | grep swift<br />
</code></p>
<p>Также проверьте, что она работает:
<br /><code>
# swift stat<br />
</code></p>

<h3 id="Ch110104">Полный отказ контроллера облака</h3>
<p>К сожалению, это трудная ситуация.
 Контроллер облака является неотъемлемой частью вашего облака.
 Если у вас есть только один контроллер, многие службы будут утрачены.</p>
<p>Чтобы избежать подобной ситуации, создайте кластер контроллера облака высокой доступности.
  Это выходит за рамки данного документа, но вы можете прочитать дополнительные сведения в проекте 
  <span class="red-heading">OpenStack High Availability Guide</span> 
  (<a href="http://docs.openstack.org/trunk/openstack-ha/content/ch-intro.html">
  http://docs.openstack.org/trunk/openstack-ha/content/ch-intro.html</a>).</p>
<p>Другой хороший способ заключается в использовании инструмента управления конфигурацией, например, 
Puppet, для автоматического создания контроллера облака.
  Это не должно занять более 15 минут, если у вас имеется запасной сервер.
  После восстановления контроллера восстановите все сделанные резервные копии (см. главу 
  <strong><a href="Ch14ru.htm">Резервное копирование и восстановление</a></strong>).</p>
<p>Кроме того, на практике, службы nova-compute на вычислительных узлах иногда не аккуратно 
подключаются к размещенной на контроллере rabbitmq в случае, когда он возвращается после совместной 
перезагрузки и требуется перезагрузка служб nova на вычислительных узлах.</p>

<h2 id="Ch1102">Отказы вычислительных узлов и их обслуживание</h2>
<p>Иногда вычислительный узел либо неожиданно испытывает сбой, либо требует перезагрузки по 
причинам технического характера.</p>

<h3 id="Ch110201">Запланированное обслуживание</h3>
<p>Если вам необходимо перезагрузить вычислительный узел в связи с плановым техническим обслуживанием 
(например, обновление программного обеспечения или аппаратных компонент), то вначале убедитесь, 
что все размещенные на нем экземпляры были перемещены с данного узла.
  Если облако использует общую систему хранения, то используйте команду <code>nova live-migration</code>.
  Вначале получите список экземпляров, требующих перемещения:
<br /><code>
# nova list --host c01.example.com --all-tenants<br />
</code></p>
<p>Затем переместите их один за другим:
 <br /><code>
# nova live-migration &lt;uuid&gt; c02.example.com<br />
</code></p>
<p>После переноса всех экземпляров обеспечьте останов всех служб <code>nova-compute</code>:
 <br /><code>
# stop nova-compute<br />
</code></p>
<p>Если вы используете систему управления настройками, например, Puppet, которая обеспечивает 
постоянную работу служб <code>nova-compute</code>, вы можете временно перенести файлы инициализации:
 <br /><code>
# mkdir /root/tmp<br />
# mv /etc/init/nova-compute.conf /root/tmp<br />
# mv /etc/init.d/nova-compute /root/tmp<br />
</code></p>
<p>Затем закройте свои вычислительные узлы, выполните ваши работы по обслуживанию и включите их снова.
 Вы можете снова запустить службу <code>nova-compute</code>, откатив предыдущие команды:
 <br /><code>
# mv /root/tmp/nova-compute.conf /etc/init<br />
# mv /root/tmp/nova-compute /etc/init.d/<br />
</code></p>
<p>Затем запустите службу <code>nova-compute</code>:
 <br /><code>
# start nova-compute<br />
</code></p>
<p>Теперь вы можете по желанию вернуть назад экземпляры на их первоначальные вычислительные узлы.</p>

<h3 id="Ch110202">После перезагрузки вычислительных узлов</h3>
<p>После перезагрузки вычислительного узла, убедитесь что он успешно загрузился.</p>
<p>Это включает в себя проверку того, что служба <code>nova-compute</code> работает:
 <br /><code>
# ps aux | grep nova-compute
# status nova-compute<br />
</code></p>
<p>Также убедитесь, что она успешно подключилась к серверу AMQP:
 <br /><code>
# grep AMQP /var/log/nova/nova-compute<br />
2013-02-26 09:51:31 12427 INFO nova.openstack.common.rpc.common [-] Connected  to AMQP server on 199.116.232.36:5672<br />
</code></p>
<p>После того, как вычислительный узел успешно заработал, вы должны заняться размещенными на 
этом вычислительном узле экземплярами, поскольку ни один из них не запущен.
 В зависимости от вашего SLA у вашего пользователя или клиента, вам, возможно, придется запустить 
 каждый экземпляр и убедиться в том, что он стартовал корректно:</p>

<h3 id="Ch110203">Экземпляры</h3>
<p>Вы можете создать список размещенных на вычислительном узле экземпляров, выполнив следующую команду:
 <br /><code>
# nova list --host c01.example.com --all-tenants<br />
</code></p>
<p>После получения списка,вы можете использовать команду nova для запуска каждого экземпляра:
 <br /><code>
# nova reboot &lt;uuid&gt;<br />
</code></p>
<p><img src="i/Tip.jpg" alt="Совет" />После каждого неожиданного останова экземпляра могут возникать 
проблемы при загрузке.
 Например, экземпляр может потребовать выполнение <code>fsck</code> в корневом разделе.
 Если подобное произойдет, пользователь может использовать VNC консоль инструментальной панели 
 (Dashboard) для исправления подобных проблем.</p>
<p>Если экземпляр не загружается, что означает, что <code>virsh list</code> никогда не показывает 
экземпляр даже предпринимающим попытку загрузки, выполните следующее на вычислительном узле:
 <br /><code>
# tail -f /var/log/nova/nova-compute.log<br />
</code></p>
<p>Попробуйте еще раз выполнение команды <code>nova reboot</code> снова.
 Вы должны будете увидеть сообщение об ошибке, объясняющее почему экземпляр был не в состоянии загрузиться.</p>
<p>В большинстве случаев ошибка обусловлена чем-то в файле libvirt XML 
(<code>/etc/libvirt/qemu/instance-xxxxxxxx.xml</code>), который больше не существует.
 Вы можете применить восстановление файла XML, а также перезагрузку экземпляра, выполнив:
 <br /><code>
# nova reboot --hard &lt;uuid&gt;<br />
</code></p>

<h3 id="Ch110204">Проверка и восстановление данных после отказа экземпляра</h3>
<p>При некотором развитии событий экземпляры работают, однако не доступны через SSH и не отвечают 
ни на какие команды.
 Консоль VNC может отобразить ошибки загрузки или об ошибках тревоги ядра (kernel panic).
 Это может быть признаком разрушения файловой системы на самой виртуальной машине.
 Если вам необходимо восстановить файлы или проверить содержимое экземпляра, для монтирования диска 
 может использоваться <code>qemu-nbd</code>.</p>
<p>Если вы осуществляете доступ или просматриваете содержимое и данные пользователя, вначале 
получите его разрешение!</p>
<p>Для доступа к диску экземпляра (<code>/var/lib/nova/instances/instance-xxxxxx/disk</code>) 
необходимо соблюдать следующие шаги:<ol>
 <li>Временно остановите экземпляр с помощью команды <code>virsh</code>
 <li>Подключите к диску устройство <code>qemu-nbd</code>
 <li>Смонтируйте устройство <code>qemu-nbd</code>
 <li>Размонтируйте устройство после осмотра
 <li>Отсоедините устройство <code>qemu-nbd</code>
 <li>Возобновите экземпляр
 </ol></p>
<p>Если вы не выполните шаги с 4 по 6, OpenStack Compute больше не сможет управлять экземпляром.</p>
<p>Он не будет отвечать ни на какие выполняемые OpenStack Compute команды и помечается остановленным.</p>
<p>После того, как вы смонтируете дисковый файл, вы должны иметь возможность доступа к нему и 
рассматривать его как обычный каталог с файлами и структурами каталогов.
 Тем не менее, мы не рекомендуем вам редактировать или брать любые файлы, поскольку это может изменить 
 acl и сделать экземпляр незагружаемым, если этого еще не произошло.</p>
<p><ol>
 <li>Приостановите экземпляр с помощью команды <code>virsh</code> - принимая во внимание 
 внутренний идентификатор (ID).
 <br /><code>
root@compute-node:~# virsh list<br />
Id Name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;State<br />
----------------------------------<br />
1 &nbsp;instance-00000981 running<br />
2 &nbsp;instance-000009f5 running <br />
30 instance-0000274a running<br />
<br />
root@compute-node:~# virsh suspend 30<br />
Domain 30 suspended<br />
</code>Присоедините к диску устройство <code>qemu-nbd</code>:
 <br /><code>
root@compute-node:/var/lib/nova/instances/instance-0000274a# ls -lh<br />
total 33M<br />
-rw-rw---- 1 libvirt-qemu kvm 6.3K Oct 15 11:31 console.log<br />
-rw-r--r-- 1 libvirt-qemu kvm 33M Oct 15 22:06 disk<br />
-rw-r--r-- 1 libvirt-qemu kvm 384K Oct 15 22:06 disk.local<br />
-rw-rw-r-- 1 nova nova 1.7K Oct 15 11:30 libvirt.xml<br />
root@compute-node:/var/lib/nova/instances/instance-0000274a# qemu-nbd -c /dev/nbd0 `pwd`/disk<br />
</code>
 <li>Смонтируте  устройство <code>qemu-nbd</code>.
 Например, если vda диск, а vda1 корневой раздел, <code>qemu-nbd</code> экспортирует устройства в 
 <code>/dev/nbd0</code> и в <code>/dev/nbd0p1</code> соответственно.
 <br /><code>
#mount the root partition of the device<br />
root@compute-node:/var/lib/nova/instances/instance-0000274a# mount /dev/nbd0p1 /mnt/<br />
# List the directories of mnt, and the vm's folder is display<br />
# You can inspect the folders and access the /var/log/ files<br />
</code>
 Для изучения второго или эфемерного диска используйте альтернативную точку монтирования, если вы 
 хотите иметь в одно и то же время смонтированными и первичный, и вторичный диски.
 <br /><code>
# umount /mnt<br />
# qemu-nbd -c /dev/nbd1 `pwd`/disk.local<br />
# mount /dev/nbd1 /mnt/<br />
<br />
root@compute-node:/var/lib/nova/instances/instance-0000274a# ls -lh /mnt/<br />
total 76K<br />
lrwxrwxrwx. 1 root root 7 Oct 15 00:44 bin -&gt; usr/bin<br />
dr-xr-xr-x. 4 root root 4.0K Oct 15 01:07 boot<br />
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 dev<br />
drwxr-xr-x. 70 root root 4.0K Oct 15 11:31 etc<br />
drwxr-xr-x. 3 root root 4.0K Oct 15 01:07 home<br />
lrwxrwxrwx. 1 root root 7 Oct 15 00:44 lib -&gt; usr/lib<br />
lrwxrwxrwx. 1 root root 9 Oct 15 00:44 lib64 -&gt; usr/lib64<br />
drwx------. 2 root root 16K Oct 15 00:42 lost+found<br />
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 media<br />
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 mnt<br />
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 opt<br />
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 proc<br />
dr-xr-x---. 3 root root 4.0K Oct 15 21:56 root<br />
drwxr-xr-x. 14 root root 4.0K Oct 15 01:07 run<br />
lrwxrwxrwx. 1 root root 8 Oct 15 00:44 sbin -&gt; usr/sbin<br />
drwxr-xr-x. 2 root root 4.0K Feb 3 2012 srv<br />
drwxr-xr-x. 2 root root 4.0K Oct 15 00:42 sys<br />
drwxrwxrwt. 9 root root 4.0K Oct 15 16:29 tmp<br />
drwxr-xr-x. 13 root root 4.0K Oct 15 00:44 usr<br />
drwxr-xr-x. 17 root root 4.0K Oct 15 00:44 var<br />
</code>
 <li>После того, как вы завершили просмотр, размонтируйте точку монтирования и освободите устройство 
 <code>qemu-nbd</code>
 <br /><code>
root@compute-node:/var/lib/nova/instances/instance-0000274a# umount /mnt<br />
root@compute-node:/var/lib/nova/instances/instance-0000274a# qemu-nbd -d <br />
/dev/nbd0/dev/nbd0 disconnected<br />
</code>
 <li>Возобновите экземпляр с помощью <code>virsh</code>
<br /><code>
root@compute-node:/var/lib/nova/instances/instance-0000274a# virsh list<br />
Id Name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;State<br />
----------------------------------<br />
1 &nbsp;instance-00000981 running<br />
2 &nbsp;instance-000009f5 running<br />
30 instance-0000274a paused<br />
root@compute-node:/var/lib/nova/instances/instance-0000274a# virsh resume 30<br />
Domain 30 resumed<br />
</code>
 </ol></p>

<h3 id="Ch110205">Тома</h3>
<p>Если поврежденные экземляры также имели присоединенные тома, сначала создайте список UUID экземпляров и томов:
 <br /><code>
mysql&gt; select nova.instances.uuid as instance_uuid, cinder.volumes.id as volume_uuid, cinder.volumes.status, <br />
cinder.volumes.attach_status, cinder.volumes.mountpoint, cinder.volumes.display_name from cinder.volumes<br />
inner join nova.instances on cinder.volumes.instance_uuid=nova.instances.uuid 
where nova.instances.host = 'c01.example.com';<br />
</code></p>
<p>Вы должны увидеть результат, подобный приводимому ниже:
 <br /><code>
+--------------+------------+-------+--------------+-----------+--------------+<br />
|instance_uuid |volume_uuid |status |attach_status |mountpoint | display_name |<br />
+--------------+------------+-------+--------------+-----------+--------------+<br />
|9b969a05 &nbsp; &nbsp; &nbsp;|1f0fbf36 &nbsp; &nbsp;|in-use |attached &nbsp; &nbsp; &nbsp;|/dev/vdc &nbsp; | test &nbsp; &nbsp; &nbsp; &nbsp; |<br />
+--------------+------------+-------+--------------+-----------+--------------+<br />
1 row in set (0.00 sec)<br /></code></p>
<p>Далее вручную отключите и повторно подключите тома:
 <br /><code>
# nova volume-detach &lt;instance_uuid&gt; &lt;volume_uuid&gt;<br />
# nova volume-attach &lt;instance_uuid&gt; &lt;volume_uuid&gt; /dev/vdX<br />
</code></p>
<p>Где <code>X</code> - соответствующая точка монтирования.
 Перед выполнением приведенных выше действий убедитесь, что экземпляр успешно загрузился и находится 
 на окне регистрации (login screen).</p>

<h3 id="Ch110206">Полный отказ вычислительного узла</h3>
<p>Если вычислительный узел отказал и не может быть восстановлен в течение нескольких часов или более, 
вы можете возобновить все размещенные на отказавшем узле экземпляры, если вы используете 
совместно используемую систему хранения для <code>/var/lib/nova/instances</code>.</p>
<p>Чтобы сделать это, создайте список UUID размещенных на отказавшем узле экземпляров, выполнив 
следующий запрос к базе данных nova:
 <br /><code>
mysql&gt; select uuid from instances where host = 'c01.example.com' and deleted = 0;<br />
</code></p>
<p>Далее сообщите Nova, что все экземпляры, которые раньше были размещены на c01.example.com, теперь 
размещаются на c02.example.com:
 <br /><code>
mysql&gt; update instances set host = 'c02.example.com' where host = 'c01.example.com' and deleted = 0;<br />
</code></p>
<p>После этого, используйте команду <code>nova</code> для перезагрузки всех экземпляров, 
которые были запущены на на c01.example.com, одновременно генерируя их XML файлы:
 <br /><code>
# nova reboot --hard &lt;uuid&gt;<br />
</code></p>
<p>Наконец, вновь присоедините тома, используя метод, аналогичный описанному в разделе 
<strong><a href="#Ch110205">Тома</a></strong></p>

<h3 id="Ch110207">/var/lib/nova/instances</h3>
<p>Стоит отметить, этот каталог в контексте отказавших вычислительных узлов.
  Этот каталог содержит образы дисков libvirt KVM на файловой основе для экземпляров, размещенных на 
  этом вычислительном узле.
  Если вы не работаете со своим облаком в среде совместно используемой системы хранения, этот каталог 
  является уникальным во всех вычислительных узлах.</p>
<p><code>/var/lib/nova/instances</code> содержит два типа каталогов.</p>
<p>Первый является _base.
 Он содержит все кэшированные основные образы из glance для каждого уникального образа, который был 
 запущен на вычислительном узле.
 Файлы, завершающиеся <code>_20</code> (или другим номером), являются эфемерными базовыми образами.</p>
<p>Остальные каталоги имеют название <code>instance-xxxxxxxx</code>.
 Эти каталоги соответствуют экземплярам, работающим на данном вычислительном узле.
 Файлы внутри относятся к одному из файлов в каталоге <code>_base</code>.
 Они являются файлами, существенно основанными на разницах, которые содержат только внесенные в 
 оригинальный каталог <code>_base</code> изменения.</p>
<p>Все файлы и директории в <code>/var/lib/nova/instances</code> именованы уникально.
 Файлы в _base имеют уникальные имена для образа glance, на котором они основаны, а имена каталогов 
 <code>instance-xxxxxxxx</code> имеют уникальные для данного конкретного экземпляра.
 Например, если вы копируете все данные из <code>/var/lib/nova/instances</code> с одного вычислительного 
 узла на другой, вы не перезаписываете все файлы и не наносите ущерб образам, которые имеют такое же 
 уникальное имя, поскольку они являются по- существу, тем же самым файлом.</p>
<p>Хотя этот метод не задокументирован и не поддерживается, вы можете использовать его, когда ваш 
вычислительный узел постоянно отключается, но у вас есть экземпляры локально хранящиеся на нем.</p>

<h2 id="Ch1103">Отказы узлов хранения и их обслуживание</h2>
<p>В связи с высокой отказоустойчивостью хранилищ объектов, обусловленной тем, что проблемы узлов 
хранения объектов намного проще, по сравнению с проблемами, с которыми мы сталкиваемся на вычислительных узлах.</p>

<h3 id="Ch110301">Перезагрузка узла хранения</h3>
<p>Если узлу хранения требуется перезагрузка- просто перезагрузите его.
 Запросы к размещенным на нем данным будут перенаправлены на другие копии до окончания процесса загрузки сервера.</p>

<h3 id="Ch110302">Завершение работы узла хранения</h3>
<p>Если вам нужно выключить узел хранения на длительный период времени (от одних суток и более), 
рассмотрите возможность удаления узла из кольца системы хранения.
 Например:
 <br /><code>
# swift-ring-builder account.builder remove &lt;ip address of storage node&gt;<br />
# swift-ring-builder container.builder remove &lt;ip address of storage node&gt;<br />
# swift-ring-builder object.builder remove &lt;ip address of storage node&gt;<br />
# swift-ring-builder account.builder rebalance<br />
# swift-ring-builder container.builder rebalance<br />
# swift-ring-builder object.builder rebalance <br />
</code></p>
<p>Далее, перераспределите файлы кольца на другие узлы:
 <br /><code>
# for i in s01.example.com s02.example.com s03.example.com<br />
&gt; do<br />
&gt; scp *.ring.gz $i:/etc/swift<br />
&gt; done <br />
</code></p>
<p>Данные действия эффективно изымают узел хранения из кластера хранения.</p>
<p>Когда узел способен присоединиться к кластеру, просто опять добавьте его в кольцо.
 Точный синтаксис добавления узла в ваш кластер Swift с использованием <code>swift-ring-builder</code> 
 сильно зависит от исходных параметров, использовавшихся при первоначальном построении вашего кластера.
 Обратитесь, пожалуйста, к тем командам.</p>

<h3 id="Ch110303">Замена диска Swift</h3>
<p>В случае выхода из строя диска на узле хранения объектов его замена относительно проста.
 Она предполагает, что среда вашего хранилища объектов настроена правильно, т.е. хранящиеся на 
 отказавшем диске данные также дублированы на другие диски в среде хранения объектов. </p>
<p>В данном примере предполагается, что отказал <code>/dev/sdbhas</code>.</p>
<p>Во- первых размонтируйте диск:
 <br /><code>
# umount /dev/sdb<br />
</code></p>
<p>Далее, физически удалить диск из сервера и замените его на работающий диск.
Убедитесь, что операционная система распознала новый диск:
<br /><code>
# dmesg | tail<br />
</code></p>
<p>Вы должны увидеть сообщение о <code>/dev/sdb</code>.
Поскольку рекомендуется не использовать разделы на диске swift, просто отформатируйте диск целиком:
 <br /><code>
# mkfs.xfs /dev/sdb<br />
</code></p>
<p>В конце смонтируйте диск:
 <br /><code>
# mount -a<br />
</code></p>
<p>Swift должен увидеть новый диск и убедиться, что не существует никаких данных.
  Затем он начинает репликацию данных на диск с других существующих копий.</p>

<h2 id="Ch1104">Обработка полного отказа</h2>
<p>Обычный способ справиться с восстановлением при полном отказе системы, например, при при 
отключении электропитания в центре обработки данных, заключается в присвоении каждой службе приоритета 
с последующим восстановлением по порядку.</p>
<p>
       1 Внутрисетевое подключение
 <br />2 Службы резервирования систем хранения
 <br />3 Подключение к общедоступной сети для пользователей виртуальных машин
 <br />4 Хосты nova-compute, nova-network, cinder
 <br />5 Виртуальные машины пользователей
 <br />10 Службы очередей сообщений и базы данных
 <br />15 Службы Keystone
 <br />20 Планировщик cinder
 <br />21 Службы каталога образов и доставки
 <br />22 Службы nova-scheduler
 <br />98 Cinder-api
 <br />99 Службы Nova-api
 <br />100 Узел инструментальной панели
 </p>
<p>Используйте приведенный пример списка приоритетов, чтобы быть уверенным, что связанные с пользователем 
службы будут восстановлены как можно быстрее, но не раньше, чем вернется на место стабильная среда.
 Конечно, несмотря на то, что каждый шаг указан в качестве одной строки, он требует значительной работы.
 Например, сразу после запуска базы данных, вы должны проверить ее целостность, или, после запуска служб 
 nova вы должны убедиться, что гипервизор совпал с базой данных и исправлены все несоответствия.</p>

<h2 id="Ch1105">Управление конфигурацией</h2>
<p>Обслуживание облака OpenStack требует чтобы вы управляли множеством физических серверов и их число 
может расти со временем.
 Поскольку управление узлами вручную является способствующим появлению ошибок, мы настоятельно рекомендуем 
 вам использовать средства управления конфигурацией.
 Эти инструменты автоматизируют процесс обеспечения того, чтобы все узлы были настроены правильно и 
 содействуют вам в поддержке информации о конфигурации (такой, как параметры пакетов и конфигурации) в 
 контролируемом версиями хранилище (repository).</p>
<p>Доступны различные инструменты управления конфигурацией и данное руководство не рекомендует какой- то 
один определенный.
 Двумя наиболее популярными в сообществе OpenStack являются <span class="red-heading">Puppet</span> 
 (<a href="https://puppetlabs.com/">https://puppetlabs.com/</a>) с доступными 
 <span class="red-heading">OpenStack Puppet modules</span> 
 (<a href="http://github.com/puppetlabs/puppetlabs-openstack">
 http://github.com/puppetlabs/puppetlabs-openstack</a>) и 
 <span class="red-heading">Chef</span> (
 <a href="http://opscode.com/chef">http://opscode.com/chef</a>) с доступными 
 <span class="red-heading">OpenStack Chef recipes</span> 
 (<a href="https://github.com/opscode/openstack-chef-repo">https://github.com/opscode/openstack-chef-repo</a>).
 Другие, более новые средства настройки включают 
 <span class="red-heading">Juju</span> 
 (<a href="https://juju.ubuntu.com/">https://juju.ubuntu.com/</a>), 
 <span class="red-heading">Ansible</span> (<em>Прим. перев.: по всей видимости, необходимо 
 пользоваться ссылкой <a href="http://www.ansible.com">http://www.ansible.com</a>, 
 вместо приводимой в первоисточнике: 
 <a href="http://ansible.cc">http://ansible.cc</a></em>) и 
 <span class="red-heading">Salt</span> (<a href="http://saltstack.com">http://saltstack.com</a>), 
 а более зрелые инструменты управления конфигурацией включают 
 <span class="red-heading">CFEngine</span> 
 (<a href="http://cfengine.com">http://cfengine.com</a>) и 
 <span class="red-heading">Bcfg2</span> (<a href="http://bcfg2.org">http://bcfg2.org</a>).</p>

<h2 id="Ch1106">Работа с аппаратными средствами</h2>
<p>Аналогично с начальным развертыванием, вы должны убедиться, что все оборудование надлежащим образом 
включено перед его добавлением в работу.
  Запустите используемое аппаратными средствами программное обеспечение на свои ограничения - 
  используйте до предела возможности ОЗУ, ЦПУ, диска и сети.
  Доступно много опций, причем обычно они дублируются в виде программного обеспечения замера 
  производительности, следовательно вы получаете хорошее представление о производительности вашей системы.</p>

<h3 id="Ch110601">Добавление вычислительного узла</h3>
<p>Если вы обнаружите, что вы уже достигли или приближаетесь к пределу пропускной способности ваших 
вычислительных ресурсов, вы должны планировать добавление дополнительных вычислительных узлов.
  Выполнить добавление дополнительных узлов довольно просто.
  Процесс добавления узлов аналогичен размещению первых вычислительных узлов в вашем облаке: 
  используйте автоматизированную систему развертывания для самозагрузки сервера для работы на 
  аппаратном уровне с операционной системой, а затем воспользуйтесь системой управления конфигурацией 
  для установки и настройки службы OpenStack Compute.
  Как только служба Compute установлена и настроена аналогично другим вычислительным узлам, она автоматически привязывается к облаку.
  Контроллер облака замечает новый узел/ узлы и начинает планирование для запуска там экземпляров.</p>
<p>Если узлы блочного хранилища OpenStack размещаются раздельно с вычислительными узлами, та же процедура 
применяется к одним и тем же очередям и системам опроса используемым в обеих службах.</p>
<p>Мы рекомендуем вам использовать одно и то же оборудование для новых вычислительных узлов и узлов 
блочного хранения.
 По крайней мере убедитесь, что используются процессоры одного типа, чтобы не прерывать миграцию в 
 онлайн- режиме.</p>

<h3 id="Ch110602">Добавление узла хранения объектов</h3>
<p>Добавление нового узла хранения объектов отличается от добавления вычислительного узлов или узлов 
блочного хранилища.
 Вам все еще требуется использование ваших систем управления автоматическим развертыванием и настройкой 
 для начальной настройки.
 После выполнения этих действий вам нужно добавить локальные диски узла хранения объектов в кольцо 
 системы хранения.
 Правильная команда выполнения подобного действия абсолютно та же, что и команда, выполнявшаяся для 
 первоначального добавления дисков в кольцо.
 Просто повторно выполните эту команду на прокси- сервере хранения объектов для всех дисков на новом 
 узле хранения объектов.
 Как только это будет сделано, сбалансируйте кольцо и скопируйте полученные файлы кольца на другие 
 узлы хранения объектов.</p>
<p><img src="i/Tip.jpg" alt="Tip" />Если ваш новый узел хранения объектов имеет количество дисков 
отличное от значения для оригинальных узлов, команда добавления нового узла отличается от 
первоначальных команд.
  Эти параметры меняются от среды к среде.</p>

<h3 id="Ch110603">Замена компонентов</h3>
<p>В крупномасштабных реализациях, подобных облачным инфраструктурам, отказы оборудования обычны.</p>
<p>Продумайте свои процессы и сбалансируйте сохранение времени и доступность.
 Например, кластер хранения объектов легко может жить с отказавшими дисками в течение некоторого 
 промежутка времени, если он имеет достаточную емкость.
 Или же, если система вычислительных узлов не заполнена вы можете рассмотреть онлайн- миграцию 
 экземпляров с хоста, имеющего сбои ОЗУ, пока у вас есть время на решение данной проблемы.</p>

<h2 id="Ch1107">Базы данных</h2>
<p>Почти все компоненты OpenStack имеют в своей основе для хранения постоянной информации базу данных.
 Обычно MySQL является такой базой данных.
 К этим базам данных применимо стандартное администрирование MySQL.
 OpenStack не настраивает базы данных экстраординарным способом.
 Основное администрирование включает в себя улучшение производительности, высокую доступность, 
 резервное копирование, восстановление и исправление.
 Для получения дополнительной информации ознакомьтесь со стандартным руководством администрирования MySQL.</p>
<p>Вы можете выполнить ряд трюков с базой данных для более быстрого получения информации 
или исправления ошибок несогласованности данных.
  Например, экземпляр был прекращен, но статус не был обновлен в базе данных.
  Подобные трюки обсуждаются в этой книге.</p>

<h3 id="Ch110701">Связь с базой данных</h3>
<p>Просмотрите файл конфигурации компонентов, чтобы увидеть, как каждый компонент OpenStack 
получает доступ к соответствующей базе данных.
Ищите <code>sql_connectionor</code> или просто <code>connection</code>:
<br /><code>
 # grep -hE &quot;connection ?=&quot; /etc/nova/nova.conf /etc/glance/glance-*.conf <br />
/etc/cinder/cinder.conf /etc/keystone/keystone.conf<br />
 sql_connection = mysql://nova:nova@cloud.alberta.sandbox.cybera.ca/nova<br />
 sql_connection = mysql://glance:password@cloud.example.com/glance <br />
 sql_connection = mysql://glance:password@cloud.example.com/glance <br />
sql_connection=mysql://cinder:password@cloud.example.com/cinder <br />
 connection = mysql://keystone_admin:password@cloud.example.com/keystone<br />
</code></p>
<p>Строки подключения имеют следующий формат:
 <br /><code>
mysql:// &lt;username&gt; : &lt;password&gt; @ &lt;hostname&gt; / &lt;database name&gt;<br />
</code></p>

<h3 id="Ch110702">Производительность и оптимизация</h3>
<p>По мере роста облака MySQL используется все больше и больше.
 Если вы подозреваете, что MySQL может стать узким местом, вы должны начать изучение оптимизации MySQL.
 Руководство MySQL имеет целый раздел, посвященный этой теме 
 <span class="red-heading">Optimization Overview</span>
 (<a href="http://dev.mysql.com/doc/refman/5.5/en/optimize-overview.html">
 http://dev.mysql.com/doc/refman/5.5/en/optimize-overview.html</a>).</p>

<h2 id="Ch1108">HDWMY</h2>
<p>Приведем краткий список различных задач на выполнение каждый час, день, неделю, месяц и год.
 Пожалуйста, обратите внимание, что эти задачи не являются обязательными и предписанными, однако их 
 выполнение - полезная идея:
</p>

<h3 id="Ch110801">Каждый час</h3>
<ul>
 <li>Проверяйте вашу систему мониторинга на наличие предупреждений и реагируйте на них.
 <li>Проверяйте очередь билетов на доступ к объектам на предмет новых билетов.
</ul>

<h3 id="Ch110802">Ежедневно</h3>
<ul>
 <li>Проверьте наличие экземпляров в состоянии отказа или в странном состоянии и 
 выясните причину. 
 <li>Проверьте появление исправлений безопасности и примените их в случае необходимости.
</ul>

<h3 id="Ch110803">Еженедельно</h3>
<ul>
 <li>Проверьте использование облака: <ul>
  <li>Квоты пользователя
  <li>Дисковое пространство
  <li>Использование образов
  <li>Большие экземпляры
  <li>Использование сети (пропускная способность и использование IP).
 </ul>
 <li>Убедитесь, что ваши механизмы предупреждения все еще работают.
</ul>

<h3 id="Ch110804">Ежемесячно</h3>
<ul>
 <li>Проверьте использование и тренды за прошедший месяц.
 <li>Проверьте наличие пользовательских учетных записей, подлежащих удалению.
 <li>Проверьте наличие учетных записей операторов, подлежащих удалению.
</ul>

<h3 id="Ch110805">Ежеквартально</h3>
<ul>
 <li>Проведите анализ использования и трендов за последний квартал.
 <li>Подготовьте все квартальные отчеты об использовании и статистике. 
 <li>Проведите анализ и планирование необходимых добавлений в облако.
 <li>Проведите анализ и планирование главных обновлений OpenStack.
</ul>

<h3 id="Ch110806">Каждые полгода</h3>
<ul>
 <li>Обновляйте OpenStack.
 <li>Проведите очистку после обновления OpenStack (вы должны быть в курсе о наличии всех 
 неиспользуемых или новых служб?)
</ul>

<h2 id="Ch1109">Выявление неисправных компонентов</h2>
<p>Сбор OpenStack-ом сведений о том как взаимодействуют между собой различные компоненты обязателен.
 Например, загрузка образов требует взаимодействия от <code>nova-api</code>, </code>glance-api</code>, 
 <code>glance-registry</code>, <code>Keystone</code> и, потенциально, <code>swift-proxy</code>.
 В результате, иногда бывает трудно определить точно источник проблемы.
  Цель данного раздела заключается в помощи в этом.
</p>

<h3 id="Ch110901">Хвосты журналов</h3>
<p>Первое место для просмотра файла журнала, связанного с командой, которую вы пытаетесь запустить.
 Например, если nova <code>list</code> завершилась неудачей, попробуйте запустить просмотр хвоста 
 файла журнала Nova и запустите команду снова:
<br />Terminal 1:
<br /><code>
 &nbsp; # tail -f /var/log/nova/nova-api.log<br />
</code>
<br />
<br />Terminal 2:
<br /><code>
 &nbsp; # nova list<br />
</code></p>
<p>Ознакомьтесь со всеми ошибками и трассировкой в файле журнала.
 Для получения дополнительной информации ознакомьтесь с главой 
 <strong><a href="Ch13ru.htm">Ведение журналов и мониторинг</a></strong>.</p>
<p>Например, если nova не может получить доступ к glance, просмотрите журнал glance-api:
<br />Terminal 1:
<br /><code>
 &nbsp; # tail -f /var/log/glance/api.log<br />
</code>
<br />
<br />Terminal 2:
<br /><code>
 &nbsp; # nova list<br />
</code></p>
<p>Промывайте, полощите и повторяйте пока не найдете основную причину проблемы.</p>

<h3 id="Ch110902">Запуск демонов с использованием интерфейса командной строки</h3>
<p>К сожалению, иногда ошибка не очевидна из файлов журналов.
  В этом случае измените тактику и используйте различные команды, возможно, запустите 
  службу непосредственно в командной строке.
  Например, если служба <code>glance-api</code> отказывается стартовать и остается работающей, 
  попробуйте запустить демон из командной строки:
<br /><code>
# sudo -u glance -H glance-api<br />
</code></p>
<p>Это может вывести ошибку и причину проблемы.</p>
<p><img src="i/Tip.jpg" alt="Tip" />При работе демона с sudo необходим флаг <code>-H</code>, так как 
некоторые демоны будут записывать файлы относительно домашнего каталога пользователя и данная запись 
может окончиться неудачей при опускании <code>-H</code>.</p>

<h3 id="Ch110903">Пример осложнений</h3>
<p>Однажды утром, вычислительному узлу не удалось запустить никакие экземпляры.
 Файлы журналов были слегка туманны, утверждая, что определенный экземпляр не смог стартовать.
 Такое завершение оказалось ложным следом, поскольку данный экземпляр был просто первым экземпляром 
 в алфавитном порядке, так что это был первый экземпляр, с которого должен был начать nova-compute.</p>
<p>Дальнейший поиск неисправностей показал, что libvirt вовсе не работает.
  Это имело больше смысла.
  Если libvirt не работает, то никакой экземпляр не может виртуализироваться с использованием KVM.
  После попытки запуска libvirt, она сразу молча падает.
  Журналы libvirt не объясняют причину.</p>
<p>Далее, демон <code>libvirtd</code> был запущен в командной строке.
 Наконец, полезное  сообщение об ошибке: он не может подключиться к d-bus.
 Как не смешно это звучит, libvirt, и, следовательно, <code>nova-compute</code>, полагаются на 
 <code>d-bus</code> и по какой-то причине d-bus отказывает.
 Простой старт <code>d-bus</code> устанавливает всю цепочку необходимым образом и вскоре все 
 восстанавливается и работает.</p>

<h2 id="Ch1110">Обновления</h2>
<p>За исключением систем хранения объектов, обновление с одной версии OpenStack на другую 
требует большого объема работы.</p>
<p>Процесс обновления обычно выполняет следующие действия:<ol>
 <li>Прочитайте пояснительную записку к релизу.
 <li>Найдите несовместимости между различными версиями.
 <li>Спланируйте расписание обновления и выполните его по порядку на испытательном кластере.
 <li>Запустите обновление.
</ol></p>
<p>Вы можете выполнять обновление при запущенных экземплярах.
 Тем не менее, такой подход может быть опасным.
 Не забывайте о соотвествующих уведомлениях пользователям и резервном копировании.</p>
<p>Наиболее общий, кажущийся успешным, порядок следующий:<ol>
 <li>Обновите службу идентификации OpenStack (keystone).
 <li>Обновите службу образов OpenStack (glance).
 <li>Обновите службы вычислительных узлов OpenStack (nova).
 <li>Обновите службы блочных хранилищ OpenStack (cinder).
</ol></p>
<p>Для каждого из этих шагов выполните следующие под-этапы:<ol>
 <li>Остановите службы.
 <li>Создайте резервные копии фалов конфигурации и баз данных.
 <li>Обновите пакеты с использованием менеджера пакетов из вашего дистрибутива.
 <li>Обновите файлы конфигурации в соответствии с пояснительной запиской релиза.
 <li>Примените обновления базы данных.
 <li>Перезапустите службы.
 <li>Убедитесь, что все работает.
</ol></p>
<p>Вероятно, наиболее важным шагом из всех является тестирование перед обновлением.
 Особенно, если вы обновляете сразу после выпуска новой версии, не обнаруженные ошибки могут помешать выполнению.
 Некоторые установщики предпочитают подождать пока не будет объявлен выпуск точка один.
 Тем не менее, если у вас есть большая система, вы можете отслеживать разработку и тестировать релиз, 
 тем самым гарантируя, что фиксированы ошибки для ваших случаев использования.</p>
<p>Чтобы выполнить обновление OpenStack Compute, сохраняя при этом работающие экземпляры, вы должны иметь 
возможность использовать он-лайн миграцию для перемещения машины в системе по мере выполнения обновлений, 
а затем вернуть их обратно после того, так как это станет обязанностью гипервизора.
  Тем не менее, очень важно убедиться, что изменения в базе данных были успешно выполнены, в противном 
  случае может возникнуть противоречивое состояние кластера.</p>
<p>Хорошей мыслью также является выполнение некоторой &quot;очистки&quot; кластера перед запуском 
обновления для того, чтобы убедиться в согласованности.
 Например, иногда имеются сообщения о проблемах с экземплярами, которые не были полностью стерты из 
 системы после их удаления.
 Запустите команду эквивалентную следующей:
<br /><code>
 &nbsp; $ virsh list --all
</code><br />
 для поиска удаленных экземпляров, которые до сих пор зарегистрированы в гипервизоре,а удаление их 
 до запуска обновления может помочь избежать проблем.</p>

<h2 id="Ch1111">Деинсталляция</h2>
<p>Хотя мы всегда рекомендовали бы выполнять деинсталляцию систем с помощью автоматизированной 
системы развертывания с нуля, иногда вы должны удалить OpenStack из системы тяжелым способом.
  Приведем как:<ul>
 <li>Удалите все пакеты
 <li>Удалите все оставшиеся файлы
 <li>Удалите базы данных</ul></p>
<p>Эти действия зависят от вашего исходного дистрибутива, но в целом вы должны искать команды 
&quot;purge&quot; в менеджере пакетов, подобные <code>aptitude purge ~c $package</code>.
  Затем вы можете осуществить поиск потерянных файлов в каталогах, упомянутых в данном руководстве.
  Для удаления должным образом базы данных, обратитесь к руководству, соответствующего используемому 
  вами продукту.</p>

<table width="100%" border="0" cellpadding="2" cellspacing=0 class="bg_White"><tbody><tr>
 <td align="left"><a href="Ch10ru.htm">Глава 10</a></td>
 <td align="center"><a href="index.htm">Оглавление</a></td>
 <td align="right"><a href="Ch12ru.htm">Глава 12</a></td>
</tr><tr><td colspan="3" style="border-bottom: thin solid;">&nbsp;</tr>
<tr><td colspan="2" valign="top">Перевод: Copyright ©&nbsp;2015 &nbsp;<img src="/i/mdl-reg.jpg" widht="35" height="12" style="border-style: none;">.<br>
All rights reserved.<br />
Ссылки обязательны (Refs and links obligatory).</td>
<td valign="top" align="right"><em><a href="http://www.mdl.ru">http://www.mdl.ru</a></em></td></tr>
</tbody></table>


<td align="right" valign="top">
<script language="javascript">
WriteABC('GPFS');
//--></script>
</tr>
</tbody></table>
</tbody></table>
</body>
</html>
