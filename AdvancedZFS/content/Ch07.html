<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xmlns:ng="http://docbook.org/docbook-ng" xmlns:fb="http://ogp.me/ns/fb#">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link href="https://netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css" rel="stylesheet"/>
<title>Глава 7. Кэширование - Мастерство FreeBSD: ZFS для профессионалов</title>
<meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"/>
<meta name="mavenGroupId" content="www.mdl.ru"/>
<meta name="mavenArtifactId" content="AdvancedZFS"/>
<meta name="mavenVersionId" content="1.0.0"/>
<link rel="home" href="index.html" title="Мастерство FreeBSD: ZFS для профессионалов"/>
<link rel="up" href="index.html" title="Мастерство FreeBSD: ZFS для профессионалов"/>
<link rel="prev" href="Ch06.html" title="Глава 6. Профессиональное оборудование"/>
<link rel="next" href="Ch08.html" title="Глава 8. Производительность"/>
<meta name="git-sha" content=""/>
<meta name="buildTime" content=""/>
<script type="text/javascript">
            //The id for tree cookie
            var treeCookieId = "advanced-zfs";
            var language = "en";
            var w = new Object();
            //Localization
            txt_filesfound = 'Results';
            txt_enter_at_least_1_char = "You must enter at least one character.";
            txt_browser_not_supported = "Please enable JavaScript.";
            txt_please_wait = "Please wait. Search in progress...";
            txt_results_for = "Results for: ";
</script>
<style type="text/css">
            input {
            margin-bottom: 5px;
            margin-top: 2px;
            }

            .folder {
            display: block;
            height: 22px;
            padding-left: 20px;
            background: transparent url(../common/jquery/treeview/images/folder.gif) 0 0px no-repeat;
            }
</style>
<link rel="shortcut icon" href="../MdlLogo.gif" type="image/gif"/>
<link rel="stylesheet" type="text/css" href="../common/css/positioning.css"/>
<link rel="stylesheet" type="text/css" href="../common/css/custom.css"/>
<link rel="canonical" href="http://onreader.mdl.ru/FreeBSDMasteryZFS/content/index.html"/>
<!--[if IE]>
	<link rel="stylesheet" type="text/css" href="../common/css/ie.css"/>
<![endif]-->
<link rel="stylesheet" type="text/css" href="../common/jquery/theme-redmond/jquery-ui-1.8.2.custom.css"/>
<link rel="stylesheet" type="text/css" href="../common/jquery/treeview/jquery.treeview.css"/>
<script type="text/javascript" src="http://code.jquery.com/jquery-1.11.0.min.js"><!----></script>
<script type="text/javascript" src="../common/jquery/jquery-ui-1.8.2.custom.min.js"><!----></script>
<script type="text/javascript" src="../common/jquery/jquery.cookie.js"><!----></script>
<script type="text/javascript" src="../common/jquery/treeview/jquery.treeview.min.js"><!----></script>
<link rel="stylesheet" type="text/css" href="http://cdn.jsdelivr.net/qtip2/2.2.0/jquery.qtip.min.css"/>
<script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/qtip2/2.2.0/jquery.qtip.min.js">
<!--jQuery plugin for glossary popups. --></script><script type="text/javascript" src="search/htmlFileList.js"><!----></script>
<script type="text/javascript" src="search/htmlFileInfoList.js"><!----></script>
<script type="text/javascript" src="search/nwSearchFnt.js"><!----></script>
<script type="text/javascript" src="search/stemmers/en_stemmer.js">
<!--//make this scalable to other languages as well.--></script>
<script type="text/javascript" src="search/index-1.js"><!----></script>
<script type="text/javascript" src="search/index-2.js"><!----></script>
<script type="text/javascript" src="search/index-3.js"><!----></script>
<script type="text/javascript">
	    var _gaq = _gaq || [];
	    _gaq.push(['_setAccount', 'UA-17511903-1']);
	    
	    _gaq.push(['_setDomainName', '.openstack.org']);	        
</script>
<script type="text/javascript" src="../common/ga.js"><!----></script>
<script language="javascript" src="/js/common.js"></script>
<link rel="stylesheet" href="../common/css/googlecode.css">
<script src="../common/highlight.pack.js"></script>
</head>
<body>
<!----><script type="text/javascript"><!--
hljs.initHighlightingOnLoad();
HeaderName = 'Глава 7. Кэширование';
PrevRef = 'Ch06.html';
UpRef = 'index.html';
NextRef = 'Ch08.html';//-->
</script>
<!----><script type="text/javascript" src="HeaderAndToolbar.js">
</script><script type="text/javascript"><!--
document.write(HeaderAndToolbar); //-->
</script>
<div id="content">
 <div class="part">
  <div xmlns="" class="titlepage"><div><div><h1 xmlns="http://www.w3.org/1999/xhtml" class="title">
   Глава 7. Кэширование</h1>
  </div></div></div>

  <div class="toc"><p><strong>Содержание</strong></p>
   <dl>
   <dt><span class="chapter"><a href="Ch07.html">7. Кэширование</a></span></dt>
   <dd><dl>
     <dt><span class="chapter"><a href="Ch07.html#01">Кэш адаптивной замены</a></span></dt>
     <dd><dl>
       <dt><span class="chapter"><a href="Ch07.html#0101">Традиционный буферирующий кэш</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0102">Проектирование ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0103">Использование памяти ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0104">Zfs-stats</a></span></dt>
     </dl></dd>
     <dt><span class="chapter"><a href="Ch07.html#02">Модификации ARC</a></span></dt>
     <dd><dl>
       <dt><span class="chapter"><a href="Ch07.html#0201">Ограничение размера ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0202">Метаданные и ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0203">Наборы данных и ARC</a></span></dt>
     </dl></dd>
     <dt><span class="chapter"><a href="Ch07.html#03">ARC 2 уровня</a></span></dt>
     <dd><dl>
       <dt><span class="chapter"><a href="Ch07.html#0301">Использование памяти L2ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0302">Кэширование L2ARC</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0303">Потоковая обработка файлов</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0304">Скорость записи L2ARC</a></span></dt>
     </dl></dd>
     <dt><span class="chapter"><a href="Ch07.html#04">Целевой журнал ZFS</a></span></dt>
     <dd><dl>
       <dt><span class="chapter"><a href="Ch07.html#0401">Синхронные и асинхронные транзакции</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0402">Целевой журнал ZFS</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0403">Отдельный целевой журнал ZFS</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0404">Настройка целевого журнала по наборам данных</a></span></dt>
       <dt><span class="chapter"><a href="Ch07.html#0405">Выполнение синхронных записей в стеке</a></span></dt>
     </dl></dd>
     <dt><span class="chapter"><a href="Ch07.html#05">zpool.cache</a></span></dt>
   </dl></dd>
   </dl>
  </div>
   <p>Как и любая другая файловая система, ZFS применяет кэширование в памяти для увеличения производительности.
   Однако, в отличие от большинства остальных файловых систем, системные администраторы могут выполнять тонкую 
   настройку этих кэшей для регулирования поведения системы. ZFS кэширует список системных пулов в файле 
   <span class="term"><code>zpool.cache</code></span>. Она может применять устройства кэширования для чтения и 
   записи. Однако, находящимся на самом виду кэшем является кэш адаптивной замены (ARC).</p>
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="01"> </a>Кэш адаптивной замены</h3>
   </div></div></div>
   <p>Вызов данных из оперативной памяти намного быстрее доступа к файлу с диска. Unix- подобные 
   операционные системы обычно хранят копии файлов с наиболее частыми обращениями в буферном кэше в ОЗУ. 
   ZFS использует наиболее искушённый и наиболее эффективный тип кэширования, кэш адаптивной замены, или 
   <span class="emphasis"><em>ARC</em></span> (Adaptive Replacement Cache). Понимание ARC начинается с 
   осознание кэширующего буфера.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0101"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Традиционный буферирующий кэш </span></h4>
   </div></div></div>
   <p>Буферирующий кэш выбирает для кэширования данные на основе алгоритма <a class="link" 
   href="https://ru.wikipedia.org/wiki/Алгоритмы_кэширования#Least_recently_used_.28.D0.92.D1.8B.D1.82.D0.B5.D1.81.D0.BD.D0.B5.D0.BD.D0.B8.D0.B5_.D0.B4.D0.B0.D0.B2.D0.BD.D0.BE_.D0.BD.D0.B5.D0.B8.D1.81.D0.BF.D0.BE.D0.BB.D1.8C.D0.B7.D1.83.D0.B5.D0.BC.D1.8B.D1.85.29" 
   target="_top">вытеснения давно неиспользуемых данных</a>, или <span class="emphasis"><em>LRU</em></span> 
   (Least Recently Used). LRU является списком, хранящимся по последнему времени доступа к порции данных.
   При  каждом применении объекта он перемещается в вершину списка. По мере заполнения списка система 
   отбрасывает элементы из нижней части этого списка до тех пор, пока не получит достаточное пространство 
   для вставки новых элементов в вершину списка.</p>
   <p>Буферирующий кэш работает достаточно хорошо для обеспечения выгод производительности, однако, при 
   определённых условиях метод LRU вызывает нежелательное поведение.</p>
   <p>Рассмотрим ночное резервное копирование. Программа резервного копирования сканирует весь жёсткий диск
   для поиска файлов, модифицированных с момента последнего резервного копирования. Выполнение такого 
   сканирования добавляет такой файл в системе в вершину нашего списка, позволяя только что отсканированным 
   файлам опускаться вниз списка. По окончанию резервного копирования буферирующий кэш наполнен данными, 
   которые никого не беспокоят. Тем временем, критически важные базы данных были полностью удалены на 
   диск. Это называется замусориванием кэша (<span class="emphasis"><em>cache thrashing</em></span>).</p>
   <p>ARC позволяет избегать подобных проблем.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0102"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Проектирование ARC </span></h4>
   </div></div></div>
   <p>ARC также кэширует файлы, которые только что были прочитаны с диска. Вместо одиночного списка ARC 
   имеет два парных списка. Один является <a class="link" 
   href="https://ru.wikipedia.org/wiki/Алгоритмы_кэширования#Most_Recently_Used_.28.D0.9D.D0.B0.D0.B8.D0.B1.D0.BE.D0.BB.D0.B5.D0.B5_.D0.BD.D0.B5.D0.B4.D0.B0.D0.B2.D0.BD.D0.BE_.D0.B8.D1.81.D0.BF.D0.BE.D0.BB.D1.8C.D0.B7.D0.BE.D0.B2.D0.B0.D0.B2.D1.88.D0.B8.D0.B9.D1.81.D1.8F.29" 
   target="_top">самым последним использованным</a>, или <span class="emphasis"><em>MRU</em></span> 
   (Most Recently Used), списком, отслеживающим блоки файловой системы во многом аналогично буферирующему кэшу, 
   а второй является списком наиболее часто используемых блоков MFU (Most Frequently Used), отслеживающим 
   регулярно используемые блоки файловой системы.</p>
   <p>Добавление списка MFU уменьшает воздействие процессов замусоривания кэша, подобных гипотетическим 
   заданиям резервного копирования. Хотя сканирование всех файлов очищает список MRU, оно не воздействует 
   на список MFU. Разовое сканирование блоков для целей резервного копирования на является &quot;частым&quot;.
   Наиболее часто применяемые файлы остаются кэшированными в оперативной памяти. Выполнение вашего 
   резервного копирования всё же влияет на дисковый ввод/ вывод, уменьшая производительность записи, 
   однако система обслуживает наиболее популярные файлы из кэшированной в оперативной памяти копии.</p>
   <p>Каждый список работает в паре с теневым (<span class="emphasis"><em>ghost</em></span>) списком, 
   который содержит информацию о блоках, которые были выселены из этого списка. Когда списки MRU или MFU 
   заполняются, блоки из нижней части этих списков отбрасываются. Отслеживая такие удаляемые из 
   кэшей блоки, ARC предотвращает постоянное циклическое занесение в кэш и удаление из него. В случае, 
   когда блок используется достаточно часто, ARC также может принимать решение для обеспечения ему места 
   в списке MFU.</p>
   <p>Почти во всех случаях ARC является само-регулируемым, а ручное вмешательство системных администраторов 
   может только ухудшить производительность. Может оказаться, что ваше конкретное приложение потребует 
   специальной обработки. Однако, прежде чем начать возиться с ним, осознайте как ARC ведёт себя.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0103"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Использование памяти ARC </span></h4>
   </div></div></div>
   <p>ARC спроектирован и для жадных, и для щедрых. Если система имеет свободную оперативную память, 
   а ARC предполагает, что, возможно, может извлечь из этого преимущества, ARC захватывает память. 
   При всяком чтении с диска ARC кэширует файл в оперативной памяти. ARC продолжает кэшировать файлы до тех 
   пор, пока система не использует всю свою оперативную память.</p>
   <p>FreBSD резервирует 1ГБ оперативной памяти для ядра и прикладных программ. Вся оставшаяся оперативная 
   память является предметом для справедливой игры ARC. В продолжительно работающих системах с большим 
   объёмом систем хранения и не очень большим объёмом оперативной памяти не будет сюрпризом увидеть, что 
   ваша ARC потребляет основную часть системной памяти.</p>
   <p>Однако, ARC имеет очень низкий приоритет для запросов к памяи. Если приложение запрашивает оперативную
   память, а система не имеет достаточно свободной памяти, ядро усекает ARC, предоставляя приложению 
   запрошенную им память. Процесс возвращения оперативной памяти ARC в вашу систему для свободного 
   применения не является мгновенным; он может занять несколько секунд.</p>
   <p>Таким образом: если память свободна, ARC будет её использовать. Если что-то испытывает потребность в 
   этой памяти, ARC возвращает её назад. Современные сервера имеют в наличии много оперативной памяти. Они могли 
   бы применять её для чего- то полезного. Старая поговорка &quot;Свободная память- потерянная память&quot;
   всё ещё является верной.</p>
   <p>Наиболее простой способ проверки размера ARC состоит в применении <span class="term"><code>top(1)</code></span>.
   Приведём часть вывода <span class="term"><code>top</code></span> для сервера с 32ГБ ОЗУ и 20ТБ дискового 
   пространства.</p>
	   <pre class="screen"><code>
…
Mem: 168M Active, 116M Inact, 24G Wired, 1168K Cache, 449M Buf, 7052M Free
ARC: 23G Total, 15G MFU, 7398M MRU, 18K Anon, 412M Header, 88M Other
…
 	   </code></pre>   <p></p>
   <p>Строка <span class="emphasis"><em>Mem</em></span>, появляется в верхней части вывода почти всех 
   Unix- подобных систем и она предлагает подробности того, как много оперативной памяти системы используется 
   для различных типов задач. Хотя ARC является поднабором закреплённой памяти, ARC выводится отдельно, 
   поэтому она может предложить дополнительные подробности.</p>
   <p>Первый элемент, <span class="emphasis"><em>Total</em></span>, отображает общую используемую ARC 
   память.</p>
   <p>Файлы, имеющие частый доступ к ним, отображены в пространстве <span class="emphasis"><em>MFU</em></span>.
   У нас отображено 15ГБ данных MFU в ARC.</p>
   <p>Записи <span class="emphasis"><em>MRU</em></span> показывают 7 398МБ, применяемых для хранения файлов, 
   к кторым был осуществлён самый последний доступ.</p>
   <p>Данные, перемещаемые из одной очереди в другую, а также операции асинхронной записи ожидающие сброса 
   на диск отображаются в пространстве <span class="emphasis"><em>Anon</em></span>. Приводимая в списке 
   <span class="emphasis"><em>Header</em></span> память используется для метаданных самой ARC. Данные 
   23ГБ ARC требуют 412МБ метаданных. <span class="emphasis"><em>Other</em></span> включает вещи наподобие 
   метаданных исключительно для времени исполнения, применяемых для помощи поиска ARC содержимого в своём 
   кэше. Строго говоря, эта память не является частью кэша самого по себе, однако она поддерживает
   инфраструктуру.</p>
   <p>Хотя ARC является жадной по отношению к оперативной памяти, заметим, что продолжительно работающие 
   системы всё же имеют несколько гигабайт свободной памяти. Диски являются чрезвычайно заполненными, 
   однако необходимый реальным пользователям объём данных относительно невелик. Всякий, кто управляет 
   файловыми серверами узнаёт такую модель - каждое подразделение деловых учётных записей имеет одну 
   главную таблицу плюс 15 миллиардов слегка отличающихся копий этой таблицы для различных дат, причём 
   все они жизненно необходимы и должны быть сохранены для потомков навечно. Если ARC использует большую 
   часть вашей системной памяти, то это происходит по той причине, что процессы осуществляют доступ к 
   файлам. ARC не охотится за оправданиями для высасывания ОЗУ.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0104"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Zfs-stats </span></h4>
   </div></div></div>
   <p>ZFS раскрывает производительность, настройки и системы показателей ZFS множеством 
   <span class="term"><code>sysctls</code></span> в <span class="term"><code>vfs.zfs</code></span> и 
   <span class="term"><code>kstat.zfs</code></span>. Сами по себе эти значения обычно означают очень мало чего,
   однако они становятся понятными при их сравнении друг с другом. Вместо того, чтобы разбирать эти 
   значения напрямую мы настоятельно рекомендуем для исследования ARC применять пакет 
   <span class="term"><code>zfs-stats </code></span>.</p>
   <p>Для получения основной информации о вашем ARC, например, текущим размером и длиной каждой очереди
   внутри ARC, воспользуйтесь <span class="term"><code>zfs-stats -A</code></span>. Приведём интересный 
   фрагмент отчёта <span class="term"><code>zfs-stats</code></span> из системы Лукаса.</p>
	   <pre class="screen"><code>
# zfs-stats -A
…
ARC Summary: (HEALTHY)
    Memory Throttle Count:      0
…
 	   </code></pre>
   <p>Memory Throttle Count (количество дросселирований памяти) сообщает сколько раз ваш ARC сокращался 
   возвращая память своему ядру для её применения другим процессом. Если значение Memory Throttle Count
   высокое, вы можете рассмотреть уменьшение предела для размера ARC для гарантирования достаточного 
   объёма свободной памяти для ваших других процессов. Сжатие памяти ARC не означает что система должна 
   иметь больше памяти, тем не менее, дополнительная память будет иметь именно такой эффект.</p>
	   <pre class="screen"><code>
…
ARC Size:                       36.22% 10.89 GiB
  Target Size: (Adaptive)      100.00% 30.07 GiB
  Min Size (Hard Limit):        12.50%  3.76 GiB
  Max Size (High Water):           8:1 30.07 GiB
…
 	   </code></pre>
   <p>Данный конкретный ARC составляет 36.22% своего максимального размера, или 10.89ГБ. Он настроен на 
   максимальный размер 30.07ГБ. Минимум составляет 3.76ГБ.</p>
	   <pre class="screen"><code>
…
ARC Size Breakdown:
  Recently Used Cache Size:    50.00%  15.03   GiB
  Frequently Used Cache Size:  50.00%  15.03   GiB
…
 	   </code></pre>
   <p>ARC поровну делит выделение памяти для кэшей MRU и MFU.</p>
   <p>По сравнению с общим отчётом, отчёт об эффективности ARC, получаемый посредством  <span class="term"><code>zfs-stats
   -E</code></span> представляет больший интерес. Приведём наиболее интересные фрагменты вывода другого сервера.</p>
	   <pre class="screen"><code>
# zfs-stats -E
…
ARC Efficiency:               78.40m
  Cache Hit Ratio:    97.76%  76.65m
  Cache Miss Ratio:    2.24%   1.75m
  Actual Hit Ratio:   97.76%  76.65m
 	   </code></pre>
   <p>ZFS выделяет большие объёмы памяти для кэширования файловой системы. Верхние строки данного отчёта 
   показывают как много преимуществ мы из этого получаем. Cache Hit Ratio (число попаданий в кэш) показывает,
   какое число дисковых запросов обслуживалось из ARC вместо того, чтобы обращаться к диску. В данном 
   случае, 97.76% всех запросов чтения для данной машины было обслужено из оперативной памяти. Следующее 
   после процентов значение является простым количеством запросов. Данный хост обслужил 76.65 миллионов 
   дисковых запросов из ARC.</p>
   <p>Далее, <span class="term"><code>zfs-stats</code></span> из какого именно кэша получались кэшированные 
   файлы.</p>
	   <pre class="screen"><code>
CACHE HITS BY CACHE LIST:
  Most Recently Used:            3.35%   2.57m
  Most Frequently Used:         96.65%  74.08m
  Most Recently Used Ghost:      0.04%  28.81k
  Most Frequently Used Ghost:    0.08%  63.26k
 	   </code></pre>
   <p>Кэш MRU, который имеет сходство с традиционным кэширующим буфером, обслуживает 3.35% всех полученных из 
   ARC файлов. 96.65% всех файлов поступают из кэша MFU. Несомненно, существует некое перекрытие между этими
   очередями - в отсутствие кэша MFU некоторые файлы с частым доступом должны появляться в кэше MRU. Однако 
   это прекрасная иллюстрация того, почему ARC применяет кэш MFU.</p>
   <p>Теневые кэши (ghost) содержат списки данных, которые были недавно кэшированы, однако были отброшены 
   из-за давления памяти или прочих ограничений. Стоит ли добавить больше памяти и увеличить размер ARC 
   чтобы улучшить соотношение попаданий в кэш? При 0.04% и 0.08% попаданиях в теневой список добавление 
   дополнительной памяти не должно серьёзно улучшить кэширование. ARC данного хоста заполнен только на 
   36%, следовательно элементы не были отселены. Эти незначительные проценты могут быть десятками тысяч 
   запросов, однако в сравнении с миллионами запросов, обслуживаемыми кэшем, это почти ничто. 
   Дополнительная память может усилить прочие процессы, но не ARC.</p>
   <p>Далее мы рассмотрим типы данных, получаемых из нашего ARC.</p>
   <p></p>
	   <pre class="screen"><code>
CACHE HITS BY DATA TYPE:
  Demand Data:      97.15% 74.46m
  Prefetch Data:     0.00%      0
  Demand Metadata:   2.85%  2.19m
  Prefetch Metadata: 0.00%      0
 	   </code></pre>
   <p>Собственно данные являются содержимым файлов, в то время как метаданные это всё об этих файлах. 
   Мы обсуждаем упреждающую выборку в <a class="link" href="Ch08.html#04" target="_top">Главе 8</a>, 
   однако какой бы ни была предварительная выборка, ясно, что здесь она не вступает в игру.</p>
   <p>С обратной стороны монеты мы увидим какого сорта данные не кэшируются.</p>
	   <pre class="screen"><code>
CACHE MISSES BY DATA TYPE:
  Demand Data:       31.72%  556.25k
  Prefetch Data:      0.00%        0
  Demand Metadata:   68.28%    1.20m
  Prefetch Metadata:  0.00%       19
 	   </code></pre>
   <p>Сколько запросов системы должно обслуживаться ARC? Это полностью зависит от рабочей нагрузки. 
   Веб- сервер, который обслуживает одни и те же данные снова и снова ожидает высокого соотношения попадания 
   в кэш. Ожидайте более низкого уровня соотношения попадания в кэш на серверах, на которых клиенты 
   осуществляют доступ к большому числу различных файлов. Если ваш пул имеет терабайты данных в миллионах 
   файлов, однако клиенты никогда не осуществляют доступ к одним и тем же файлам данных дважды, 
   кэширование файлов в памяти не поднимет производительность.</p>
  </div>
   
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="02"> </a>Модификации ARC</h3>
   </div></div></div>
   <p>ARC ZFS управляет сам собой в несметном большинстве случаев. В большинстве вариантов, при которых 
   ARC не может сам себя настроить, проблемы наилучшим образом решаются путём добавления аппаратных 
   средств. Однако, порой регулировка памяти или производительности ZFS может потребовать от вас времени 
   пока вы не получите и не установите оборудование. Зачастую регулировка вашего ARC является соответсвующим 
   ответом для определённого приложения.</p>
   <p>Вы можете тонко настраивать свой ARC установкой верхних и нижних границ используемой им памяти, а также 
   управляя тем что, где и зачем ARC должен кэшировать.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0201"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Ограничение размера ARC </span></h4>
   </div></div></div>
   <p>Установка FreeBSD убирает в сторону 1ГБ оперативной памяти для ядра и работающих программ, делая 
   возможным для ZFS поглощать всё остальное в ARC если того запросит производительность системы. Вы 
   можете изменить это резервируя минимальный объём памяти для ARC и/или устанавливая жёсткий предел тому, 
   как много памяти может получить ARC.</p>
   <p>Все установки ARC задаются в байтах. Н текущий день мы управляем памятью в гигабайтах. Для установки 
   значения для ZFS умножьте необходимое для ZFS значение гигабайт на 1024<sup>3</sup>.</p>
   <p>ARC с готовностью уступает память по запросам, однако освобождение памяти не происходит моментально.
   Освобождение памяти из-под ARC и её выделение другому процессу бесспорно отнимет больше времени, чем 
   выделение свободной памяти тому же самому процессу. В хосте с очень большим ARC, сброс гигабайтов объектов 
   из кэша может занять значительную часть секунды.</p>
   <p>Вы можете решить ограничить объем оперативной памяти который может использовать ARC, высвобождая память 
   для приложений. Чтобы установить это, воспользуйтесь настройкой времени загрузки <span 
   class="term"><code>vfs.zfs.arc_max</code></span>. По умолчанию FreeBSD устанавливает это значение к общему 
   объёму оперативной памяти минус 1ГБ. А мы здесь установим верхний предел в 20ГБ в 
   <span class="term"><code><em>boot/loader.conf</em></code></span>.</p>
	   <pre class="screen"><code>
vfs.zfs.arc_max=&quot;21474836480&quot;
 	   </code></pre>
   <p>Максимальный размер ARC не является жёстким пределом, но всё- таки нечто большим чем маркер наивысшего 
   уровня воды. Когда ARC достигает этого размера, ZFS начинает поспешно уменьшает размер своего кэша. 
   Самые последние важные элементы добавляются к теневой (ghost) очереди и сбрасываются. Если вы будете 
   наблюдать за размером ARC, вы сможете наблюдать использование памяти, колеблющееся вокруг <span 
   class="term"><code>vfs.zfs.arc_max</code></span> при воздействии сжатия памяти на систему.</p>
   <p>Также возможно что приложение может воспользоваться великодушием ARC против него и сжимать её наличие.
   Минимальный размер ARC по умолчанию составляет одну восьмую от её максимального размера. (Строго говоря, 
   минимальный размер ARC составляет половину максимального объёма используемой в ARC метаданными памяти, 
   который составляет четверть от максимального объёма ARC.) Для установки минимального размера ARC 
   воспользуйтесь настройкой времени загрузки <span class="term"><code>vfs.zfs.arc_min</code></span>. Как 
   и максимальный размер, минимальный размер выражается в байтах. В нашем примере я устанавливаю минимум 
   размера ARC на размер 4ГБ в <span class="term"><code><em>boot/loader.conf</em></code></span>.</p>
	   <pre class="screen"><code>
vfs.zfs.arc_min=&quot;4294967296&quot;
 	   </code></pre>
   <p>Лукас обычно устанавливает верхние и нижние пределы размера ARC только когда он собирается объяснять 
   как работает ARC управленцам без технического образования. &quot;Да, PostgreSQL может применять много 
   памяти. ARC использует оперативную память. Однако ARC только кэширует наполнение, которое вызывается 
   PostgreSQL и PostgreSQL должен быть запущен и сделать это, следовательно нет проблем&quot; (если вы вновь 
   вернётесь к этому разговору, постарайтесь не добавлять: &quot;Чёрт, как вы сможете получить свой 
   воздух сегодня?&quot; это никогда не завершается хорошо.)</p>
   <p>FreeBSD 10.2 и более поздние версии могут определять какой объём памяти ARC следует попытаться оставить 
   свободным для его использования прочими процессами, применив <span class="term"><code>sysctl 
   vfs.zfs.arc_free_target</code></span>. Это значение отличается от прочих в данном разделе, поскольку оно
   определяется в страницах, а не в байтах. Страница составляет 4096 байт оперативной памяти, поэтому значение 
   в 2ГБ будет выражаться как 524288 (2 * 1024<sup>3</sup> / 4096). Когда значение свободной памяти падает ниже 
   этого значения, начинает выполняться чистильщик (reper) ядра. Чистильщик выполняет две функции: он 
   выравнивает размер ARC чтобы гарантировать что существует достаточно свободной памяти и, кроме того, он 
   выполняет дефрагментацию KMEM Arena. В то время когда ZFS быстро выделяет и освобождает небольшие участки 
   памяти по мере перемещения файлов в- и из- ARC, он замусоривает ими различные участки памяти ядра. Эта память 
   не возвращается в <span class="emphasis"><em>свободное применение</em></span> поке не освободятся все 
   выделения в arena. Это выявляет само по себе, что объём закреплённой (<span 
   class="emphasis"><em>wired</em></span>) памяти является более значимым, чем весь размер ARC плюс ожидаемые 
   прочие закреплённые страницы, такие как ваш стек сетевой среды. В отличие от предыдущих параметров настройки, 
   <span class="term"><code>vfs.zfs.arc_free_target</code></span> может выравниваться в работающей системе и 
   иметь немедленное воздействие.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0202"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Метаданные и ARC </span></h4>
   </div></div></div>
   <p>Метаданные файловой системы содержат все материалы о файлах за исключением самих файлов: каталоги, 
   полномочия, владельцев, размер, свойства и тому подобное. ARC кэширует всю эту информацию в точности 
   как он кэширует содержимое файла. В конце концов, доступ к содержимому файла требует доступа к метаданным 
   этого файла.</p>
   <p>По умолчанию ARC применяет четверть максимального размера своего кэша для кэширования таких метаданных.
   Хотя этого почти всегда достаточно, если система имеет целую кучу мелких файлов, вам может понадобиться 
   расширить данный предел. Настройка времени загрузки <span class="term"><code>vfs.zfs.arc_meta_limit</code></span>
   позволит вам установить особенный предел для метаданных, который может быть как больше, так и меньше 
   заданного по умолчанию. Вот жёсткий код кэширования метаданных вашего ARC 8ГБ в 
   <span class="term"><code><em>boot/loader.conf</em></code></span></p>
   	   <pre class="screen"><code>
vfs.zfs.arc_meta_limit=&quot;8589934592&quot;
 	   </code></pre>
   <p>Если <span class="term"><code>zfs-stats -E</code></span> показывает, что вы вытаскиваете из ARC намного 
   больше данных чем метаданных, вы можете рассмотреть возможность увеличения предела метаданных и понаблюдать 
   не улучшит ли это производительность. Помните, что по умолчанию минимальный размер ARC составляет половину 
   от объёма, который может использоваться метаданными. К тому же, кэшированные метаданные не могут использовать
   пространства больше, чем весь объём ARC.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0203"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Наборы данных и ARC </span></h4>
   </div></div></div>
   <p>Некоторые данные имеют регулярные шаблоны доступа, что делает ARC неподходящим - ко времени, когда 
   к файлу будет осуществлён повторный доступ, он уже израсходует все пределы по длительности в списках и в MRU, 
   и в MFU. Это обычно происходит только когда набор данных содержит миллионы файлов и когда вы легко можете 
   предсказывать их применение. Сообщение вашей ARC не кэшировать эти файлы освободит память для кэширования 
   файлов, которым это может быть полезно.</p>  
   <p>Свойство ZFS <span class="term"><strong class="userinput"><code>primarycache</code></strong></span>
   определяет какая часть информации набора данных должна поступать в ARC. Значение по умолчанию, 
   <span class="term"><code><em>all</em></code></span>, означает кэшировать данные файла точно также, как и 
   метаданные.</p>  
   <p>Установка <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> в
   <span class="term"><code><em>metadata</em></code></span> сообщает ARC кэшировать только метаданные 
   каждого файла, а не само содержание. Вы можете найти это удобным для каталогов, которые содержат большое 
   число файлов. Без кэширования метаданных выполнение <span class="term"><code>ls(1)</code></span> в 
   большом каталоге может потребовать нескольких минут, поскольку ZFS читает диски и собирает информацию. 
   Кэширование только метаданных запрещает упреждающее чтение, что может приносить производительности 
   больше вреда, чем помощи от кэширования метаданных.</p>  
   <p>Установка <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> в 
   значение <span class="term"><code><em>none</em></code></span> сообщает ARC не кэшировать ничего в этом 
   наборе данных. Вы не можете выполнять предварительное чтение без кэширования, однако если набор данных 
   не кэшируется, производительность явно не является проблемой.</p>  
   <p>Например, сервер Лукаса имеет набор данных с многими терабайтами бесчисленного количества файлов, 
   созданных за последние 15 лет. При крайне редком случае доступа к этим файлам, они просматриваются по 
   порядку. Сервер даже близко не имеет достаточно памяти для эффективного кэширования содержимого всех 
   этих файлов. Сообщение для ARC кэшировать метаданных на этом наборе данных означает что 
   <span class="term"><code>ls(1)</code></span> и тому подобное всё-таки будут работать шустро, но нам 
   не нужна бесполезная суматоха ARC этой машины.</p>
   <p>Чтобы свойство <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> 
   возымело эффект, вы должны размонтировать и смонтировать набор данных для этого.</p>  
	   <pre class="screen"><code>
# zfs set primarycache=metadata cdr/cdr
# zfs unmount cdr/cdr
# zfs mount cdr/cdr
 	   </code></pre>
   <p>ARC теперь свободен для полезной работы, например, для кэширования временных файлов, создаваемых 
   при анализе этих данных.</p>
   <p>Процедура размонтирования удаляет всю кэшированную информацию об этом наборе данных и из ARC, и из 
   L2ARC.</p>
  </div>
   
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="03"> </a>ARC 2 уровня</h3>
   </div></div></div>
   <p>ARC постоянно подрезает себя для сохранения в пределах разрешённого ему размера. Файлы, к которым 
   продолжительное время нет обращений, выпадают из списка MRU. Обычно выпадающие из ARC элементы улетучиваются - 
   хотя они упоминаются в теневых (ghost) списках, поэтому MFU может распознать их если они вновь 
   появляются, система полагается на дисковую копию этого файла.</p>
   <p>ARC 2 уровня, или <span class="emphasis"><em>L2ARC</em></span>, является вторым кэшем чтения. 
   L2ARC перехватывает выпадающие из вашего ARC элементы. При применении небольшого, быстрого диска с 
   большим резервом перезаписи для кэширования данных ARC, вы можете одновременно уменьшить нагрузку 
   чтения в вашем основном хранилище системы и улучшить производительность чтения. Команда 
   <span class="term"><code>zpool(8)</code></span> вызывает L2ARC, <span class="emphasis"><em>устройство 
   кэша</em></span>.</p>
   <p>Хотя L2ARC и пишет на диск, данные на нём не выдерживают перезагрузку. Хотя всё здесь и сохранено, 
   индексы этих данных разрушены. Даже если эти индексы были доступны, пул всё же мог быть изменён на 
   другой машине до перезагрузки системы. ZFS не может доверять информации на устройстве кэширования.</p>
   <p>По состоянию на начало 2016г существует почти завершённая реализация устойчивого L2ARC. При наличии 
   устойчивого L2ARC система будет перезагружать предыдущий L2ARC и загружать всё назад в оперативную 
   память. Хотя данная функциональность может никогда не получить широкого распространения, те пользователи, 
   которые интенсивно используют L2ARC, смогут проверить её в недалёком будущем.</p>
   <p>Применение L2ARC стоновится существенным при наличии большого числа пользователей, виртуальных машин 
   и приложений, имеющих доступ к одному и тому же набору данных. Если ваш рабочий набор больше чем объём 
   памяти, который вы можете себе позволить, вашим вторым шансом будет L2ARC на базе устройства SSD или 
   NVMe. Для большинства приложений, например, типичный NAS для домашнего применения или для предприятия, 
   L2ARC не увеличит производительность. L2ARC может даже приносить вред производительности, потребляя 
   оперативную память.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0301"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Использование памяти L2ARC </span></h4>
   </div></div></div>
   <p>Раз L2ARC содержит целую группу кэшированных данных и метаданных, индекс этих данных располагается 
   в пределах ARC. В качестве практического метода, каждый гигабайт L2ARC требует 25МБ ARC. (Значение 
   зависит от размера сектора диска, свойства <span class="term"><strong class="userinput"><code>recordsize</code></strong></span> 
   и других характеристик набора данных, что общепризнанно делает сложным вычисление действительного размера.)
   Достаточно здраво предполагать полностью используемого L2ARC проглотит 25ГБ ARC.</p>
   <p>Большинство L2ARC не находятся рядом с тарабайтом - пока, по крайней мере. SSD с достаточным циклом 
   перезаписей чтобы сделать его удобным для кэша чтения всё ещё достаточно дорогостоящи чтобы большинство из 
   нас имело их про запас завалявшимися. Те, кто планирует объёмные массивы хранения, скажем, из 40 дисков, 
   должны помнить о них. {<span class="emphasis"><em>Прим. пер.: На текущий момент достаточно дискусионно: 
   в-- первых многие производители серверов и СХД уже предлагают eMLC по цене 15k/10k шпиндельных дисков 
   того же объёма, во-вторых, никто не мешает вам менять бытовые SSD в качестве L2ARC как перчатки - 
   SMART даёт достаточно хорошие прогнозы.</em></span>}</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0302"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Кэширование L2ARC </span></h4>
   </div></div></div>
   <p>L2ARC может кэшировать только выпадающие из ARC данные. Данные, которые никогда не попадали в ARC не 
   могут появиться и в L2ARC.</p>
   <p>Предположим, вы запретили кэширование ARC для всех наборов данных в определённом пуле установив 
   значение <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> в 
   <span class="emphasis"><em>none</em></span>. Добавление L2ARC к этому пулу не улучшит производительность 
   ZFS. Не существует данных, вываливающихся в L2ARC.</p>
   <p>Вы можете решить, что его будет существенно иметь в наборе данных, для которого ARC содержит 
   метаданные, в то время как L2ARC кэширует реальные данные файла. Набор данных <span class="term"><code><em>cdr/cdr</em></code></span>
   Лукаса с многими терабайтами из предыдущего раздела может показаться великолепным кандидатом для этого.
   И, раз уж он установит <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> в 
   <span class="emphasis"><em>metadata</em></span>, он непременно увидит, что произойдёт нечто подобное</p>
   	   <pre class="screen"><code>
# zfs get primarycache,secondarycache zroot/cdr
NAME       PROPERTY        VALUE     SOURCE
zroot/cdr  primarycache    metadata  local
zroot/cdr  secondarycache  all       default
 	   </code></pre>
   <p>Проблема состоит в том, что ARC кэширует только метаданные, поэтому единственный материал, который 
   может выталкиваться в L2ARC, это метаданные. L2ARC может содержать только то, что есть в ARC, или 
   подмножество этого.</p>
   <p>По умолчанию ARC кэширует все системные доступы, поэтому L2ARC делает то же самое.</p>
   <p>Вы можете управлять тем, как каждый набор данных использует L2ARC при помощи свойства 
   <span class="term"><strong class="userinput"><code>secondarycache</code></strong></span>. как и в случае с
   <span class="term"><strong class="userinput"><code>secondarycache</code></strong></span>, 
   <span class="term"><strong class="userinput"><code>primarycache</code></strong></span> может быть установлен
   в значения <span class="emphasis"><em>all</em></span>, <span class="emphasis"><em>metadata</em></span>
   или <span class="emphasis"><em>none</em></span>. Значением по умолчанию является 
   <span class="emphasis"><em>all</em></span>, что означает, что данные коорые достаточно значимы для 
   первичного ARC будут помещаться в L2ARC.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0303"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Потоковая обработка файлов </span></h4>
   </div></div></div>
   <p>Большая часть времени для чтения файла расходуется на перемещение головок диска в их позицию над 
   пластиной. Будучи позиционированными, головки читают данные просто великолепно. Это называется 
   <span class="emphasis"><em>потоковой передачей</em></span> (streaming). В большинстве систем  
   множество дисков основного пула быстрее чем один или два диска L2ARC, поэтому при обработке 
   больших файлов, выходящих за размеры оперативной памяти, было бы более быстрым читать их с диска. 
   В этом случае наличие L2ARC кэша потоковой обработки файлов не имеет смысла, поэтому по умолчанию 
   отключается.</p>
   <p>Если у вас имеется L2ARC, который быстрее основного пула, вы можете захотеть разрешить кэширование 
   больших файлов. Настройка времени загрузки <span class="term"><code>vfs.zfs.l2arc_noprefetch</code></span>
   управляет кэшированием потоковой обработки файлов. Значение по умолчанию, <span class="emphasis"><em>1</em></span>
   запрещает кэшировать потоковую обработку файлов. Установка его значение в 
   <span class="emphasis"><em>0</em></span> делает возможным кэширование, как в примере с 
   <span class="term"><code><em>/boot/loader.conf</em></code></span>.</p>
   	   <pre class="screen"><code>
vfs.zfs.l2arc_noprefetch=0
 	   </code></pre>
   <p>Эта подстройка имеет эффект только при импорте пула. FreeBSD импортирует пулы перед просмотром 
   <span class="term"><code><em>/boot/loader.conf</em></code></span>, поэтому это значение должно быть 
   установлено при начальной загрузке.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0304"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Скорость записи L2ARC </span></h4>
   </div></div></div>
   <p>SSD не так надёжны, как шпиндельные диски. Даже если вы контактируете с подготовленным поставщиком
   (наподобие iX Systems {<span class="emphasis"><em>Прим. пер.: или <a class="link" href="http://www.mdl.ru" 
   target="_top">mdl.ru</a></em></span>}), который очень искушён в ZFS и точно знает какие диски лучше 
   применять для L2ARC, вы можете набивать SSD только ограниченное число раз перед тем как он помрёт.
   Хотя ZFS великолепно управляет умирающим или уже погибшим L2ARC, постоянная утрата диска дорогостояща, 
   временеёмка и раздражающа для системного администратора. ZFS реализует множество подстроек скорости 
   записи в L2ARC для продления срока службы диска.</p>
   <p>Как и большинство ARC, L2ARC при своей настройке применяет байты. Большинство настроек L2ARC имеет 
   значение в мегабайтах. Умножайте нужные вам значения на 1024<sup>2</sup>.</p>
   <p>В процессе нормальной работы ZFS пишет только 8МБ в секунду в каждое устройство L2ARC. Это позволяет
   избегать высасывания устройства SSD, а также помогает избегать <a class="link" 
   href="https://ru.wikipedia.org/wiki/Пробуксовка_(информатика)" target="_top">пробуксовки кэша</a> 
   (<span class="emphasis"><em>Cache thrashing</em></span>- запись в кэш большого количество данных, при 
   которой кэш просто заканчивается и перезаписывается новыми данными ещё до их применения). Если вам нужен 
   L2ARC системы для обработки больших объёмов данных, вы можете поднять его значение при помощи
   <span class="term"><code>vfs.zfs.l2arc_write_max</code></span>. Не поднимайте его настолько высоко, что 
   сделаете чтение медленным.</p>
   <p>При первой загрузке системы L2ARC пуст. Пустой L2ARC это не очень хорошо. ZFS выполняет после загрузки 
   системы фазу турбо прогрева (Turbo Warmup Phase), при которой она записывает дополнительные данные в 
   L2ARC вдобавок к пределу, определяемому <span class="term"><code>vfs.zfs.l2arc_write_max</code></span>.
   Фаза турбо прогрева продолжается пока ARC не сбросит первый элемент в L2ARC. Продолжительность времени 
   для этого целиком зависит от системы. По умолчанию, ZFS может записывать дополнительные 8МБ на 
   каждое устройство L2ARC в процессе фазы турбо прогрева. Выделением дополнительной полосы пропускания
   управляет <span class="term"><code>sysctl vfs.zfs.l2arc_write_boost</code></span>.</p>
   <p>Вы можете изменить этот <span class="term"><code>sysctl</code></span> в любой момент. Вот пример 
   установки обоих на 16МБ.</p>
	   <pre class="screen"><code>
# sysctl vfs.zfs.l2arc_write_max=16777216
vfs.zfs.l2arc_write_max: 8388608 -&gt; 16777216
# sysctl vfs.zfs.l2arc_write_boost=16777216
vfs.zfs.l2arc_write_boost: 8388608 -&gt; 16777216
 	   </code></pre>
   <p>Безусловно, программные установки не могут превышать ограничения оборудования.</p>
   <p>SSD уже не настолько хрупки, как это когда то было. Современные SSD уровня центра обработки данных, 
   например, 200ГБ Intel DC S3700 имеет скорость износа (Endurance Rating) &quot;10 записей диска в день на 
   протяжении 5 лет&quot;. Это транслируется в примерно 2000ГБ в день, или 23МБ/с. Вы можете постоянно 
   писать 23МБ/с на этот диск и, согласно Intel, израсходуете его в течение 5 лет. Для высокопроизводительных 
   серверов, регулировка этого дросселирования <span class="term"><code>sysctl</code></span> в сторону 
   увеличения и добавление заметки заказать новое кэширующее устройство через 58 месяце будет иметь смысл. 
   Поскольку ваш хост вероятно не выполняет запись в L2ARC на полном газу всё время, настройка этого 
   параметра может иметь даже больше смысла. {<span class="emphasis"><em>Прим. пер.: При разнице в стоимости
   eMLC и бытовых SSD на порядок, может быть не лишённой смысла и стратегия храниеия &quot;про 
   запас&quot; бытового SSD диска на случай износа его предшественника с последующей дозакупкой всё 
   уменьшающихся в стоимости новых замен.</em></span>}</p>
  </div>
   
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="04"> </a>Целевой журнал ZFS</h3>
   </div></div></div>
   <p>Кэш предназначен не только для чтения данных. ZFS применяет кэширование также и для записей, применяя 
   целевой журнал ZFS (<span class="emphasis"><em>ZIL</em></span>, ZFS Intent Log). ZFS сбрасывает записи в 
   ZIL, а затем обрабатывает эти записи, добавляя их соответствующим образом в ZFS. Каждый пул имеет свой 
   собственный ZIL. При обычном применении ZFS использует порции пространства каждого поставщика для 
   соответствующего ZIL. По вашему желанию вы можете добавить дополнительное устройство для его применения 
   в качестве ZIL. Строго говоря, ваш ZIL в точности не является кэшем записи. Однако, он является видом 
   кэширования и поэтому мы обсудим его в данной главе.</p>
   <p>Однако, ZIL не рабботает таким образом, как предполагает большинство люей. Чтобы понимать когда 
   пулу требуется отделять журналирующее устройство, а когда нет, вы должны понимать как ZFS осуществляет 
   запись данных.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0401"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Синхронные и асинхронные транзакции </span></h4>
   </div></div></div>
   <p>ZFS целиком посвящена целостности данных, которые достигают постоянного хранилища. Данные на диске 
   всегда должны быть согласованы. Система может потерять данные в промежутке между программой и диском, но 
   никакая файловая система не может защитить данные в полёте, расположенные только в оперативной памяти.</p>
   <p>Чтобы обеспечить целостность на диске ZFS группирует запросы на запись в группы транзакции (<span 
   class="emphasis"><em>txgs</em></span>, transaction groups). Группа транзакций является порцией данных и 
   связанных с файловой системой метаданных. Когда вы просите свою систему выполнить запись на диск, ZFS собирает
   такие запросы в группу транзакции. Одна группа транзакции может содержать подобные запросы на запись от 
   многих несвязанных процессов. Когда группа собирает достаточно данных, или истекает установленный 
   промежуток времени, эта группа транзакции записывается на соответствующий диск. Подобный предустановленный 
   интервал времени может быть длительным и составлять 30 секунд, или быть короче, 5 секунд, в зависимости от 
   того, какую редакцию FreeBSD вы выполняете.</p>
   <p>Группа транзакции является списком что-делать. В точности как ваш список дел, если не случится ничего 
   ужасного, продерётся сквозь волков. (Волки, конечно не интересуются бумажными списками. Чтобы эта метафора 
   работала хорошо, мы представляем что вы пишите свои списки дел на жареных отбивных). Данные чувствительны 
   к отказам системы пока они не записаны целиком на диск. В случае отказа системы или её кончины до выполнения 
   записи группы транзакции на диск, эти данные будут утрачены. Уменьшение таймаута группы транзакции может 
   уменьшить объём теряемых данных, но это также плохо влияет на производительность.</p>
   <p>Так как вы- системный администратор, это именно ваша работа управлять риском потери данных. Давайте 
   пройдёмся по записи данных на диск.</p>
   <p>Программа передаёт ядру порцию данных и просит: &quot;Пожалуйста, запиши это на диск.&quot; Программа 
   не выполняется пока ядро не выдаст подтвердит приём данных. Если уж ядро скжет, &quot;Я получило данные,&quot; 
   программа продолжит своё выполнение. Ожидание программой этого отклика называется блокировкой по вводу/ выводу
   (<span class="emphasis"><em>blocking on I/O</em></span>).</p>
   <p>Важный вопрос стсотит в том, когда именно ядро подтвержает приём этих данных? Когда данные добавлены в группу 
   транзакции, или когда они записаны на диск?</p>
   <p>При нормальной работе ядро подтверждает данные, когда эти данные находятся в памяти как часть незавершённой 
   группы транзакции. Эти данные не находятся на диске - ваше ядро просто берёт на себя ответственность за 
   данные. Если бы ваша система была рестораном, обед сейчас должен был бы находится в руках официанта на пути 
   к клиенту, однако официант ещё может идти. Варианты асинхронных операций являются распространёнными в современных 
   файловых системах, например, в различных версиях etfs Linux и UFS BSD.</p>
   <p>Кроме того, файловая система может работать в синхронном режиме, при котором подтверждение данных от ядра 
   происходит только тогда, когда биты реально записываются на физический носитель хранения. Синхронное монтирование 
   очень  надёжное, однако также и чрезвычайно медленное. Записывающая данные программа будет блокирована 
   ожиданием пока физическое оборудование не предоставит ответ на запрос записи ядром. Определённые програмы, 
   например серверы баз данных, запрашивают синхронное подтверждение для определённых файлов применяя вызов системы 
   <span class="term"><code>fsync(2)</code></span>. Системный администратор может монтировать набор данных 
   синхронным, таким образом система получит подтверждение только когда запись данных выполнена, или могут 
   применять программу <span class="term"><code>fsync(2)</code></span> для сообщения системе о необходимости 
   сброса всего на диск прямо сейчас.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0402"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Целевой журнал ZFS </span></h4>
   </div></div></div>
   <p>Когда ZFS записывает в синхронном режиме, она не выталкивает немедленно группу транзакции на диск. Вместо 
   этог, такие записи сбрасываются в ZIL. Они не являются аккуратными запрошенными блоками наборов данных ZFS; 
   вместо этого ни просто являются кучей блоков на вашем диске. Как только группа транзакций записана на диск, 
   блоки в ZIL записываются на свои соответствующие местоположения.</p>
   <p>Процесс импорта пула проверяет ZIL на наличие данных, которые ещё не достигли конечного дома. Если система 
   находит блоки в полёте в импорте пула, она выполняет эти транзакции.</p>
   <p>Обычно пул использует небольшие порции пространства на каждом поставщике в качестве ZIL. Да, это означает 
   что каждая синхронная запись должна быть выполнена на физическом устройстве хранения дважды. Однако пул 
   применяет ZIL только синхронно- записываемых данных. Обычно асинхронные записи сохраняются в оперативной 
   памяти и фиксируются как часть регулярной группы транзакции.</p>
   <p>Иногда вы можете улчшить производительность, помещая ZIL на выделенное, быстрое устройство, называемое 
   отдельным целевым журналом (Separate Intent LOG).</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0403"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Отдельный целевой журнал ZFS </span></h4>
   </div></div></div>
   <p>Вы можете выделить из пула применяя отдельный целевой журнал (Separate Intent Log), или 
   <span class="emphasis"><em>SLOG</em></span>. Перемещая ZIL на отдельное, выделенное оборудование, вы 
   избегаете выполнение записи одних и тех же данных дважды на одного поставщика хранения. Если оборудование 
   SLOG быстрее чем ваш пул, ядро может выдать подтверждение данных намного быстрее, улучшая производительность
   выполняющего запрос приложения.</p>
   <p>Вопреки обычному применению, SLOG не является тем же предметом, что и ZIL. SLOG является аппаратными 
   средствами. ZIL же функционирует либо в SLOG, либо на поставщике хранения. Вы можете запустить SLOG в 
   окне, но ругать вы можете только ZIL.</p>
   <p>Самым быстрым, наиболее надёжным, но и самым затратным SLOG является микросхема NVRAM. Наболее часто 
   применяемыми являются SSD с большим количеством перезаписей {<span class="emphasis"><em>Прим. пер.: eMLC</em></span>}.
   Вы даже можете применять очень быстрые диски SAS, однако они менее надёжные. {<span class="emphasis"><em>Прим. пер.: 
   сомнительное утверждение. Более существенны действующие в настоящее время программы у многих производителей 
   оборудования предложения этих типов устройств одного объёма по одной цене.</em></span>}. Для каждого из 
   таких устройств необходим собственный источник питания, например, батарея или суперконденсатор, чтобы 
   позволить им завершить запись в случае отказа питания системы.</p>
   <p>SLOG не должен быть очень большим <span class="term"><code>sysctl vfs.zfs.dirty_data_max</code></span> 
   устанавливает максимально возможный объём данных на лету. Значением по умолчанию ZFS FreeBSD 10 состоит в 
   применении ZIL с размером, равным одной десятой оперативной памяти системы. Вы можете применять одно 
   устройство соотвествующего оборудования для поддержки поставщиков SLOG более чем для одного пула, однако это 
   также расщепляет ввод/ вывод данного устройства между такими пулами. Одна из причин для применения SLOG 
   состоит в том, чтиобы справиться с нехваткой ввода/ вывода.</p>
   <p>Не все SSD или NVRAM создаются одинаковыми. Многие устройства маркируемые как &quot;долгоиграющие&quot;
   (high endurance), не являются достаточно надёжными для обработки всех записей даже в пулах среднего 
   размера. Для приложений, для которых целостность данных жизненно необходима, авторы настоятельно 
   рекомендуют вам консультироваться с поставщиком аппаратных средств, специализирующемся в ZFS, например, 
   IX Systems (<a class="link" href="http://www.ixsystems.com" target="_top">http://www.ixsystems.com</a>) 
   {<span class="emphasis"><em>Прим. пер.: или <a class="link" href="http://www.mdl.ru" target="_top">mdl.ru</a></em></span>}.
   Надлежащим образом выбранный SLOG может чрезвычайно ускорить ваши программы, в то время как плохой выбор 
   может разрушить ваш пул.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0404"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Настройка целевого журнала по наборам данных </span></h4>
   </div></div></div>
   <p>Вы не можете управлять тем, как (или если) набор данных применяет ZIL при установленном свойстве 
   <span class="term"><strong class="userinput"><code>sync</code></strong></span>. Во много по аналогии с 
   монтированием обычной файловой системы <span class="term"><code>sync</code></span> или
   <span class="term"><code>async</code></span>, свойство <span class="term"><strong class="userinput"><code>sync</code></strong></span>
   предписывает набору данных стоит ли набору данных воспринимать запросы 
   <span class="term"><code>fsync(2)</code></span>.</p>
   <p>Установка по умолчанию, <span class="emphasis"><em>standard</em></span>, сообщает набору данных применять ZIL 
   для запросов синхронизации. Если программа использует <span class="term"><code>fsync(2)</code></span> для 
   запроса, который  ядро не подтвердит, пока данные не сохранятся на диске, данные записываются в ZIL. Все прочие 
   данные записываются асинхронно как часть группы транзакции. Это значение по умолчанию.</p>
   <p>Установка <span class="term"><strong class="userinput"><code>sync</code></strong></span> в значение 
   <span class="emphasis"><em>always</em></span> пересылает все значения в соответствующий ZIL. Никакие записи 
   не являются асинхронными. Это самый надёжный способо управления данными, однако он имеет существенные 
   отрицательные последствия в производительности. Вы можете выбрать установку этого свойства на наборе данных, 
   выделенном для критически важных данных.</p>
   <p>Установка <span class="term"><strong class="userinput"><code>sync</code></strong></span> в значение 
   <span class="emphasis"><em>disabled</em></span> полностью запрещает применение ZIL на этом наборе данных. Все 
   записи являются асинхронными. Система врёт всем программам что применяет <span class="term"><code>fsync(2)</code></span>.
   Никогда, никогда не запрещайте свои ZIL на каком- либо наборе данных, используемом для баз данных или NFS 
   {<span class="emphasis"><em>Прим. пер.: а также Samba и прочих совместно используемых ресурсах!</em></span>}.
   В действительности, единственная причина запретить ZIL на наборе данных состоит в том, чтобы проверить что ZIL 
   не вызывает проблем с производительностью у вашего определённого приложения. Если запрет вашего ZIL устраняет 
   проблему приложения, несомненно зарегистрируйте сообщение об ошибке или установите быстрое устройство SLOG.</p>
   <p>Практически во всех случаях оставляйте <span class="term"><strong class="userinput"><code>sync</code></strong></span> в значение 
   <span class="emphasis"><em>standard</em></span> У вас может быть один или два набора данных которым нужен
   <span class="term"><strong class="userinput"><code>sync</code></strong></span>, установленный в значение 
   <span class="emphasis"><em>always</em></span>.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="0405"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Выполнение синхронных записей в стеке </span></h4>
   </div></div></div>
   <p>ZFS в конечном итоге выступает в какчестве основы хранения данных для многих различных приложений, 
   например, таких как Network File System (NFS) и iSCSI. Вы можете применять zvol для виртуальных дисков 
   системы. Все эти различные уровни работают независимо. Хотя они и могут общаться друг с другом посредством 
   общих системных вызовов и API, они не могут управлять друг другом. Каждый уровень стека приложений может 
   (и занимается этим регулярно) врёт прочим уровням. И нигде более это не так очевидно и не так наиболее 
   опасно, как в системном вызове <span class="term"><code>fsync(2)</code></span>.</p>
   <p>Предположим, у вас есть виртуальная машина, которая работает с неким диском iSCSI, в основе которого 
   лежит zvol вашего сервера. Операционная система виртуальной машины запрашивает синхронную запись на диск.
   Стек iSCSI берёт этот запрос и передаёт его ZFS. Если вы установили 
   <span class="term"><strong class="userinput"><code>sync</code></strong></span> в значение 
   <span class="emphasis"><em>disabled</em></span> на этом zvol, ZFS сдавленно хихикает, приговаривая 
   &quot;Синхронно? Конечно! Ты поимеешь это, дружище!&quot;, и продолжает делать что-нибудь до следующей 
   txg.</p>
   <p>Вы можете установить <span class="term"><strong class="userinput"><code>sync</code></strong></span> в значение 
   <span class="emphasis"><em>always</em></span> на этом zvol, принимая провал производительности во имя 
   целостности данных. Однако если данный стек iSCSI запрещает выполнение синхронных записей, вы получите провал 
   без каких бы то ни было преимуществ. Любой уровень сложного стека приложений может запретить выполнение 
   синхронных записей.</p>
   <p>Если целостность данных важна, убедитесь что синхронная запись работает во всём стеке приложений.</p>
  </div>
   
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="05"> </a>zpool.cache</h3>
   </div></div></div>
   <p>Теперь давайте обсудим кэш, о котором вы будете слышать, но который не влияет на ежедневное системное 
   администрирование: файл <span class="term"><code><em>/boot/zfs/zpool.cache</em></code></span>.</p>
   <p>Файл <span class="term"><code><em>zpool.cache</em></code></span> содержит описание текущего активного пула 
   и его поставщиков. Когда вы загружаете систему ZFS, ядро проверяет в корне вашего пула файл 
   <span class="term"><code><em>zpool.cache</em></code></span> для обнаружения того, какие системные пулы 
   оно должно импортировать.</p>
   <p>Файл <span class="term"><code>zdb(8)</code></span> использует информацию в файле кэша для отладки. Вы 
   не можете использовать отладчик в пуле без кэша.</p>
   <p>Вы можете управлять местоположением файла кэша при помощи свойства
   <span class="term"><strong class="userinput"><code>cachefile</code></strong></span>. Вот мы изменяем файл 
   кэша для вашего пула <span class="term"><code>work</code></span>.</p>
	   <pre class="screen"><code>
# zpool set cachefile=/work/zfs/work.zpool.cache work
 	   </code></pre>
   <p>Несмотря на большую давность и всё- таки устаревающую документацию, почти никогда нет причины изменять 
   местоположение файла кэша современной ZFS в FreeBSD.</p>
   <p>Теперь, когда вы приобрели навыки кэширования, мы можем обсуждать производительность.</p>
  </div>
   
   
 </div>

<!----><script type="text/javascript" src="FooterAndSidebar.js">
</script><script type="text/javascript"><!--</div id="content"> is inside next code
document.write(FooterAndSidebar);//-->
</script>

</body></html>l>