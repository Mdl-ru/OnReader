<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:exsl="http://exslt.org/common" xmlns:ng="http://docbook.org/docbook-ng" xmlns:fb="http://ogp.me/ns/fb#">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link href="https://netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css" rel="stylesheet"/>
<title>Глава 10. Ещё о Ceph - Книга рецептов Ceph</title>
<meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"/>
<meta name="mavenGroupId" content="www.mdl.ru"/>
<meta name="mavenArtifactId" content="CephCookbook"/>
<meta name="mavenVersionId" content="1.0.0"/>
<link rel="home" href="index.html" title="Книга рецептов Ceph"/>
<link rel="up" href="index.html" title="Книга рецептов Ceph"/>
<link rel="prev" href="Ch09.html" title="Глава 9. Менеджер виртуального хранения Ceph"/>
<link rel="next" href="UnderstandingErasureCodingOffload.html" title="Приложения"/>
<meta name="git-sha" content=""/>
<meta name="buildTime" content=""/>
<script type="text/javascript">
            //The id for tree cookie
            var treeCookieId = "treeview-openstack-operations-guide";
            var language = "en";
            var w = new Object();
            //Localization
            txt_filesfound = 'Results';
            txt_enter_at_least_1_char = "You must enter at least one character.";
            txt_browser_not_supported = "Please enable JavaScript.";
            txt_please_wait = "Please wait. Search in progress...";
            txt_results_for = "Results for: ";
</script>
<style type="text/css">
            input {
            margin-bottom: 5px;
            margin-top: 2px;
            }

            .folder {
            display: block;
            height: 22px;
            padding-left: 20px;
            background: transparent url(../common/jquery/treeview/images/folder.gif) 0 0px no-repeat;
            }
</style>
<link rel="shortcut icon" href="../MdlLogo.gif" type="image/gif"/>
<link rel="stylesheet" type="text/css" href="../common/css/positioning.css"/>
<link rel="stylesheet" type="text/css" href="../common/css/custom.css"/>
<link rel="canonical" href="http://onreader.mdl.ru/CephCookbook/content/index.html"/>
<!--[if IE]>
	<link rel="stylesheet" type="text/css" href="../common/css/ie.css"/>
<![endif]-->
<link rel="stylesheet" type="text/css" href="../common/jquery/theme-redmond/jquery-ui-1.8.2.custom.css"/>
<link rel="stylesheet" type="text/css" href="../common/jquery/treeview/jquery.treeview.css"/>
<script type="text/javascript" src="http://code.jquery.com/jquery-1.11.0.min.js"><!----></script>
<script type="text/javascript" src="../common/jquery/jquery-ui-1.8.2.custom.min.js"><!----></script>
<script type="text/javascript" src="../common/jquery/jquery.cookie.js"><!----></script>
<script type="text/javascript" src="../common/jquery/treeview/jquery.treeview.min.js"><!----></script>
<link rel="stylesheet" type="text/css" href="http://cdn.jsdelivr.net/qtip2/2.2.0/jquery.qtip.min.css"/>
<script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/qtip2/2.2.0/jquery.qtip.min.js">
<!--jQuery plugin for glossary popups. --></script><script type="text/javascript" src="search/htmlFileList.js"><!----></script>
<script type="text/javascript" src="search/htmlFileInfoList.js"><!----></script>
<script type="text/javascript" src="search/nwSearchFnt.js"><!----></script>
<script type="text/javascript" src="search/stemmers/en_stemmer.js">
<!--//make this scalable to other languages as well.--></script>
<script type="text/javascript" src="search/index-1.js"><!----></script>
<script type="text/javascript" src="search/index-2.js"><!----></script>
<script type="text/javascript" src="search/index-3.js"><!----></script>
<script type="text/javascript">
	    var _gaq = _gaq || [];
	    _gaq.push(['_setAccount', 'UA-17511903-1']);
	    
	    _gaq.push(['_setDomainName', '.openstack.org']);	        
</script>
<script type="text/javascript" src="../common/ga.js"><!----></script>
<script language="javascript" src="/js/common.js"></script>
<link rel="stylesheet" href="../common/css/googlecode.css">
<script src="../common/highlight.pack.js"></script>
</head>
<body>
<!----><script type="text/javascript"><!--
hljs.initHighlightingOnLoad();
HeaderName = 'Глава 10. Ещё о Ceph';
PrevRef = 'Ch09.html';
UpRef = 'index.html';
NextRef = 'UnderstandingErasureCodingOffload.html';//-->
</script>
<!----><script type="text/javascript" src="HeaderAndToolbar.js">
</script><script type="text/javascript"><!--
document.write(HeaderAndToolbar); //-->
</script>
<div id="content">
 <div class="part">
  <div xmlns="" class="titlepage"><div><div><h1 xmlns="http://www.w3.org/1999/xhtml" class="title">
   Глава 10. Ещё о Ceph</h1>
  </div></div></div>
  <div class="partintro"><div xmlns=""/>
  <p>В данной главе мы обсудим следующие рецепты:
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Эталонное тестирование кластера Ceph</p>
	 </li><li class="listitem">
	 <p>Базовый уровень производительности дисков</p>
	 </li><li class="listitem">
	 <p>Базовый уровень производительности сетевой среды</p>
	 </li><li class="listitem">
	 <p>Показатели Ceph RADOS</p>
	 </li><li class="listitem">
	 <p>RADOS load-gen</p>
	 </li><li class="listitem">
	 <p>Эталонное тестирование блочного устройства Ceph</p>
	 </li><li class="listitem">
	 <p>Эталонное тестирование Ceph RBD с применением FIO</p>
	 </li><li class="listitem">
	 <p>Сокет администратора Ceph</p>
	 </li><li class="listitem">
	 <p>Применение команды ceph tell</p>
	 </li><li class="listitem">
	 <p>Ceph REST API</p>
	 </li><li class="listitem">
	 <p>Профилирование памяти Ceph</p>
	 </li><li class="listitem">
	 <p>Развёртывание Ceph с применением Ansible</p>
	 </li><li class="listitem">
	 <p>Инструментарий ceph-objectstore</p>
	 </li>
   </ul>
   </div>
  </p>
  </div>

  <div class="toc"><p><strong>Содержание</strong></p>
   <dl>
   <dt><span class="chapter"><a href="Ch10.html">10. Ещё о Ceph</a></span></dt>
   <dd><dl>
	<dt><span class="section"><a href="Ch10.html#1001">Введение</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1002">Эталонное тестирование кластера Ceph</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1003">Базовый уровень производительности дисков</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1004">Базовый уровень производительности сетевой среды</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1005">Показатели Ceph RADOS</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1006">RADOS load-gen</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1007">Эталонное тестирование блочного устройства Ceph</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1008">Эталонное тестирование Ceph RBD с применением FIO</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1009">Сокет администратора Ceph</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1010">Применение команды ceph tell</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1011">Ceph REST API</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1012">Профилирование памяти Ceph</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1013">Развёртывание Ceph с применением Ansible</a></span></dt>
	<dt><span class="section"><a href="Ch10.html#1014">Инструментарий ceph-objectstore</a></span></dt>
   </dl></dd>
   </dl>
  </div>
  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1001"> </a>Введение</h3>
   </div></div></div>
   <p>В предыдущих главах мы рассматривали различные способы развёртывания, предоставления и администрирования Ceph. В этой главе мы 
   изучим эталонное тестирование имеющегося кластера Ceph, что является необходимым для выполнения этапом перед перемещением решения 
   в промышленное применение. Мы также обсудим расширенные методы администрирования и обнаружения неисправностей Ceph с применением 
   разъёма администратора, REST API и инструментария ceph-objectstore. Наконец мы изучим профилирование памяти Ceph, а также 
   развёртывание Ceph с применением Ansible, который является очень эффективным способом развёртывания Ceph.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1002"> </a>Эталонное тестирование кластера Ceph</h3>
   </div></div></div>
   <p>Настоятельно рекомендуется перед размещением под рабочие нагрузки промышленного применения выполнить эталонное тестирование 
   вашего кластера Ceph. Эталонное тестирование даст вам приблизительные результаты того, что ваш кластер будет предоставлять в процессе 
   рабочих нагрузок по чтению, записи, латентности и т.д.</p>
   <p>Перед выполнением действительного эталонного тестирования неплохо установить отправные точки для ожидаемой максимальной 
   производительности измерив производительность присоединяемого к кластерному узлу оборудования, например, дисков и элементов 
   сетевой среды.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1003"> </a>Базовый уровень производительности дисков</h3>
   </div></div></div>
   <p>Тестирование базового уровня производительности дисков выполняется в два этапа. Вначале мы измеряем производительность 
   отдельного диска, а после этого мы измерим производительность всех наших дисков, подключённых к одному узлу OSD при их работе
   одновременно.</p>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Для получения реалистичных результатов я выполняю эталонное тестирование описанное в данном рецепте не на самом развёрнутом кластере, 
	   а на физических аппаратных средствах. Мы также можем выполнить это тесты на вашем кластере Ceph, размещающемся на виртуальной машине, но мы 
	   можем не получить привлекательных результатов.</p>
	   </td></tr></table>
     </div>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100301"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Производительность отдельного диска на запись </span></h4>
   </div></div></div>
   <p>Для получения производительности дисковых чтения и записи мы воспользуемся командой 
   <span class="term"><code>dd</code></span> с <span class="term"><code>oflag</code></span> установленным в значение 
   <span class="term"><code>direct</code></span> для достижения обхода дискового кэша с целью получения реалистичных результатов.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100302"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Прекратите кэширование.</p>
	   <pre class="screen">
# echo 3 &gt; /proc/sys/vm/drop_caches
	   </pre>
	 </li><li class="listitem">
      <p>Используйте <span class="term"><code>dd</code></span> для записи файла с именем <span class="term"><code>deleteme</code></span>
	  и размером <span class="term"><code>10ГБ</code></span> заполненным нулями <span class="term"><code>/dev/zero</code></span> в качестве 
	  входного файла <span class="term"><code>if</code></span> в каталог, в который смонтирован OSD Ceph, т.е. 
	  <span class="term"><code>/var/lib/ceph/osd/ceph-0/</code></span>.</p>
	   <pre class="screen">
# dd if=/dev/zero of=/var/lib/ceph/osd/ceph-0/deleteme bs=10G count=1 oflag=direct
	   </pre>
	 </li>
	 </ol>
   </div>
   <p>В идеале вы должны повторить шаги 1 и 2 несколько раз и получить среднее значение. В нашем случае среднее значение для операций записи 
   приближается к 319 МБ/с, как показано на следующем экранном снимке:</p>
	   <pre class="screen">
	[root@ceph-node1 ~]# dd if=/dev/zero of=/var/lib/ceph/osd/ceph-0/deleteme bs=10G count=1 oflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 6.66535 s, 322 MB/s 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# dd if=/dev/zero of=/var/lib/ceph/osd/ceph-0/deleteme bs=10G count=1 oflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 7.09217 s, 303 MB/s 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# dd if=/dev/zero of4var/lib/ceph/osd/ceph-0/deleteme bs=10G count=1 oflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 6.45077 s, 333 MB/s 
	[root@ceph-node1 ~]# 
	   </pre>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100303"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Производительность множества дисков на запись </span></h4>
   </div></div></div>
   <p>В качестве следующего шага мы выполним <span class="term"><code>dd</code></span> 
   на всех ваших дисках OSD используемых Ceph на этом узле, <span class="term"><code>ceph-node1</code></span>, для получения суммарной 
   производительности дисковой записи предоставляемой отдельным узлом.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100304"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Получите общее число дисков, используемых на вашем OSD Ceph, в моём случае это 25 дисков:</p>
	   <pre class="screen">
# mount | grep -i osd | wc -l
	   </pre>
	 </li><li class="listitem">
      <p>Прекратите кэширование.</p>
	   <pre class="screen">
# echo 3 &gt; /proc/sys/vm/drop_caches
	   </pre>
	 </li><li class="listitem">
      <p>Следующая команда выполнит команду <span class="term"><code>dd</code></span> на всех дисках OSD Ceph:</p>
	   <pre class="screen">
# for i in `mount | grep osd | awk '{print $3}'`; do (dd if=/dev/zero of=$i/deleteme bs=10G count=1 oflag=direct &) ; done
	   </pre>
	 </li>
	 </ol>
   </div>
   <p>Для получения средней суммарной производительности записи возьмите среднее значение от всех скоростей записи.
   В моём случае среднее значение достигло 60 МБ/с.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100305"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Производительность отдельного диска на чтение </span></h4>
   </div></div></div>
   <p>Для получения производительности дискового чтения отдельного диска мы снова воспользуемся командой 
   <span class="term"><code>dd</code></span>.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100306"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Прекратите кэширование.</p>
	   <pre class="screen">
# echo 3 &gt; /proc/sys/vm/drop_caches
	   </pre>
	 </li><li class="listitem">
      <p>Используйте <span class="term"><code>dd</code></span> для записи файла с именем <span class="term"><code>deleteme</code></span>, 
	  который мы создале в процессе тестирования записи. Мы будем читать наш файл <span class="term"><code>deleteme</code></span> 
	  в <span class="term"><code>/dev/null</code></span> с <span class="term"><code>iflag</code></span> установленным в значение
	  <span class="term"><code>direct</code></span>:</p>
	   <pre class="screen">
# dd if=/var/lib/ceph/osd/ceph-0/deleteme of=/dev/null bs=10G count=1 iflag=direct
	   </pre>
	 </li>
	 </ol>
   </div>
   <p>В идеале вы должны повторить шаги 1 и 2 несколько раз и получить среднее значение. В нашем случае среднее значение для операций записи 
   приближается к 178 МБ/с, как показано на следующем экранном снимке:</p>
	   <pre class="screen">
	[root@ceph-node1 ~]# echo 3 &gt; /proc/sys/vm/drop_caches 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# dd if=/var/lib/ceph/osd/ceph-0/deleteme of=/dev/null bs=10G count=1 iflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 12.0557 s, 178 MB/s 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# dd if=/var/lib/ceph/osd/ceph-O/deleteme of=/dev/null bs=10G count=1 iflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 12.0452 s, 178 MB/s 
	[root@ceph-node1 ~]# dd if=/var/lib/ceph/osd/ceph-O/deleteme of=/dev/null bs=10G count=1 iflag=direct 
	0+1 records in 
	0+1 records out 
	2147479552 bytes (2.1 GB) copied, 12.0408 s, 178 MB/s 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# 
	   </pre>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100307"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Производительность множества диска на чтение </span></h4>
   </div></div></div>
   <p>Аналогично производительности отдельного диска на чтение мы используем <span class="term"><code>dd</code></span> 
   для получения суммарной производительности на чтение множества дисков.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100308"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Получите общее число дисков, используемых на вашем OSD Ceph, в моём случае это 25 дисков:</p>
	   <pre class="screen">
# mount | grep -i osd | wc -l
	   </pre>
	 </li><li class="listitem">
      <p>Прекратите кэширование.</p>
	   <pre class="screen">
# echo 3 &gt; /proc/sys/vm/drop_caches
	   </pre>
	 </li><li class="listitem">
      <p>Следующая команда выполнит команду <span class="term"><code>dd</code></span> на всех ваших дисках Ceph OSD:</p>
	   <pre class="screen">
# for i in `mount | grep osd | awk '{print $3}'`; do (dd if=$i/deleteme of=/dev/null bs=10G count=1 iflag=direct &); done
	   </pre>
	 </li>
	 </ol>
   </div>
   <p>Для получения суммарной производительности дискового чтения возьмите среднее всех ваших скоростей чтения. В моём случае среднее
   значение приближается к 123 МБ/с.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100309"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Результаты </span></h4>
   </div></div></div>
   <p>На основании выполненных нами тестов результаты выглядят следующим образом. Эти результаты очень сильно изменяются от среды к 
   среде; используемоё вами оборудование и число дисков в вашем узле OSD могут иметь большое значение.</p>
        <table rules="all" width="500" style="text-align:center" id="add_fs_options">
        <caption></caption><col width="30%"/><col width="35%"/><col width="35%"/><thead><tr valign="top">
          <th>Операция</th>
          <th>на диск</th>
          <th>агрегированная</th>
        </tr></thead><tr valign="top">
          <td><p>чтение</td>
          <td><p>178МБ/с</p></td>
          <td><p>123МБ/с</p></td>
        </tr></thead><tr valign="top">
          <td><p>запсиь</td>
          <td><p>319МБ/с</p></td>
          <td><p> 60МБ/с</p></td>
        </tr></tbody></table>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1004"> </a>Базовый уровень производительности сетевой среды</h3>
   </div></div></div>
   <p>В этом разделе мы выполним тесты для исследования базовой производительности сети между узлами OSD Ceph. Для этого мы воспользуемся 
   утилитой <span class="term"><code>iperf</code></span>. Убедитесь что пакет <span class="term"><code>iperf</code></span> установлен на 
   ваших узлах. <span class="term"><code>iperf</code></span> является простым средством тестирования полосы пропускания сети точка- точка, 
   которое работает в модели клиент- сервер.</p>
   <p>Для запуска эталонного тестирования сети выполните <span class="term"><code>iperf</code></span> с параметром сервера на своём первом 
   узле, а с параметром клиента на своём втором узле.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100401"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>На <span class="term"><code>Ceph-node1</code></span> выполните <span class="term"><code>iperf</code></span> с 
	  <span class="term"><code>-s</code></span> для вашего сервера и <span class="term"><code>-p</code></span> для прослушивания 
	  определённого порта:</p>
	   <pre class="screen">
# iperf -s -p 6900

	[root@ceph-node1 ~]# iperf -s -p 6900 
	-----------------------------------------------------------
	Server listening on TCP port 6900 
	TCP window size: 85.3 KByte (default) 
	-----------------------------------------------------------
	[ 4] local 10.100.1.201 port 6900 connected with 10.100.1.202 port 39630 
	[ ID] Interval Transfer Bandwidth 
	[ 4] 0.0-10.0 sec  11.5 GBytes  9.87 Gbits/sec 
	^C[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# 
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Вы можете опустить параметр <span class="term"><code>-p</code></span> если порт TCP <span class="term"><code>5201</code></span> 
	   открыт или вы можете любой другой порт который открыт и не используется.</p>
	   </td></tr></table>
     </div>
	 </li><li class="listitem">
      <p>На <span class="term"><code>Ceph-node2</code></span> выполните <span class="term"><code>iperf</code></span> с параметром клиента,
	  <span class="term"><code>-c</code></span>:</p>
	   <pre class="screen">
# iperf -c ceph-node1 -p 6900

	[root@ceph-node2 ~]# iperf -c ceph-node1 -p 6900 
	-----------------------------------------------------------
	Client connecting to 10.100.1.201, TCP port 6900 
	TCP window size: 95.8 KByte (default) 
	-----------------------------------------------------------
	[ 3] local 10.100.1.202 port 39630 connected with 10.100.1.201 port 6900 
	[ ID] Interval Transfer Bandwidth 
	[ 3] 0.0-10.0 sec  11.5 GBytes  9.87 Gbits/sec 
	[root@ceph-node2 ~]# 
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Вы можете также применить параметр <span class="term"><code>-p</code></span> в своей команде <span class="term"><code>iperf</code></span> 
	   для определения числа параллельных соединений потоков создаваемых с вашим сервером. Это вернёт реалистичные результаты если у вас имеется 
	   техника связывания каналов, например, LACP.</p>
	   </td></tr></table>
     </div>
	 </li>
	 </ol>
   </div>
   <p>Результаты показывают, что у нас есть привлекательное сетевое соединение с 9.8Gb/s между нашими узлами Ceph. Аналогично мы можем выполнять 
   проверку пропускной способности сети для всех других узлов в вашем кластере Ceph. Полоса пропускания сетевой среды в реальности зависит от 
   применяемой вами между узлами Ceph сетевой инфраструктуры.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100402"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Смотрите также...</span></h4>
   </div></div></div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><a class="link" href="Ch08.html#080303" target="_top">Глава 8. Планирование промышленного применения и настройка производительности Ceph</a>, 
	 в которой вы найдёте дополнительную информацию связанную с сетевой средой.</p>
	 </li>
    </ul>
    </div>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1005"> </a>Показатели Ceph RADOS</h3>
   </div></div></div>
   <p>Ceph поставляется со встроенным инструментом эталонного тестирования, называемым RADOS bench, который может применяться для измерения 
   производительности кластера Ceph на уровне пула. Инструмент RADOS bench поддерживает тестирование записи, последовательного чтения и 
   случайного чтения, а также позволяет очищаться от временных данных эталонного тестирования, что очень аккуратно.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100501"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p>Давайте выполним некоторые тесты при помощи <span class="term"><code>rados</code></span> bench:</p>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Для выполнения 10 секундного теста записи в ваш пул <span class="term"><code>rbd</code></span> без очистки
	  выполните следующую команду:</p>
	   <pre class="screen">
# rados bench -p rbd 10 write --no-cleanup
	   </pre>
	 <p>После выполнения данной команды мы получим следующий снимок экрана:</p>
	   <pre class="screen">
	[root@ceph-node1 ~]# rados bench -p rbd 10 write --no-cleanup 
	Maintaining 16 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects 
	Object prefix: benchmarkdata_ceph-node1_3124629 
	  sec Cur ops   started finished  avg MB/s   cur MB/s  last lat   avg lat 
	    0       0         0        0         0          0         -         0 
	    1      16       118      102    407.85        408 0.0584212  0.127569 
	    2      16       207      191   381.895        356   0.20105  0.150813 
	    3      16       279      263   350.581        288  0.141772  0.168736 
	    4      16       351      335   334.921        288   0.57108  0.181988 
	    5      16       420      404    323.13        276 0.0724497   0.19139 
	    6      16       479      463   308.601        236  0.137025  0.194498 
	    7      16       547      531   303.367        272  0.253194  0.206116 
	    8      16       615      599   299.441        272  0.172813  0.208689 
	    9      16       692      676   300.386        308   0.48298  0.209028 
	   10      16       747      731   292.345        220  0.123282  0.211807 
	Total time run:         10.721111 
   Total writes made:      747 
   Write size:             4194304 
   Bandwidth (MB/sec):     278.702 
   
   Stddev Bandwidth:       102.44 
   Max bandwidth (MB/sec): 408 
   Min bandwidth (MB/sec): 0 
   Average Latency:        0.227756 
   Stddev Latency:         0.234691 
   Max latency:            1.5534 
   Min latency:            0.041106 
   [root@ceph-node1 ~]# 
	   </pre>
	 </li><li class="listitem">
      <p>Аналогично, для выполнения 10 секундного теста чтения в вашем пуле <span class="term"><code>rbd</code></span>, выполните следующую команду:</p>
	   <pre class="screen">
# rados bench -p rbd 10 seq

	[root@ceph-node1 ~]# rados bench -p rbd 10 seq 
	   sec Cur ops   started  finished  avg MB/s  cur MB/s  last lat   avg lat 
	     0       0         0         0         0         0         -         0 
	     1      16       247       231   923.573       924  0.181625 0.0620505 
	     2      16       489       473   945.703       968 0.0366547 0.0645318 
	     3      16       648       632   698.411       636  0.306308 0.0814809 
	 Total time run:        4.223407 
	Total reads made:     747 
	Read size:            4194304 
	Bandwidth (MB/sec):   707.486
	
	Average Latency:      0.0901875 
	Max latency:          1.03252 
	Min latency:          0.00977891 
	[root@ceph-node1 ~]#  
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>В данном случае может оказаться интересным узнать почему тест чтения завершился через несколько секунд или почему он не выполнялся 
	   предписанные 10 секунд. Это обусловлено тем, что ваша скорость чтения быстрее скорости записи, и <span class="term"><code>rados 
	   bench</code></span> завершил чтение всех имеющихся данных, сгенерированных в процессе вашего теста записи. Однако подобное поведение 
	   во многом зависит от вашей инфраструктуры оборудования и программных средств.</p>
	   </td></tr></table>
     </div>
	 </li><li class="listitem">
      <p>Аналогично выполните тестирование случайного чтения при помощи <span class="term"><code>rados bench</code></span>, исполнив следующую 
	  команду:</p>
	   <pre class="screen">
# rados bench -p rbd 10 rand
	   </pre>
	 </li>
	 </ol>
   </div>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100502"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это работает...</span></h4>
   </div></div></div>
   <p>Синтаксис для <span class="term"><code>rados bench</code></span> таков:</p>
	   <pre class="screen">
# rados bench -p &lt;pool_name&gt; &lt;seconds&gt; &lt;write|seq|rand&gt; -b &lt;block size&gt; -t --no-cleanup
	   </pre>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><span class="term"><code>-p</code></span> или <span class="term"><code>--pool</code></span>: определяет имя пула</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>&lt;seconds&gt;</code></span>: время тестирования в секундах
	 </p>
	 </li><li class="listitem">
	 <p><span class="term"><code>&lt;write|seq|rand&gt;</code></span>: тип тестирования, {а именно}: запись, последовательное чтение 
	 или случайное чтение
	 </p>
	 </li><li class="listitem">
	 <p><span class="term"><code>-b</code></span>: для определения размера блока; по умолчанию он 4M
	 </p>
	 </li><li class="listitem">
	 <p><span class="term"><code>-t</code></span>: число параллельных потоков; по умолчанию 16
	 </p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--no-cleanup</code></span>: Ваши временные данные, которые записываются в указанный пул при помощи 
	 <span class="term"><code>rados bench</code></span> не должны быть вычищены. Эти данные будут использованы для операций чтения 
	 когда они используются при последовательных чтениях ил случайных чтениях. Значение по умолчанию установлено в очистку.</p>
	 </li>
    </ul>
    </div>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Как было определено в последнем рецепте, я выполняю эти тесты на физическом кластере Ceph. Вы конечно можете выполнить эти команды 
	   на кластере Ceph созданном на виртуальных машинах, однако вы можете не получить удовлетворительных результатов в виртуальной среде.</p>
	   </td></tr></table>
     </div>
   <p><span class="term"><code>rados bench</code></span> является достаточно удобным инструментом для быстрого измерения сырой производительности 
   вашего кластера Ceph и вы можете творчески проектировать свои собственные тесты на основе профилей записи, чтения и случайного чтения.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1006"> </a>RADOS load-gen</h3>
   </div></div></div>
   <p>Слегка похожий на <span class="term"><code>rados bench</code></span>, <span class="term"><code>rados load-gen</code></span> 
   является другим интересным инструментом, предоставляемым Ceph, который работает сразу после вынимания из коробки. Как подсказывает его 
   название, инструмент <span class="term"><code>rados load-gen</code></span> может применяться для генерации нагрузки и может оказаться 
   полезным при симуляции сценариев с высокой нагрузкой.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100601"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p>Давайте попытаемся создать некоторую нагрузку на наш кластер Ceph при помощи следующей команды:</p>
	   <pre class="screen">
# rados -p rbd load-gen \
   --num-objects 50 \
   --min-object-size 4M \
   --max-object-size 4M \
   --max-ops 16 \
   --min-op-len 4M \
   --max-op-len 4M \
   --percent 5 \
   --target-throughput 2000 \
   --run-length 60
	   </pre>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100602"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это работает...</span></h4>
   </div></div></div>
   <p>Синтаксис <span class="term"><code>rados load-gen</code></span> таков:</p>
	   <pre class="screen">
# rados -p &lt;pool-name&gt; load-gen
	   </pre>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><span class="term"><code>--num-objects</code></span>: общее число объектов</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--min-object-size</code></span>: минимальный размер объекта в байтах</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--max-object-size</code></span>: максимальный размер объекта в байтах</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--min-ops</code></span>: минимальное число операций</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--max-ops</code></span>: максимальное число операций</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--min-op-len</code></span>: минимальная длина операции</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--max-op-len</code></span>: максимальная длина операции</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--max-backlog</code></span>: максимум незавершённых заданий (в МБ)</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--percent</code></span>: процент операций чтения</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--target-throughput</code></span>: пропускная способность получателя (в МБ)</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--run-length</code></span>: общее время работы в секундах</p>
	 </li>
    </ul>
    </div>
   <p>Эта команда сгенерирует нагрузку на ваш кластер Ceph посредством записи 50 объектов в пул <span class="term"><code>rbd</code></span>.
   Длина каждого из этих объектов и операций составит в размере 4M с 5% чтения и временем тестирования 60 секунд.</p>
	   <pre class="screen">
	[root@ceph-node1 ~]# rados -p rbd load-gen \
	&gt; --num-objects 50 \ 
	&gt; --min-object-size 4M \ 
	&gt; --max-object-size 4M \ 
	&gt; --max-ops 16 \ 
	&gt; --min-op-len 4M \ 
	&gt; --max-op-len 4M \ 
	&gt; --percent 5 \ 
	&gt; --target-throughput 2000 \ 
	&gt; --run-length 60 
	run length 60 seconds 
	preparing 50 objects 
	load-gen will run 60 seconds 
	    1: throughput=0MB/sec pending data=0 
	READ : oid=obj-xtuVtifS5zQ55da off=0 len=4194304 
	READ : oid=obj-0NvPNB07Lz1rTra off=0 len=4194304 
	WRITE : oid=obj-UeV2NunBsTSrYUw off=0 len=4194304 
	op 17 completed, throughput=4MB/sec 
	READ : oid=obj-fL1p0c_7CgEtjlk off=0 len=4194304 
	op 18 completed, throughput=8MB/sec 
	   </pre>
   <p>Вывод был усечён для краткости мотивации. Когда команда <span class="term"><code>load-gen</code></span> завершится, она вычистит за собой 
   все объекты, которые она создавала для тестирования и выведет рабочую пропускную способность.</p>
	   <pre class="screen">
	op 5519 completed, throughput=373MB/sec 
	waiting for all operations to complete 
	cleaning up objects 
	op 5522 completed, throughput=367MB/sec 
	op 5521 completed, throughput=367MB/sec 
	[root@ceph-node1 ~]# 
	   </pre>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100603"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Есть кое-что еще...</span></h4>
   </div></div></div>
   <p>Также вы можете наблюдать за состоянием вашего кластера на предмет скорости/ операций чтения и записи с помощью команды 
   <span class="term"><code>watch ceph -s</code></span>, в то время, пока будет работать <span class="term"><code>rados 
   load-gen</code></span>, просто чтобы смотреть что происходит.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1007"> </a>Эталонное тестирование блочного устройства Ceph</h3>
   </div></div></div>
   <p>Описанные в последнем рецепте инструменты <span class="term"><code>rados bench</code></span> и <span class="term"><code>rados 
   load-gen</code></span> применяются для эталонного тестирования пула вашего кластера Ceph. В данном рецепте мы сосредоточимся на 
   эталонном тестировании блочного устройства Ceph пи помощи инструмента <span class="term"><code>rbd bench-write</code></span>.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100701"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Ceph rbd bench-write</span></h4>
   </div></div></div>
   <p>Интерфейс командной строки <span class="term"><code>ceph rbd</code></span> предоставляет параметр, называемый 
   <span class="term"><code>bench-write</code></span>, который является инструментом для выполнения операций эталонного тестирования в 
   ваше блочное устройство RADOS Ceph.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100702"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p>Чтобы выполнить эталонное тестирование блочного устройства Ceph, нам необходимо создать некое блочное устройство и отобразить 
   на ваш узел клиента Ceph:</p>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Создайте блочное устройство с именем <span class="term"><code>block-device1</code></span> и размером 1ГБ, а потом отобразите его:</p>
	   <pre class="screen">
# rbd create block-device1 --size 10240
# rbd info --image block-device1
# rbd map block-device1
# rbd showmapped

	[root@ceph-client1 ~]# rbd create block-device1 --size 10240 
	[root@ceph-client1 ~]# rbd info --image block-device1 
	rbd image 'block-device1': 
	        size 10240 MB in 2560 objects 
	        order 22 (4096 kB objects) 
	        block_name_prefix: rb.0.4cbacc.238e1f29 
	        format: 1 
	[root@ceph-client1 ~]# rbd map block-device1 
	/dev/rbd0 
	[root@ceph-client1 ~]# rbd showmapped 
	id pool image         snap device 
	0  rbd  block-device1 -    /dev/rbd0 
	[root@ceph-client1 ~]# 
	   </pre>
	 </li><li class="listitem">
      <p>Создайте файловую систему на вашем блочном устройстве и смонтируйте её.</p>
	   <pre class="screen">
# mkfs.xfs /dev/rbd0
# mkdir -p /mnt/ceph-block-device1
# mount /dev/rbd0 /mnt/ceph-block-device1
# df -h /mnt/ceph-block-device1

	[root@ceph-client1 ~]# mkfs.xfs /dev/rbd0 
	log stripe unit (4194304 bytes) is too large (maximum is 256KiB) 
	log stripe unit adjusted to 32KiB 
	meta-data=/dev/rbd0              isize=256    agcount=17, agsize=162816 blks 
	         =                       sectsz=512   attr=2, projid32bit=1 
	         =                       crc=0        finobt=0 
	data     =                       bsize=4096   blocks=2621440, imaxpct.25 
	         =                       sunit=1024   swidth=1024 blks 
	naming   =version 2              bsize=4096   ascii-ci=0 ftype=0 
	log      =internal log           bsize=4096   blocks=2560, version=2 
	         =                       sectsz=512   sunit=8 blks, lazy-count=1 
	realtime =none                   extsz=4096   blocks=0, rtextents=0 
	[root@ceph-client1 ~]# mkdir -p /mnt/ceph-block-device1 
	[root@ceph-client1 ~]# mount /dev/rbd0 /mnt/ceph-block-device1 
	[root@ceph-client1 ~]# df -h /mnt/ceph-block-device1 
	Filesystem      Size  Used Avail Use% Mounted on 
	/dev/rbd0        10G   33M   10G   1% /mnt/ceph-block-device1 
	[root@ceph-client1 ~]# 
	   </pre>
	 </li><li class="listitem">
      <p>Для выполнения эталонного тестирования <span class="term"><code>block-device1</code></span> на запись общей длиной выполните 
	  следующую команду:</p>
	   <pre class="screen">
# rbd bench-write block-device1 --io-total 5368709200

	[root@ceph-client1 ~]# rbd bench-write block-device1 --io-total 5368709200 
	bench-write  io_size 4096 io_threads 16 bytes 5368709200 pattern seq 
	  SEC       OPS   OPS/SEC   BYTES/SEC 
	    1     67285  67304.27  275678272.46 
	    2    145469  72743.93  297959122.08 
	    3    224701  74906.90  306818647.61 
	    4    301802  75427.40  308950632.76 
	    5    372142  74432.24  304874445.83 
	    6    444010  75344.90  308612698.37 
	    7    517287  74363.64  304593457.23 
	    8    599236  74906.98  306818990.26 
	    9    672587  74178.98  303837121.67 
	   10    732910  72153.50  295540718.90 
	   11    784764  68150.81  279145733.52 
	   12    852044  66951.41  274232980.96 
	   13    918326  63817.89  261398064.96 
	   14    982399  61962.40  253797984.31 
	   15   1047148  62847.78  257424494.92 
	   16   1107514  64550.09  264397152.44 
	   17   1163126  62216.51  254838831.07 
	   18   1226368  61607.43  252344039.05 
	   19   1286892  60898.32  249439520.77 
	elapsed: 51 ops: 1310721 ops/sec: 25221.56 bytes/sec: 103307522.97 
[root@ceph-client1 ~]# 
	   </pre>
      <p>Как вы можете увидеть, вывод результатов <span class="term"><code>rbd bench-write</code></span> хорошо форматирован.</p>
	 </li>
	 </ol>
   </div>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100703"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это работает...</span></h4>
   </div></div></div>
   <p>Синтаксис <span class="term"><code>rbd bench-write</code></span> выглядит следующим образом:</p>
	   <pre class="screen">
# rbd bench-write &lt;RBD image name&gt;
	   </pre>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><span class="term"><code>--io-size</code></span>: размер записи в байтах; по умолчанию 4М</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--io-threads</code></span>: число потоков; по умолчанию 16</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--io-total</code></span>: общее число байт для записи; по умолчанию 1024M</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--io-pattern &lt;seq|rand&gt;</code></span>: шаблон записи, по умолчанию установлен <span class="term"><code>seq</code></span></p>
	 </li>
    </ul>
    </div>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100704"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Есть кое-что еще...</span></h4>
   </div></div></div>
   <p>Для подстройки ваших размера блока, количества потоков и шаблонов ввода/ вывода вы можете применять с
   <span class="term"><code>rbd bench-write</code></span> различные параметры.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100705"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Смотрите также...</span></h4>
   </div></div></div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><a class="link" href="Ch02.html" target="_top">Глава 2. Работа с блочными устройствами Ceph</a> подробно рассматривает создание 
	 блочного устройства Ceph.</p>
	 </li>
    </ul>
    </div>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1008"> </a>Эталонное тестирование Ceph RBD с применением FIO</h3>
   </div></div></div>
   <p>FIO является аббревиатурой для Flexible I/O; это одно из самых популярных средств для создания рабочей нагрузки ввода/ вывода и 
   эталонного тестирования. FIO имеет только что добавленную встроенную поддержку RBD. FIO является высоко настраиваемым и может применяться для 
   эмуляции и эталонного тестирования практически любых видов нагрузок. В данном рецепте мы изучим то, как FIO может применяться для 
   эталонного тестирования вашего RBD Ceph.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100801"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p>Чтобы выполнить эталонное тестирование блочного устройства Ceph, нам необходимо создать некое блочное устройство и отобразить 
   на ваш узел клиента Ceph:</p>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Установите пакет FIO на свой узел, на который вы отобразили свой образ RBD Ceph. В нашем случае это узел 
	  <span class="term"><code>ceph-client1</code></span>:</p>
	   <pre class="screen">
# yum install -y fio
	   </pre>
	 </li><li class="listitem">
      <p>Так как FIO поддерживает RBD IOengine нам не нужно монтировать образ RBD как файловую систему. Для эталонного тестирования RBD нам 
	  нужно просто предоставить имя вашего образа RBD, пул и пользователя Ceph, которые будут использоваться для соединения с кластером Ceph. 
	  Создайте профиль FIO со следующим содержанием:</p>
	   <pre class="screen">
[write-4M]
description="write test with block size of 4M"
ioengine=rbd
clientname=admin
pool=rbd
rbdname=block-device1
iodepth=32
runtime=120
rw=write
bs=4M

	[root@ceph-client1 ~]# 
	[root@ceph-client1 ~]# cat write.fio 
	[write-4m] 
	description=&quot;write test with block size of 4M&quot; 
	ioengine=rbd 
	clientname=admin 
	pool=rbd 
	rbdname=block-device1 
	iodepth=32 
	runtime=120 
	rw=write 
	bs=4M 
	[root@ceph-client1 ~]# 
	   </pre>
	 </li><li class="listitem">
      <p>Для запуска эталонного тестирования FIO, выполните команду FIO, предоставив ей в качестве аргумента файл профиля FIO:</p>
	   <pre class="screen"><code>
# fio write.fio

	[root@ceph-client1 ~]#
	[root@ceph-client1 ~]# fio write.fio 
	write-4M: (g=0) rw=write, bs=4M-4M/4M-4M/4M-4M, ioengine=rbd, iodepth=32 
	fio-2.2.8 
	Starting 1 process 
	rbd engine: RBD version: 0.1.9 
	Jobs: 1 (f=0): [w(1)] [100.0% done] [0K8/107.7MB/0K8 /s] [0/26/0 iops] [eta 00m:00s] 
	write-4m: (groupid=0, jobs=1): err= 0: pid=2146255: Wed Dec 9 00:54:40 2015 
	  Description : [&quot;write test with block size of 4M&quot;]  
	  <strong style="color:#cc0000;">write: io=010240MB, bw=314736KB/s, ops=76, runt= 33316msec</strong>
	    slat (usec) : min=129 , max=15181, avg=473.98, stdev=888.02 
	    clat (msec) : min=102 , max=2949 avg=409.87, stdev=263.06 
	     lat (msec) : min=102, max=2949 , avg=410.35 , stdev=263.06 
	    clat percentiles (msec): 
	     |  1.00th=[  131],  5.00th=[ 155], 10.00th=[ 180], 20.00th=[ 219],
	     | 30.00th=[  258], 40.00th=[ 310], 50.00th=[ 351], 60.00th=[ 392],
	     | 70.00th=[  441], 80.00th=[ 545], 90.00th=[ 693], 95.00th=[ 906],
	     | 99.00th=[ 1369], 99.50th=[1762], 99.90th=[2409], 99.95th=[2474],
	     | 99.99th=[ 2966]
	  bw (KB /s): min=74908, max=568888, per=100.00%, avg=327349.43, stdev=99611.29 
	  lat (msec) 250=27.70%, 500=47.19%, 750=17.42%, 1000=4.30%, 2000=3.20% 
	  lat (msec) : &gt;=2000=0.20% 
	cpu          : usr=3.04%, sys=0.59%, ctx=268, majf=0, minf=52854
	IO depth     : 1=0.3%, 2=1.2%, 4=4.7%, 8=19.4%, 16=68.5%, 32=5.8%, &gt;=64=0.0%
	   submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%
	   complete  : 0=0.0%, 4=95.9%, 8=0.1%, 16=0.8%, 32=3.3%, 64=0.0%, &gt;=64=0.0%
	   issued	 : total=r=0/w=2560/d=0, short=r=0/w=0/d=0, dror=r=0/w=0/d=0 
	   latency   : target=0, window=0, percentile=100.00%, depth=32 
	
	Run status group 0 (all jobs):  
	 <strong style="color:#cc0000;">WRITE: io=10240MB, aggrb=314736KB/s,</strong> minb=314736KB/s, maxb=314736K8/s, mint=33316msec, maxt=33316msec
	Disk stats (read/write): 
	    dm-0: ios=0/5, merge0/0, ticks=0/10, in_queue=10, util=0.01%, aggrios=56/5, aggrmerge=0/0, aggrticks=0/0, aggrin_gueue=0, aggrutil=0.00% 
	    mdl: ios=56/5, merge0/0, ticks=0/0, in_queue=0, util=0.00%, aggrios=3/13, aggrmerge=24/0, aggrticks=1/6, aggrinqueue=7, aggrutil=0.01% 
	sdbi : ios=7/13, merge=49/0, ticks=2/6, in_queue=8, util=0.01% 
	sdbj : ios=0/13, merge=0/0 , ticks=0/6, in_queue=6, util=0.01% ] 
	[root@ceph-client1 ~]# 
	   </code></pre>
	 </li><li class="listitem">
      <p>По завершению FIO сгенерирует массу полезной информации которую следует тщательно изучить. Однако, на первый взгляд вас может 
	  заинтересовать IOPS и агрегированная пропускная способность, которые выделены на предыдущем экранном снимке.</p>
	 </li>
	 </ol>
   </div>
   <p></p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100802"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Смотрите также...</span></h4>
   </div></div></div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><a class="link" href="Ch02.html" target="_top">Глава 2. Работа с блочными устройствами Ceph</a> подробно рассматривает создание 
	 блочного устройства Ceph.</p>
	 </li><li class="listitem">
	 <p>Для получения дополнительной информации по FIO посетите 
	 <a class="link" href="https://github.com/axboe/fio" target="_top">https://github.com/axboe/fio</a>.</p>
	 </li>
    </ul>
    </div>
  </div>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1009"> </a>Сокет администратора Ceph</h3>
   </div></div></div>
   <p>Компоненты Ceph являются демонами и <a class="link" href="https://ru.wikipedia.org/wiki/Сокет_домена_UNIX" target="_top">Сокетами домена UNIX</a> 
   (Unix domain socket, UDS, или IPC-сокетами сокетами межпроцессного взаимодействия). Ceph делает возможным для нас использовать эти сокеты для 
   построения очередей демонов. Сокет администратора Ceph является мощным инструментом для получения и сборки настроек демона Ceph во время его 
   выполнения. С применением этого инструмента изменение значений настроек демона становится намного проще даже в сравнении с изменением 
   вашего файла настроек Ceph, который требует перезапуска демона.</p>
   <p>Чтобы выполнить это, нам необходимо зарегистрироваться на нашем узле, выполняющем демонов Ceph и выполнить команды 
   <span class="term"><code>ceph daemon</code></span>.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="100901"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
	<p>Существует два способа доступа к вашему сокету администратора:</p>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Воспользовавшись именем вашего демона Ceph:</p>
	   <pre class="screen">
$ sudo ceph daemon {daemon-name} {option}
	   </pre>
	 </li><li class="listitem">
	 <p>Используя абсолютный путь к файлу сокета; его местоположение по умолчанию такое:
	 <span class="term"><code>/var/run/ceph</code></span>:</p>
	   <pre class="screen">
$ sudo ceph daemon {absolute path to socket file} {option}
	   </pre>
	 </li>
    </ul>
    </div>
	<p>Теперь мы попробуем осуществить доступ к демону Ceph используя наш сокет администратора:</p>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p>Вывести список всех доступных вашему сокету администратора команд для OSD:</p>
	   <pre class="screen">
# ceph daemon osd.0 help
	   </pre>
	 </li><li class="listitem">
      <p>Вывести список всех доступных вашему сокету администратора команд для MON:</p>
	   <pre class="screen">
# ceph daemon mon.ceph-node1 help
	   </pre>
	 </li><li class="listitem">
      <p>Проверить установки настроек OSD для <span class="term"><code>osd.0</code></span>.</p>
	   <pre class="screen">
# ceph daemon osd.0 config show
	   </pre>
	 </li><li class="listitem">
      <p>Проверить установки настроек MON для <span class="term"><code>mon.ceph-node1</code></span>.</p>
	   <pre class="screen">
# ceph daemon mon.ceph-node1 config show
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Ваш демон администратора Ceph позволяет вам изменять установки настроек вашего демона во время его выполнения. Однако эти 
	   изменения временные. Для изменения настроек вашего демона на постоянно, измените ваш файл настроек Ceph.</p>
	   </td></tr></table>
     </div>
	 </li><li class="listitem">
      <p>Для получения текущего значения настроек для <span class="term"><code>osd</code></span> воспользуйтесь параметром 
	  <span class="term"><code>_recover_max_chunk</code></span> для демона <span class="term"><code>osd.0</code></span>:</p>
	   <pre class="screen">
# ceph daemon osd.0 config get osd_recovery_max_chunk
	   </pre>
	 </li><li class="listitem">
      <p>Чтобы изменить ваше значение <span class="term"><code>osd_recovery_max_chunk</code></span> для 
	  <span class="term"><code>osd.0</code></span> выполните:</p>
	   <pre class="screen">
# ceph daemon osd.0 config set osd_recovery_max_chunk 1000000
	   </pre>
	 </li>
   </ol>
   </div>
  </div>
	   <pre class="screen">
	[root@ceph-node1 ~]# ceph daemon osd.0 config get osd_recovery_max_chunk 
	{
		&quot;osd_recoverymax_chunk&quot;: &quot;8388608&quot;
	}
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# ceph daemon osd.0 config set osd_recovery_max_chunk 1000000 
	{
		&quot;success&quot;: &quot;oscLrecoveryjnaxchunk = '1000000' &quot;
	}
	[root@ceph-node1 ~]# 
	   </pre>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1010"> </a>Применение команды ceph tell</h3>
   </div></div></div>
   <p>Другим эффективным способом изменения выполняющихся настроек для демона Ceph без дополнительных накладных расходов на регистрацию на 
   этом узле является применение команды <span class="term"><code>ceph tell</code></span>:</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101001"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
	<p>Команда <span class="term"><code>ceph tell</code></span> избавляет вас от затрат на регистрацию на том узле где выполняется ваш 
	демон. Эта команда проходит через узел монитора, поэтому вы можете выполнять её с любого узла в вашем кластере:</p>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Чтобы изменить установки <span class="term"><code>osd_recovery_threads</code></span> на <span class="term"><code>ceph tell</code></span>
	 выполните:</p>
	   <pre class="screen">
ceph tell osd.0 injectargs '--osd_recovery_threads=2'
	   </pre>
	 </li><li class="listitem">
	 <p>Чтобы изменить те же установки для всех OSD по всему вашему кластеру, выполните:</p>
	   <pre class="screen">
ceph tell osd.* injectargs '--osd_recovery_threads=2'
	   </pre>
	 </li><li class="listitem">
	 <p>Также вы можете изменять множество установок в одной строке:</p>
	   <pre class="screen">
ceph tell osd.* injectargs '--osd_recovery_max_active=1 --osd_recovery_max_single_start=1 --osd_recovery_op_priority=50'
	   </pre>
	 </li>
   </ol>
   </div>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101002"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это работает...</span></h4>
   </div></div></div>
   <p>Синтаксис команды <span class="term"><code>ceph tell</code></span> следующий:</p>
	   <pre class="screen">
ceph tell {daemon-type}.{id or *} injectargs --{config_setting_name} {value}
	   </pre>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1011"> </a>Ceph REST API</h3>
   </div></div></div>
   <p>Ceph  поставляется с мощным интерфейсом доступа REST API, который позволяет вам программно адимнистрировать ваш кластер. Он может работать 
   как WSGI приложение или в качестве автономного сервера, по умолчанию прослушивая порт 5000. Он предоставляет функциональность аналогичного вида
   с предлагаемой инструментарием командной строки через интерфейс доступа HTTP. Команды передаются как HTTP запросы GET и PUT, а результат может 
   возвращаться в форматах JSON, XML и текстовом. В этом рецепте я быстро покажу вам как установить Ceph REST API и как с ним взаимодействовать.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101101"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	  <p>Создайте в своём кластере Ceph пользователя <span class="term"><code>client.restapi</code></span> с соответствующим доступом к 
	  <span class="term"><code>mon</code></span>, <span class="term"><code>osd</code></span> и <span class="term"><code>mds</code></span>:</p>
	   <pre class="screen">
# ceph auth get-or-create client.restapi mds 'allow' osd 'allow *' mon 'allow *' &gt; /etc/ceph/ceph.client.restapi.keyring
	   </pre>
	 </li><li class="listitem">
	 <p>Добавьте в свой файл <span class="term"><code>ceph.conf</code></span> следующий раздел:</p>
	   <pre class="screen">
[client.restapi]
log file = /var/log/ceph/ceph.restapi.log
keyring = /etc/ceph/ceph.client.restapi.keyring
	   </pre>
	 </li><li class="listitem">
	 <p>Выполните следующую команду для запуска <span class="term"><code>ceph-rest-api</code></span> в качестве отдельного веб-сервера в 
	 фоновом режиме:</p>
	   <pre class="screen">
# nohup ceph-rest-api &gt; /var/log/ceph-rest-api &amp;&gt; /var/log/cephrest-api-error.log &amp;
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Вы также можете выполнить <span class="term"><code>ceph-rest-api</code></span> без его продвижения, подавляя его в фоновый режим.</p>
	   </td></tr></table>
     </div>
	 </li><li class="listitem">
	 <p><span class="term"><code>ceph-rest-api</code></span> теперь должен прослушивать по адресу <span class="term"><code>0.0.0.0:5000</code></span>;
	 выполните <span class="term"><code>curl</code></span> чтобы запросить <span class="term"><code>ceph-rest-api</code></span>
	 жизнеспособность вашего кластера:</p>
	   <pre class="screen">
# curl localhost:5000/api/v0.1/health
	   </pre>
	 </li><li class="listitem">
	 <p>Аналогично проверьте состояние <span class="term"><code>osd</code></span> и <span class="term"><code>mon</code></span>
	 через <span class="term"><code>rest-api</code></span>
	 Чтобы изменить установки <span class="term"><code>osd_recovery_threads</code></span> на <span class="term"><code>ceph tell</code></span>
	 выполните:</p>
	   <pre class="screen">
# curl localhost:5000/api/v0.1/osd/stat
# curl localhost:5000/api/v0.1/mon/stat

	[root@ceph-node1 ~]# nohup ceph-rest-api &gt; /var/log/ceph-rest-api &amp;&gt; /var/log/ceph-rest-api-error.log &amp; 
	[1] 3334321 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# curl localhost:5000/api/v0.1/health 
	HEALTH_OK 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# curl localhost:5000/api/v0.1/osd/stat 
	     osdmap e989: 9 olds: 9 up, 9 in 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# curl localhost:5000/api/v0.1/mon/stat 
	e5: 3 mons at {ceph-node1.192.168.1.101:6789/0,ceph-node2=192.168.1.102:6789/0,ceph-node3=192.168.1.103:6789 /0}, election epoch 3648, quorum 0,1,2 ceph-node1,ceph-node2,ceph-node3 
	[root@ceph-node1 ~]# 
	   </pre>
	 </li><li class="listitem">
	 <p><span class="term"><code>ceph-rest-api</code></span> поддерживает большинство команд CLI Ceph. Чтобы проверить перечень доступных 
	 <span class="term"><code>ceph-rest-api</code></span> команд выполните:</p>
	   <pre class="screen">
# curl localhost:5000/api/v0.1
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Эта команда вернёт результаты в формате HTML; вам будет лучше зайти через свой веб- браузер на 
	   <a class="link" href="localhost:5000/api/v0.1" target="_top">localhost:5000/api/v0.1</a> для выстраивания вашего HTML в читаемый 
	   вид.</p>
	   </td></tr></table>
     </div>
	 </li>
   </ol>
   </div>
   <p>Это базовая реализация <span class="term"><code>ceph-rest-api</code></span>. Для применения её в промышленной реализации хорошей мыслью 
   будет развернуть её болеечем в одном экземпляре в виде приложения WSGI обёрнутого веб- сервером и с балансровкой нагрузки на входе. 
   <span class="term"><code>ceph-rest-api</code></span> является масштабируемой, обладающей малым весом службой, которая делает возможным для 
   вас администрировать вашим кластером Ceph на уровне профессионала.</p>
  </div> 

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1012"> </a>Профилирование памяти Ceph</h3>
   </div></div></div>
   <p>Профилирование памяти является процессом динамичного программного анализа с целью определения потребления программами оперативной 
   памяти и выявления способов её оптимизации. В этом рецепте мы обсудим как вы можете применять профилировщики памяти на демонах 
   Ceph для исследования памяти.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101201"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	  <p>Запустите профилировщик памяти для определённого демона:</p>
	   <pre class="screen">
# ceph tell osd.0 heap start_profiler
	   </pre>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Для автоматического запуска профилировщика при старте этого демона OSD ceph установите соответствующее значение переменной 
	   окружения <span class="term"><code>CEPH_HEAP_PROFILER_INIT=true</code></span>.</p>
	   </td></tr></table>
     </div>
	 </li><li class="listitem">
	  <p>Неплохо будет оставить профилировщик работающим на несколько часов чтобы он смог по возможности собрать как можно больше информации, 
	  относящейся к отпечаткам памяти. В это же время вы можете создать некоторую нагрузку на свой кластер.</p>
	 </li><li class="listitem">
	  <p>Далее выведите статистики кучи о собранных профилировщиком отпечатках памяти:</p>
	   <pre class="screen">
# ceph tell osd.0 heap stats

	[root@ceph-node1 ~]# ceph tell osd.0 heap start_profiler 
	osd.0 started profiler 
	[root@ceph-node1 ~]# 
	[root@ceph-node1 ~]# ceph tell osd.0 heap stats 
	osd.0 tcmalloc heap stats:------------------------------------------------
	MALLOC:      238029520 (  227.0 MiB) Bytes in use by application 
	MALLOC: +            0 (    0.0 MiB) Bytes in page heap freelist
	MALLOC: +     13789912 (   13.2 MiB) Bytes in central cache freelist
	MALLOC: +      4454720 (    4.2 MiB) Bytes in transfer cache freelist
	MALLOC: +     28537112 (   27.2 MiB) Bytes in thread cache freelists
	MALLOC: +      2863264 (    2.7 MiB) Bytes in malloc metadata
	MALLOC:   ------------
	MALLOC: =    287674528 (  274.3 MiB) Actual memory used (physical + swap)
	MALLOC: +      2031616 (    1.9 MiB) Bytes released to OS (aka unmapped)
	MALLOC:   ------------
	MALLOC:      289706144 (  276.3 MiB) Virtual address space used
	MALLOC: 
	MALLOC:          13148              Spans in use
	MALLOC:            424              Thread heaps in use
	MALLOC:           8192              Tcmalloc page size 
	------------------------------------------------
	Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). 
	Bytes released to the OS take up virtual address space but no physical memory. 
	[root@ceph-node1 ~]# 
	   </pre>
	 </li><li class="listitem">
	  <p>Вы также можете сделать дамп статистик кучи в файл, который может быть использован позже; по умолчанию он создаётся 
	  в файле дампа <span class="term"><code>/var/log/ceph/osd.0.profile.0001.heap</code></span>:</p>
	   <pre class="screen">
# ceph tell osd.0 heap dump

	[root@ceph-node1 ~]# ceph tell osd.0 heap dump 
	osd.0 dumping heap profile now. 
	------------------------------------------------
	MALLOC:      238031808 (  227.0 MiB) Bytes in use by application
	MALLOC: +            0 (    0.0 MiB) Bytes in page heap freelist
	MALLOC: +     13589456 (   13.0 MiB) Bytes in central cache freelist
	MALLOC: +      4258112 (    4.1 MiB) Bytes in transfer cache freelist
	MALLOC: +     28964656 (   27.6 MiB) Bytes in thread cache freelists
	MALLOC: +      2863264 (    2.7 MiB) Bytes in malloc metadata
	MALLOC:   ------------
	MALLOC: =    287707296 (  274.4 MiB) Actual memory used (physical + swap)
	MALLOC: +      1998848 (    1.9 MiB) Bytes released to OS (aka unmapped)
	MALLOC:   ------------
	MALLOC: =    289706144 (  276.3 MiB) Virtual address space used
	MALLOC: 
	MALLOC:          13152              Spans in use
	MALLOC:            424              Thread heaps in use
	MALLOC:           8192              Tcmalloc page size 
	------------------------------------------------
	Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()). 
	Bytes released to the OS take up virtual address space but no physical memory. 
	[root@ceph-node1 ~]# 
	   </pre>
	 </li><li class="listitem">
	  <p>Чтобы прочитать этот файл дампа вам понадобится <span class="term"><code>google-perftools</code></span>:</p>
	   <pre class="screen">
# yum install -y google-perftools
	   </pre>
	 </li><li class="listitem">
	  <p>Для просмотра журналов профайлера:</p>
	   <pre class="screen">
# pprof --text {path-to-daemon} {log-path/filename}
# pprof --text /usr/bin/ceph-osd /var/log/ceph/osd.0.profile.0001.heap
	   </pre>
	 </li><li class="listitem">
	  <p>Для гранулированного сравнения сгенерируйте несколько файлов дампов профайлера для одного и того жедемона и примените инструмент 
	  профайлера Google для их сравнения:</p>
	   <pre class="screen">
# pprof --text --base /var/log/ceph/osd.0.profile.0001.heap /usr/bin/ceph-osd /var/log/ceph/osd.0.profile.0002.heap
	   </pre>
	 </li><li class="listitem">
	  <p>Освободите память, которую выделил TCMALLOC, однако она не используется Ceph:</p>
	   <pre class="screen">
# ceph tell osd.0 heap release
	   </pre>
	 </li><li class="listitem">
	  <p>По завершению остановите профайлер:</p>
	   <pre class="screen">
# ceph tell osd.0 heap stop_profiler
	   </pre>
	 </li>
   </ol>
   </div>
   <p>Процессы демонов Ceph достаточно зрелые, и скорее всего вам не понадобятся профайлеры памяти для анализа, если только вы не 
   столкнётесь с ошибкой, которая вызывает утечки памяти. Вы можете применять обсуждавшуюся ранее процедуру для вычисления проблем памяти 
   в демонах Ceph.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1013"> </a>Развёртывание Ceph с применением Ansible</h3>
   </div></div></div>
   <p>В этой книге мы уже обсудили развёртывание Ceph множеством способов, которые включают Ceph-deploy и Virtual Storage Manager (VSM, 
   Менеджер виртуальных хранилищ). Оба этих метода требуют ручной установки и настройки вашего кластера Ceph. Однако существуют и другие 
   инструменты и методы которые могут устанавливать и развёртывать Ceph для вас высоко автоматизированным образом. При помощи таких 
   методов вам большене понадобится набирать докучливые команды для развёртывания Ceph; по вашему желанию инструменты управления 
   настройкой, такие как Ansible, Puppet, Chef и прочие могут настраивать ваш кластер Ceph.</p>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101301"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Приготовление </span></h4>
   </div></div></div>
   <p>В этом рецепте мы пройдем сквозь Ansible, который является очень простым инструментом автоматизации ИТ и управления настройкой; для 
   дополнительной информации окиньте взглядом <a class="link" href="http://www.ansible.com/how-ansible-works" 
   target="_top">http://www.ansible.com/how-ansible-works</a>. Экосистема Ceph имеет живое сообщество вокруг него, которое разрабатывает 
   готовые к применению модули Ansible для Ceph. Мы будем применять эти модули <span class="term"><code>ceph-ansible</code></span> 
   (отсылаем к <a class="link" href="https://github.com/ceph/ceph-ansible" target="_top">https://github.com/ceph/ceph-ansible</a>) для 
   развёртывания вашего кластера Ceph с помощью Ansible.</p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101302"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p>Существует два способа которыми вы можете развернуть Ceph с применением модулей <span class="term"><code>ceph-ansible</code></span>:</p>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Применить модуль <span class="term"><code>ceph-ansible</code></span> для первого запуска нескольких виртуальных машин с применением 
	 Vagrent и VirtualBox/VMware, а затем установить и настроить Ceph с помощью Ansible.</p>
	 </li><li class="listitem">
	 <p>Применить модуль <span class="term"><code>ceph-ansible</code></span> для установки и настройки кластера Ceph с применением 
	 сборников сценариев Ansible на машинах голого железа.</p>
	 </li>
    </ul>
    </div>
   <p>В этом рецепте мы применим первый метод, то есть применим модуль <span class="term"><code>ceph-ansible</code></span> для запуска 
   виртуальных машин VirtualBox и Ansible для установки и развёртывания вашего кластера Ceph.</p>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Модули <span class="term"><code>ceph-ansible</code></span> являются местом средоточения сообщества и достаточно зрелые для вас чтобы 
	   вы могли использовать их для развёртывания Ceph в промышленном применении. Я хотел бы дать высокую оценку сообществу 
	   <span class="term"><code>ceph-ansible</code></span> за великолепно проделанную работу в разработке, совершенствовании и поддержке
	   модулей <span class="term"><code>ceph-ansible</code></span>.</p>
	   </td></tr></table>
     </div>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	  <p>На своей машине хоста VirtualBox клонируйте Git самый последний модуль <span class="term"><code>ceph-ansible</code></span>:</p>
	   <pre class="screen">
$ git clone https://github.com/ceph/ceph-ansible.git
$ cd ceph-ansible
	   </pre>
	 </li><li class="listitem">
	  <p><span class="term"><code>ceph-ansible</code></span> использует Vagrant. Для этой цели он поставляется вместе с 
	  <span class="term"><code>Vagrantfile</code></span>, который сообщает Vagrant как раскручивать ВМ. Этот файл требует некоторых 
	  переменных, которые определены в другом файле: <span class="term"><code>vagrant_variables.yml</code></span>. Модуль
	  <span class="term"><code>ceph-ansible</code></span> приходит с <span class="term"><code>vagrant_variables.yml.sample</code></span>, 
	  который может быть использован напрямую с минимальными изменениями:</p>
	   <pre class="screen">
$ cp vagrant_variables.yml.sample vagrant_variables.yml
	   </pre>
	 </li><li class="listitem">
	  <p>Настройки по умолчанию для Vagrant определены в <span class="term"><code>vagrant_variables.yml</code></span> и тоже достаточно 
	  хороши. Если вы пожелаете, вы можете подстроить конфигурацию отредактировав этот файл. Поскольку это тестовый кластер, мы уменьшим 
	  число мониторов с 3 до 1 изменив переменную <span class="term"><code>mon_vms</code></span> на значение 1.</p>
	 </li><li class="listitem">
	  <p>Модуль <span class="term"><code>ceph-ansible</code></span> принуждает Vagrant использовать Ansible как своего снабженца; для этой 
	  цели скопируйте файл <span class="term"><code>site.yml.sample</code></span> под именем <span class="term"><code>site.yml</code></span>
	  в ту же иерархию:</p>
	   <pre class="screen">
$ cp site.yml.sample site.yml
	   </pre>
	 </li><li class="listitem">
	  <p>Наконец, мы выполним <span class="term"><code>vagrant up</code></span>, который запустит 4 ВМ (1 монитор Ceph и 3 OSD Ceph), а после 
	  этого он запустит установку и развёртывание Ceph при помощи инструмента управления настройкой, Ansible:</p>
	   <pre class="screen">
$ vagrant up
	   </pre>
	 </li><li class="listitem">
	  <p>Предоставление ВМ и развёртывание Ceph займут несколько минут; после завершения вы должны увидеть примерно вот это:</p>
	   <pre class="screen">
	PLAY RECAP ******************************************************************** 
	mon0                     : ok=82   changed=15   unreachable=0    failed=0 
	osd0                     : ok=67   changed=9    unreachable=0    failed=0 
	osdl                     : ok=67   changed=9    unreachable=0    failed=0 
	osd2                     : ok=67   changed=9    unreachable=0    failed=0 

	   </pre>
	 </li><li class="listitem">
	  <p>После успешного завершения вы закончите с работающим кластером Ceph, установленным и настроенным при помощи Ansible.
	  Зарегистрируйтесь на своём узле <span class="term"><code>mon0</code></span> и проверьте состояние вашего кластера:</p>
	   <pre class="screen">
$ vagrant ssh mon0
$ sudo ceph -s

	vagrant@ceph-mon0:-$ sudo ceph -s 
	    cluster 4a158d27-f750-41d5-9e7f-26ce4c9d2d45 
	     health HEALTH_OK 
	     monmap el: 1 mons at {ceph-mon0=192.168.42.10:6789/0} 
	            election epoch 2, quorum 0 ceph-mon0 
	     mdsmap e2: 0/0/1 up 
	     osdmap e21: 6 osds: 6 up, 6 in 
	            flags sortbitwise 
	      pgmap v27: 320 pgs, 3 pools, 0 bytes data, 0 objects 
	            212 MB used, 65121 MB / 65333 MB avail 
	                 320 active+clean 
	vagrant@ceph-mon0:-S 
	   </pre>
	 </li>
   </ol>
   </div>
   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101303"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Есть кое-что еще...</span></h4>
   </div></div></div>
   <p>Сборка Vagrant может быть удалена при помощи <span class="term"><code>vagrant destroy -f</code></span> и повторно создана в любое время 
   в течение нескольких минут автоматизированным, интеллектуальным образом. Вы можете отметить как просто, быстро и бесшовно было выполнено 
   это развёртывание Ceph в сравнении с ручным. Для промышленного применения следует на самом деле рассматривать инструменты управления 
   настройкой, подобные Ansible, для развёртывания Ceph для сохранения всех узлов в том же состоянии с точки зрения настройки. К тому же они 
   очень удобны для управления большими кластерами с несколькими десятками узлов.</p>
  </div>

  <div class="section">
   <div xmlns="" class="titlepage"><div><div>
    <h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="1014"> </a>Инструментарий ceph-objectstore</h3>
   </div></div></div>
   <p>Одной из ключевых особенностей Ceph является её качества самовосстановления и самоизлечения. Ceph делает это благодаря сохранению 
   множества копий групп размещения по различным OSD и гарантирует очень высокую вероятность что вы никогда не потеряете свои данные. 
   В очень редких случаях вы можете наблюдать отказы множества OSD, при которых одна или более реплик PG находятся на отказавшем OSD и 
   состояние PG становится уязвимым, что ведёт к ошибкам в жизнеспособности кластера. Для гранулированного восстановления Ceph 
   предоставляет инструмент восстановления низкого уровня групп размещения и данных объекта, называемый 
   <span class="term"><code>ceph-objectstore-tool</code></span></p>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101401"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это сделать...</span></h4>
   </div></div></div>
   <p><span class="term"><code>ceph-objectstore-tool</code></span> может быть рискованной операцией и команда обязаны выполняться либо 
   пользователем <span class="term"><code>root</code></span>, либо с <span class="term"><code>sudo</code></span>. Не пытайтесь делать это 
   на вашем промышленном кластере без привлечения поддержки Red Hat Ceph Storage, если вы не уверены в том, что вы делаете. Это может 
   вызвать необратимую потерю данных в вашем кластере.</p>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Найдите незавершённую группу размещения в вашем кластере Ceph. Примените следующую команду и получите идентификатор вашей PG и 
	 её наборы действия:</p>
	   <pre class="screen">
# ceph health detail | grep incomplete
	   </pre>
	 </li><li class="listitem">
	 <p>С помощью действующего набора определите местоположение хоста OSD:</p>
	   <pre class="screen">
# ceph osd find &lt;osd_number&gt;
	   </pre>
	 </li><li class="listitem">
	 <p>Зарегистрируйтесь на этом узле OSD и остановите OSD с которым вы собираетесь работать:</p>
	   <pre class="screen">
# service ceph stop &lt;osd_ID&gt;
	   </pre>
 	 </li>
   </ol>
   </div> 
   <p>Следующиеразделы описывают функции с вашими OSD и группами размещения, которые вы можете применять с
   <span class="term"><code>ceph-objectstore-tool</code></span>:</p>

   <div class="itemizedlist">
   <ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Для идентификации ваших объектов в пределах OSD выполните следующее. Инструмент выдаст все объекты безотносительно от 
	 их групп размещения:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op list
	   </pre>
	 </li><li class="listitem">
	 <p>Чтобы идентифицировать ваш объект внутри группы размещения выполните:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &gt;/path/to/osd&gt; --journal-path &gt;/path/to/journal&gt; --pgid &lt;pgid&gt; --op list
	   </pre>
	 </li><li class="listitem">
	 <p>Для вывода перчня групп размещения, хранящихся на некоем OSD выполните:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op list-pgs
	   </pre>
	 </li><li class="listitem">
	 <p>Если вы знаете идентификатор объекта который вы ищете, задайте его для определения идентификатора его PG:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op list &lt;object-id&gt;
	   </pre>
	 </li><li class="listitem">
	 <p>Извлеките информацию об определённой группе размещения:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pg-id&gt; --op info
	   </pre>
	 </li><li class="listitem">
	 <p>Извлеките журнал операций в группе размещения:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pg-id&gt; --op log
	   </pre>
 	 </li>
   </ol>
   </div>
   <p>Удаление группы размещения является рискованной операцией и может вызвать потерю данных; применяйте эту функциональность с 
   предосторожностью. Если вы разрушите группу размещения в OSD это не допустит одноранговый обмен или запуск службы OSD, перед 
   удалением вашей группы размещения убедитесь что у вас имеется рабочая копия группы размещения на другом OSD. В качестве 
   предосторожности перед удалением вашей PG вы можете также сделать резервную копию этой группы размещения экспортировав её в 
   файл:</p>

   <div class="itemizedlist">
   <ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p>Для удаления группы размещения выполните следующую команду:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pg-id&gt; --op remove
	   </pre>
	 </li><li class="listitem">
	 <p>Для экспорта группы размещения в файл выполните следующую команду:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pg-id&gt; --file /path/to/file --op export
	   </pre>
	 </li><li class="listitem">
	 <p>Для импорта группы размещения из файла выполните следующую команду:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --file &lt;/path/to/file&gt; --op import
	   </pre>
	 </li><li class="listitem">
	 <p>OSD может иметь объект, помеченный как &quot;lost&quot;. Для вывода списка &quot;lost&quot; или &quot;unfound&quot; объектов 
	 выполните:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op list-lost
	   </pre>
	 </li><li class="listitem">
	 <p>Чтобы найти объекты помеченный как потерянные для отдельной группы размещения определите <span class="term"><code>pgid</code></span>:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pgid&gt; --op list-lost
	   </pre>
	 </li><li class="listitem">
	 <p>Инструмент <span class="term"><code>ceph-objectstore-tool</code></span> специально используется для фиксации ваших PG 
	 утраченных объектов. OSD может иметь объекты, помеченные как &quot;lost&quot;. Для удаления этих объектов из группы размещения выполните:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op fix-lost
	   </pre>
	 </li><li class="listitem">
	 <p>Для фиксации потерянных объектов в определённой группе размещения определите <span class="term"><code>pgid</code></span>:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --pgid &lt;pg-id&gt; --op fix-lost
	   </pre>
	 </li><li class="listitem">
	 <p>Если вы уверены в подлинности потерянного объекта, который вы хотите фиксировать, определите идентификатор этого объекта:</p>
	   <pre class="screen">
# ceph-objectstore-tool --data-path &lt;/path/to/osd&gt; --journal-path &lt;/path/to/journal&gt; --op fix-lost &lt;object-id&gt;
	   </pre>
 	 </li>
   </ol>
   </div>

   <div xmlns="" class="titlepage"><div><div>
    <h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="101402"> </a>
	<span  style="color:#FEFEFE; background-color:#888888; padding:0.5em;">Как это работает...</span></h4>
   </div></div></div>
   <p>Синтаксис для <span class="term"><code>ceph-objectstore-tool</code></span> следующий:
   <span class="term"><code>ceph-objectstore-tool &lt;options&gt;</code></span>.</p>
   <p>Значения для <span class="term"><code>&lt;options&gt;</code></span> могут быть следующими:</p>
  	<div class="itemizedlist">
	<ul class="itemizedlist" type="disc">
	 <li class="listitem">
	 <p><span class="term"><code>--data-path</code></span>: путь к вашему OSD</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--journal-path</code></span>: путь к вашему журналу</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--op</code></span>: операция</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--pgid</code></span>: идентификатор группы размещения</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--skip-journal-replay</code></span>: применяйте в случае разрушения вашего журнала</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--skip-mount-omap</code></span>: применяйте когда ваш <span class="term"><code>leveldb</code></span> 
	 хранимых данных разрушен и не может быть смонтирован</p>
	 </li><li class="listitem">
	 <p><span class="term"><code>--file</code></span>: путь к вашему файлу, применяется с операцией импорта/ экспорта</p>
	 </li>
    </ul>
    </div>
	<p>Для лучшего понимания этого инструмента, давайте рассмотрим пример: пулимеет две копии объекта и PG размещаются в 
	<span class="term"><code>osd.1</code></span> и <span class="term"><code>osd.2</code></span>. На данный момент, если случится отказ, произойдёт
	следующая последовательность:</p>
   <div class="orderedlist">
     <ol class="orderedlist" type="1"><li class="listitem">
      <p><span class="term"><code>osd.1</code></span> отключается.</p>
	 </li><li class="listitem">
      <p><span class="term"><code>osd.2</code></span> обрабатывает все операции записи в деградированном состоянии.</p>
	 </li><li class="listitem">
      <p><span class="term"><code>osd.1</code></span> возвращается и выполняет одноранговый обмен с 
	  <span class="term"><code>osd.2</code></span> для репликации данных.</p>
	 </li><li class="listitem">
      <p>Внезапно <span class="term"><code>osd.2</code></span> отключается до выполнения репликации всех необходимых объектов на
	  <span class="term"><code>osd.1</code></span>.</p>
	 </li><li class="listitem">
      <p>На данный момент у вас есть данные на <span class="term"><code>osd.1</code></span>, но они просрочены.</p>
	 </li>
	 </ol>
   </div>
   <p>В результате поиска неисправностей вы определите, что вы можете прочитать свои данные <span class="term"><code>osd.2</code></span> 
   из файловой системы, но его служба <span class="term"><code>osd</code></span> не запускается. В такой ситуации вам следует применить 
   <span class="term"><code>ceph-objectstore-tool</code></span> для экспорта/ извлечения данных с отказавшего 
   <span class="term"><code>osd</code></span>. <span class="term"><code>ceph-objectstore-tool</code></span> предоставит вам достаточные 
   возможности для изучения, модификации и извлечения данных объекта и метаданных.</p>
     <div class="note" style="margin-left: 0.5in; margin-right: 0.5in;">
	   <table border="0" summary="Замечание"><tr><td rowspan="2" align="center" valign="top" width="25">
	   <img alt="[Замечание]" src="../common/images/admon/note.png"/></td><th align="left">Замечание</th></tr><tr><td align="left" valign="top">
	   <p>Вам следует избегать применения средств Linux подобных <span class="term"><code>cp</code></span> и 
	   <span class="term"><code>rsync</code></span> для восстановления данных с отказавшего OSD, поскольку эти инструменты не 
	   примут во внимание все необходимые метаданные и восстановленные объекты могут быть неиспользуемыми.</p>
	   </td></tr></table>
     </div>
   <p>Наконец, мы достигли конца этойглавы и вообще всей книги. Я надеюсь, что ваше путешествие по Книге рецептов Ceph было информативным. 
   Вы должныбыли изучить некоторые концепции Ceph, которые снабдят вас достаточной уверенностью для работы со своим кластером Ceph в 
   вашем окружении. Наши поздравления! Сейчас вы достигли следующего уровня в Ceph.</p>
   <p></p>
   <p><span class="term"><strong class="userinput"><em class="emphasis">Продолжайте Изучение, Продолжайте исследования, Продолжайте совместное применение...</em></span></strong></span></p>
   <p><span class="term"><strong class="userinput"><em class="emphasis">Ура!</em></span></strong></span></p>
  </div>

 </div>

<!----><script type="text/javascript" src="FooterAndSidebar.js">
</script><script type="text/javascript"><!--</div id="content"> is inside next code
document.write(FooterAndSidebar);//-->
</script>

</body></html>